{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6570e05",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "* [Cache and Persist](#Cache-and-Persist)\n",
    "* [Block Eviction](#Block-Eviction)\n",
    "* [Difference between a DAG and a lineage](#Difference-between-a-DAG-and-a-lineage)\n",
    "* [Executing the jar in spark](#Executing-the-jar-in-spark)\n",
    "* [Practical - movie ratings](#Practical---movie-ratings)\n",
    "* [Structured APIs](#Structured-APIs)\n",
    "* [Practical - Reading CSV](#Practical---Reading-CSV)\n",
    "* [RDD vs DataFrame vs dataset](#RDD-vs-DataFrame-vs-dataset)\n",
    "* [Reading JSON files](#Reading-JSON-files)\n",
    "* [Reading a parquet file](#Reading-a-parquet-file)\n",
    "* [Ways to define the schema](#Ways-to-define-the-schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d906fc",
   "metadata": {},
   "source": [
    "### Cache and Persist\n",
    "\n",
    "Consider if you've a RDD which you've generated by using a bunch of transformations.\n",
    "\n",
    "what if we'll want to use the rdd4 again and don't want to do the other transformations again. Using the following cache function you can use the cached rdd4 again.\n",
    "\n",
    "```\n",
    "rdd1\n",
    "rdd2\n",
    "rdd3\n",
    "rdd4.cache\n",
    "rdd5\n",
    "rdd5.collect\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b4227a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,10118.861)\n",
      "(42,11393.681)\n",
      "(62,10506.643)\n",
      "(6,10795.759)\n",
      "(46,11926.222)\n",
      "(2,11989.182)\n",
      "(93,10531.5)\n",
      "(28,10001.421)\n",
      "(59,11285.781)\n",
      "(24,10519.84)\n",
      "(39,12386.221)\n",
      "(11,10304.58)\n",
      "(64,10577.38)\n",
      "(8,11034.48)\n",
      "(60,10081.419)\n",
      "(15,10827.0205)\n",
      "(35,10310.84)\n",
      "(97,11954.379)\n",
      "(0,11049.899)\n",
      "(55,10596.18)\n",
      "(40,10372.859)\n",
      "(71,11991.32)\n",
      "(22,10038.898)\n",
      "(26,10500.801)\n",
      "(68,12750.9)\n",
      "(33,10509.318)\n",
      "(17,10065.359)\n",
      "(73,12412.398)\n",
      "(69,10246.02)\n",
      "(41,11275.238)\n",
      "(92,10758.562)\n",
      "(9,10645.299)\n",
      "(34,10661.599)\n",
      "(61,10994.96)\n",
      "(81,10225.42)\n",
      "(25,10115.221)\n",
      "(63,10830.3)\n",
      "(65,10280.699)\n",
      "(29,10065.061)\n",
      "(90,10580.82)\n",
      "(32,10992.101)\n",
      "(85,11006.861)\n",
      "(54,12130.78)\n",
      "(72,10674.879)\n",
      "(52,10490.121)\n",
      "(58,10875.461)\n",
      "(87,10412.799)\n",
      "(70,10736.501)\n",
      "(43,10737.66)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rawCustomersInfo = /user/itv002768/customerorders_practical/customerorders-201008-180523.csv MapPartitionsRDD[7] at textFile at <console>:34\n",
       "splitCust = MapPartitionsRDD[8] at map at <console>:37\n",
       "totalPurchase = ShuffledRDD[9] at reduceByKey at <console>:40\n",
       "finalInfo = MapPartitionsRDD[10] at filter at <console>:41\n",
       "doubledAmount = MapPartitionsRDD[11] at map at <console>:44\n",
       "final_ = Array((19,10118.861), (42,11393.681), (62,10506.643), (6,10795.759), (46,11926.222), (2,11989.182), (93,10531.5), (28,10001.421), (59,11285.781), (24,10519.84), (39,12386.221), (11,...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array((19,10118.861), (42,11393.681), (62,10506.643), (6,10795.759), (46,11926.222), (2,11989.182), (93,10531.5), (28,10001.421), (59,11285.781), (24,10519.84), (39,12386.221), (11,..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawCustomersInfo = sc.textFile(\"/user/itv002768/customerorders_practical/customerorders-201008-180523.csv\")\n",
    "// split on \",\" and take only 1st and third element of an array, and convert third element to float\n",
    "// Put these in a touple\n",
    "val splitCust = rawCustomersInfo.map(x => (x.split(\",\")(0), x.split(\",\")(2).toFloat ) )\n",
    "\n",
    "// Calculate the sum of amount and sort by amount in descending order\n",
    "val totalPurchase = splitCust.reduceByKey((x, y) => x+y)\n",
    "val finalInfo = totalPurchase.filter(x => x._2>5000) //customers spending more than 5K\n",
    "\n",
    "// For the first time it will run it, but for the second time it'll not execute it.\n",
    "val doubledAmount = finalInfo.map(x => (x._1 , x._2*2)).cache()\n",
    "val final_ = doubledAmount.collect()\n",
    "for(info <- final_){\n",
    "    println(info)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4cc94e",
   "metadata": {},
   "source": [
    "Cache and Persist, both are used for the same purpose.</BR>\n",
    "Rather than recalculating we can reuse. A rdd that is not cached is reevaluated each time when you call an action.\n",
    "\n",
    "The difference is that cache will cache the rdd in-memory however, persist comes with various storage levels. If we'll use persist without passing any arguments then it'll act same as cache.\n",
    "\n",
    "persist(storageLevel.LEVEL)</BR>\n",
    "* MEMORY_ONLY - Data is cached in memory in non serialized form. If not enough memory is available it won't give any error rather it'll skip the caching step.\n",
    "* DISK_ONLY - Data is cached on disk in serialized form and takes less storage.\n",
    "* MEMORY_AND_DISK - This is very widely used, Data is cached in memory and if enough memory is not available evicted blocks from memory will be placed on disk in serialized form. This mode is recommended when re-evaluation is expensive and memory resources are scarce.\n",
    "* OFF_HEAP - Blocks are cached off-heap. Off heap means outside the JVM. Problem with storing the objects in JVM is that it uses garbage collection for freeing up and it is a time taking process. It is basically grabbing a piece of memory outside JVM to make it quick. These operations are performant but not safe.\n",
    "* MEMORY_ONLY_SER - Serialized form.\n",
    "* MEMORY_AND_DISK_SER - Serialized form.\n",
    "* MEMORY_ONLY_2 - This represents the two replicas on two different worker nodes. Just for high availability of cached rdd.\n",
    "\n",
    "### Block Eviction</BR>\n",
    "Let's say there are partitions blocks which are so large then they will quickly fill up the memory used for caching. When the storage memory becomes full an eviction policy will be used to make up the space for the new blocks based on LRU algorithm.\n",
    "\n",
    "**Serialization** increases the processing cost but reduces mamory foot prints.</BR>\n",
    "In case of **non-serialization**, memory foot prints are large but low processing is required.\n",
    "\n",
    "Do not cache/persist your base rdds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d146117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2) MapPartitionsRDD[11] at map at <console>:44 [Memory Deserialized 1x Replicated]\n",
       " |       CachedPartitions: 2; MemorySize: 4.4 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n",
       " |  MapPartitionsRDD[10] at filter at <console>:41 [Memory Deserialized 1x Replicated]\n",
       " |  ShuffledRDD[9] at reduceByKey at <console>:40 [Memory Deserialized 1x Replicated]\n",
       " +-(2) MapPartitionsRDD[8] at map at <console>:37 [Memory Deserialized 1x Replicated]\n",
       "    |  /user/itv002768/customerorders_practical/customerorders-201008-180523.csv MapPartitionsRDD[7] at textFile at <console>:34 [Memory Deserialized 1x Replicated]\n",
       "    |  /user/itv002768/customerorders_practical/customerorders-201008-180523.csv HadoopRDD[6] at textFile at <console>:34 [Memory Deserialized 1x Replicated]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "// Read the cached rdd from bottom to top\n",
    "doubledAmount.toDebugString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3421dd2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "Compile Error",
     "evalue": "<console>:43: error: not found: value storageLevel\n       val doubledAmountPersist = finalInfo.map(x => (x._1 , x._2*2)).persist(storageLevel.MEMORY_AND_DISK)\n                                                                              ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "val rawCustomersInfo = sc.textFile(\"/user/itv002768/customerorders_practical/customerorders-201008-180523.csv\")\n",
    "// split on \",\" and take only 1st and third element of an array, and convert third element to float\n",
    "// Put these in a touple\n",
    "val splitCust = rawCustomersInfo.map(x => (x.split(\",\")(0), x.split(\",\")(2).toFloat ) )\n",
    "\n",
    "// Calculate the sum of amount and sort by amount in descending order\n",
    "val totalPurchase = splitCust.reduceByKey((x, y) => x+y)\n",
    "val finalInfo = totalPurchase.filter(x => x._2>5000) //customers spending more than 5K\n",
    "\n",
    "// For the first time it will run it, but for the second time it'll not execute it.\n",
    "val doubledAmountPersist = finalInfo.map(x => (x._1 , x._2*2)).persist(storageLevel.MEMORY_AND_DISK)\n",
    "val final_ = doubledAmountPersist.collect()\n",
    "for(info <- final_){\n",
    "    println(info)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35957c8f",
   "metadata": {},
   "source": [
    "### Difference between a DAG and a lineage\n",
    "\n",
    "Lineage is a dependency graph where we've to read it from bottom to top and it shows dependencies of various rdds. You can say it's a logical plan.\n",
    "\n",
    "DAG is an acylic graph and it talks about jobs, stages and tasks.\n",
    "\n",
    "### Executing the jar in spark\n",
    "Inside bin there will be spark-submit\n",
    "\n",
    "```\n",
    "./spark-submit --class WordCount /path/to/jar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2179c03",
   "metadata": {},
   "source": [
    "### Practical - movie ratings\n",
    "below is the dataset containing user_id::movie_id::rating::watch_time\n",
    "\n",
    "```\n",
    "[itv002768@g02 ~]$ hadoop fs -head /user/itv002768/week11_practicals/ratings-201019-002101.dat\n",
    "1::1193::5::978300760\n",
    "1::661::3::978302109\n",
    "1::914::3::978301968\n",
    "1::3408::4::978300275\n",
    "1::2355::5::978824291\n",
    "1::1197::3::978302268\n",
    "1::1287::5::978302039\n",
    "1::2804::5::978300719\n",
    "1::594::4::978302268\n",
    "1::919::4::978301368\n",
    "1::595::5::978824268\n",
    "1::938::4::978301752\n",
    "\n",
    "[itv002768@g02 ~]$ hadoop fs -head /user/itv002768/week11_practicals/movies-201019-002101.dat\n",
    "1::Toy Story (1995)::Animation|Children's|Comedy\n",
    "2::Jumanji (1995)::Adventure|Children's|Fantasy\n",
    "3::Grumpier Old Men (1995)::Comedy|Romance\n",
    "4::Waiting to Exhale (1995)::Comedy|Drama\n",
    "5::Father of the Bride Part II (1995)::Comedy\n",
    "6::Heat (1995)::Action|Crime|Thriller\n",
    "7::Sabrina (1995)::Comedy|Romance\n",
    "8::Tom and Huck (1995)::Adventure|Children's\n",
    "9::Sudden Death (1995)::Action\n",
    "10::GoldenEye (1995)::Action|Adventure|Thriller\n",
    "11::American President, The (1995)::Comedy|Drama|Romance\n",
    "```\n",
    "\n",
    "Problem statement: Find all the movie names having average ratings > 4.5, but atleast 1k people should have rated that movie.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a9070f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schindler's List (1993)\n",
      "Shawshank Redemption, The (1994)\n",
      "Close Shave, A (1995)\n",
      "Wrong Trousers, The (1993)\n",
      "Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)\n",
      "Godfather, The (1972)\n",
      "Usual Suspects, The (1995)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rawData = /user/itv002768/week11_practicals/ratings-201019-002101.dat MapPartitionsRDD[168] at textFile at <console>:38\n",
       "mappedRatingsRdd = MapPartitionsRDD[169] at map at <console>:39\n",
       "valMap = MapPartitionsRDD[170] at mapValues at <console>:43\n",
       "reducedRdd = ShuffledRDD[171] at reduceByKey at <console>:44\n",
       "filteredRdd = MapPartitionsRDD[172] at filter at <console>:45\n",
       "filterRatings = MapPartitionsRDD[174] at filter at <console>:46\n",
       "movieInfo = /user/itv002768/week11_practicals/movie...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "/user/itv002768/week11_practicals/movie..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawData = sc.textFile(\"/user/itv002768/week11_practicals/ratings-201019-002101.dat\")\n",
    "val mappedRatingsRdd = rawData.map(x => {\n",
    "    val fields = x.split(\"::\")\n",
    "    (fields(1), fields(2))\n",
    "})\n",
    "val valMap = mappedRatingsRdd.mapValues(x => (x.toFloat, 1.0))\n",
    "val reducedRdd = valMap.reduceByKey((x,y) => (x._1 + y._1, x._2 + y._2))\n",
    "val filteredRdd = reducedRdd.filter(x => x._2._2 > 100)\n",
    "val filterRatings = filteredRdd.mapValues(x => x._1/x._2).filter(x=> x._2 > 4.5)\n",
    "//filterRatings.collect().foreach(println)\n",
    "\n",
    "val movieInfo = sc.textFile(\"/user/itv002768/week11_practicals/movies-201019-002101.dat\")\n",
    "val requiredMovieInfo = movieInfo.map(x => {\n",
    "    val fields = x.split(\"::\")\n",
    "    (fields(0), fields(1))\n",
    "})\n",
    "\n",
    "val joinedInfo = requiredMovieInfo.join(filterRatings)\n",
    "val final_ = joinedInfo.map(x => x._2._1)\n",
    "final_.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f2950",
   "metadata": {},
   "source": [
    "## Structured APIs\n",
    "\n",
    "There are Structured APIs in the form of DataFrames or Datasets. Internally everything is happening in the form of RDDs.\n",
    "\n",
    "A DataFrame is a distributed collection of data organized in named columns. It is conceptually equivlent to a table in RDBMS. But in dataframe data will be divided in many partitions.\n",
    "\n",
    "dataframes and datasets were also available in spark1 as well. From spark2 we got better support for these two APIs and both of these are merged into a single API known as \"dataset API\".\n",
    "\n",
    "Moving forward we'll start using sparkSession instead of sparkContext. For everythin we used to create separate context like spark context, hive context, sql context etc. But Spark Session is an unified entry point of spark application. It provides a way to interact with various spark functionalities with lesser number of constructs. All are encapsulated in sparkSession.\n",
    "\n",
    "SparkSession is a singleton object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ab55287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@234e2e4f\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@234e2e4f"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// SparkSession example\n",
    "val spark = SparkSession.builder().\n",
    "appName(\"My Application-1\").\n",
    "master(\"local[2]\").\n",
    "getOrCreate()\n",
    "\n",
    "//closing the connection\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51b65c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@e4645e\n",
       "spark = org.apache.spark.sql.SparkSession@5bbe8b97\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@5bbe8b97"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// using spark conf\n",
    "import org.apache.spark.SparkConf\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cba97b",
   "metadata": {},
   "source": [
    "### Practical - Reading CSV\n",
    "```\n",
    "[itv002768@g02 ~]$ hadoop fs -put orders-201019-002101.csv /user/itv002768/week11_practicals/\n",
    "[itv002768@g02 ~]$ hadoop fs -head /user/itv002768/week11_practicals/orders-201019-002101.csv\n",
    "order_id,order_date,order_customer_id,order_status\n",
    "1,2013-07-25 00:00:00.0,11599,CLOSED\n",
    "2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT\n",
    "3,2013-07-25 00:00:00.0,12111,COMPLETE\n",
    "4,2013-07-25 00:00:00.0,8827,CLOSED\n",
    "5,2013-07-25 00:00:00.0,11318,COMPLETE\n",
    "6,2013-07-25 00:00:00.0,7130,COMPLETE\n",
    "7,2013-07-25 00:00:00.0,4530,COMPLETE\n",
    "8,2013-07-25 00:00:00.0,2911,PROCESSING\n",
    "9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81bfaff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@37d6826e\n",
       "spark = org.apache.spark.sql.SparkSession@1f90077d\n",
       "ordersDf = [order_id: int, order_date: timestamp ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: timestamp ... 2 more fields]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "// Never use the inferSchema on prod as it will only read sampleset and infer the schema\n",
    "val ordersDf = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week11_practicals/orders-201019-002101.csv\")\n",
    "ordersDf.show()\n",
    "ordersDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e130a96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n",
      "+-----------------+-----+\n",
      "|order_customer_id|count|\n",
      "+-----------------+-----+\n",
      "|            10362|    6|\n",
      "|            12027|    3|\n",
      "|            11858|    7|\n",
      "|            10623|    8|\n",
      "|            11458|    7|\n",
      "|            11748|    6|\n",
      "|            12046|    4|\n",
      "|            11317|    7|\n",
      "|            11141|    7|\n",
      "|            10206|    4|\n",
      "|            10817|    6|\n",
      "|            11033|    2|\n",
      "|            10914|    3|\n",
      "|            10703|    6|\n",
      "|            10462|    7|\n",
      "|            10121|    5|\n",
      "|            12006|    4|\n",
      "|            10081|    7|\n",
      "|            11280|    4|\n",
      "|            11500|    6|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@63e24e85\n",
       "spark = org.apache.spark.sql.SparkSession@1f90077d\n",
       "ordersDf = [order_id: int, order_date: timestamp ... 2 more fields]\n",
       "groupedOrdersDf = [order_customer_id: int, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_customer_id: int, count: bigint]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "// Never use the inferSchema on prod as it will only read sampleset and infer the schema\n",
    "val ordersDf = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week11_practicals/orders-201019-002101.csv\")\n",
    "ordersDf.show()\n",
    "ordersDf.printSchema()\n",
    "\n",
    "// Give me the number of orders made by customers whose customer_id > 10000\n",
    "val groupedOrdersDf = ordersDf\n",
    "                    .repartition(4)\n",
    "                    .where(\"order_customer_id > 10000\") // filtering\n",
    "                    .select(\"order_id\", \"order_customer_id\") // selecting columns\n",
    "                    .groupBy(\"order_customer_id\")\n",
    "                    .count()\n",
    "\n",
    "groupedOrdersDf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c4a13",
   "metadata": {},
   "source": [
    "### RDD vs DataFrame vs dataset\n",
    "\n",
    "* When we use RDD, we are dealing with low level code and we have to tell the system how to work. This is not developer friendly.\n",
    "* Lower level code lags some of the basic optimizations.\n",
    "\n",
    "To make life of developers easier they develop **DataFrame** in spark 1.3\n",
    "* Higher level constructs. We've to tell the system what we want system will take care of it.\n",
    "\n",
    "Challanges with DataFrame:\n",
    "* DataFrames do not offer strongly typed code. It means type errors are not caught at compile time instead of run time they are caught.\n",
    "* Developers felt that their flexibility has become limited.\n",
    "\n",
    "There was a way where dataframes can be converted to RDD\n",
    "\n",
    "df.rdd (whenever, we want more flexibility and type safety we can conver it to rdd).\n",
    "* This conversion from df to rdd is not seamless.\n",
    "* If we work with raw rdd by converting df to rdd, we'll miss out some major optimizations. Catalyst optimizer and tungsten engine is only possible in case of data frames.\n",
    "\n",
    "\n",
    "Just to address the above challanges DataSets came into picture in spark 1.6. It provides:\n",
    "* compile time safety.\n",
    "* We get more flexibility in terms of using lower level code.\n",
    "* Conversion from dataframe to dataset is seamless.\n",
    "* We won't lose on any of the optimizations.\n",
    "\n",
    "Before spark2.0 both dataframes and datasets were different things. In Spark2.0 dataframe and dataset were merged into a unified spark dataset API or you can say Structured API.\n",
    "\n",
    "DataFrame is nothing but a DataSet[row], row is nothing but a generic type which is found at runtime\n",
    "\n",
    "In Case of dataframe the datatypes are bound at runtime. However DataSet[Employee] is bound at compile time.\n",
    "\n",
    "DataSet[row] -> DataFrame (Type errors are caught at runtime)\n",
    "\n",
    "DataSet[Employee] -> DataSet (compile time type safety)\n",
    "\n",
    "How to convert DataFrame to a DataSet? If we replace generic row with a specific object then it becomes a dataset.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "976beadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": "cannot resolve '`order_ids`' given input columns: [order_id, order_date, order_customer_id, order_status]; line 1 pos 0;\n'Filter ('order_ids > 10)\n+- Relation[order_id#238,order_date#239,order_customer_id#240,order_status#241] csv\n",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: cannot resolve '`order_ids`' given input columns: [order_id, order_date, order_customer_id, order_status]; line 1 pos 0;",
      "'Filter ('order_ids > 10)",
      "+- Relation[order_id#238,order_date#239,order_customer_id#240,order_status#241] csv",
      "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)",
      "  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)",
      "  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)",
      "  at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)",
      "  at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)",
      "  at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)",
      "  at org.apache.spark.sql.Dataset.filter(Dataset.scala:1504)",
      "  ... 42 elided"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "// Never use the inferSchema on prod as it will only read sampleset and infer the schema\n",
    "val ordersDf = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week11_practicals/orders-201019-002101.csv\")\n",
    "ordersDf.show()\n",
    "ordersDf.printSchema()\n",
    "\n",
    "// This will work fine\n",
    "//ordersDf.filter(\"order_id > 10\").show()\n",
    "\n",
    "/* this will give error at runtime instead of compile time\n",
    "annot resolve '`order_ids`' given input columns: [order_id, order_date, order_customer_id, order_status]; line 1 pos 0;\n",
    "*/\n",
    "ordersDf.filter(\"order_ids > 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b4c9cf",
   "metadata": {},
   "source": [
    "How to convert a dataframe to a dataset?\n",
    "\n",
    "Create a Case Class and create dataset[OrdersData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9f9f03b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Unknown Error",
     "evalue": "<console>:30: error: not found: value SparkSession\n       val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n                   ^\n<console>:42: error: Unable to find encoder for type OrdersData. An implicit Encoder[OrdersData] is needed to store OrdersData instances in a Dataset. Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.\n       val ordersDs = ordersDf.as[OrdersData]\n                                 ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "import java.sql.Timestamp\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "case class OrdersData(order_id: Int, order_date: Timestamp, order_customer_id: Int, order_status: String)\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "// Never use the inferSchema on prod as it will only read sampleset and infer the schema\n",
    "val ordersDf: Dataset[Row] = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week11_practicals/orders-201019-002101.csv\")\n",
    "\n",
    "/* \n",
    "This import is required if you want to convert dataframe to dataset or vice versa\n",
    "and you cannot put it in the starting because it requires spark session\n",
    "*/\n",
    "import spark.implicits._\n",
    "val ordersDs = ordersDf.as[OrdersData]\n",
    "\n",
    "ordersDs.filter(x => x.order_id <10 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357f0bba",
   "metadata": {},
   "source": [
    "Converting a dataframe to datasets there is an overhead involved and this is for casting it to a particular type that's why dataframe is preferred.\n",
    "\n",
    "When we're dealing with dataframes then the searialization is managed by tungston binary format which is very fast. But when we're dealing with datasets then the serialization is managed by Java which is quiet slow and impacts the performance. Definately, datasets will help us in minimizing the developer mistakes at compile time but it comes with a cost of serialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967bb41",
   "metadata": {},
   "source": [
    "**Operations we've done till now**\n",
    "\n",
    "1. Read the data\n",
    "\n",
    "There can be two kinds of data sources:\n",
    "- External data source - e.g dbms, redshift, mongo, external API.\n",
    "- Internal data source - s3, hdfs, azure, gcs\n",
    "\n",
    "We've the flexibility in spark to create a dataframe from an external data source.</BR>\n",
    "Spark is good at processing but not that efficient for data ingestion.\n",
    "\n",
    "for ex - spark provides a jdbc mysql connector to fetch the data but it is not a good practice. Instead we'll use sqoop to get the data in hdfs and use that as a internal data source.\n",
    "\n",
    "2. Perform the transformations\n",
    "\n",
    "3. Writing the data to target(sink)\n",
    "\n",
    "Again, it can be internal or external. But not recommended to write on external data sources.\n",
    "\n",
    "**Below is the standard way of reading the files instead of using csv method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c52dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@549f4ded\n",
       "spark = org.apache.spark.sql.SparkSession@2b38cf\n",
       "ordersDf = [order_id: int, order_date: timestamp ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: timestamp ... 2 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "// Never use the inferSchema on prod as it will only read sampleset and infer the schema\n",
    "val ordersDf = spark.read.format(\"csv\")\n",
    "               .option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .option(\"path\", \"/user/itv002768/week11_practicals/orders-201019-002101.csv\").load()\n",
    "ordersDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4d8613",
   "metadata": {},
   "source": [
    "### Reading JSON files\n",
    "\n",
    "```\n",
    "[itv002768@g02 ~]$ hadoop fs -head /user/itv002768/week11_practicals/players-201019-002101.json\n",
    "{\"player_id\":101, \"player_name\":\"R Sharma\", \"age\":33, \"role\":\"Batsman\", \"team_id\":11, \"country\":\"IND\"}\n",
    "{\"player_id\":102, \"player_name\":\"S Iyer\", \"age\":25, \"role\":\"Batsman\", \"team_id\":15, \"country\":\"IND\"}\n",
    "{\"player_id\":103, \"player_name\":\"T Boult\", \"age\":30, \"role\":\"Bowler\", \"team_id\":13, \"country\":\"NZ\"}\n",
    "{\"player_id\":104, \"player_name\":\"MS Dhoni\", \"age\":38, \"role\":\"WKeeper\", \"team_id\":14, \"country\":\"IND\"}\n",
    "{\"player_id\":105, \"player_name\":\"S Watson\", \"age\":39, \"role\":\"Allrounder\", \"team_id\":12, \"country\":\"AUS\"}\n",
    "{\"player_id\":106, \"player_name\":\"S Hetmyer\", \"age\":23, \"role\":\"Batsman\", \"team_id\":16, \"country\":\"WI\"}\n",
    "```\n",
    "\n",
    "To deal with malformed json line we use the following read modes:\n",
    "* PERMISSIVE - This is default mode, it sets all the fields to null if it encounters a corrupt row. Corrupt record will be shown under a new column _corrupt_record\n",
    "* DROPMALFORMED - It'll not consider the malformed rows.\n",
    "* FAILFAST - If a malformed record is found an exception is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5e85f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- player_id: long (nullable = true)\n",
      " |-- player_name: string (nullable = true)\n",
      " |-- role: string (nullable = true)\n",
      " |-- team_id: long (nullable = true)\n",
      "\n",
      "+---+-------+---------+-----------+----------+-------+\n",
      "|age|country|player_id|player_name|      role|team_id|\n",
      "+---+-------+---------+-----------+----------+-------+\n",
      "| 33|    IND|      101|   R Sharma|   Batsman|     11|\n",
      "| 25|    IND|      102|     S Iyer|   Batsman|     15|\n",
      "| 30|     NZ|      103|    T Boult|    Bowler|     13|\n",
      "| 38|    IND|      104|   MS Dhoni|   WKeeper|     14|\n",
      "| 39|    AUS|      105|   S Watson|Allrounder|     12|\n",
      "| 23|     WI|      106|  S Hetmyer|   Batsman|     16|\n",
      "+---+-------+---------+-----------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@3041bf43\n",
       "spark = org.apache.spark.sql.SparkSession@2b38cf\n",
       "ordersDf = [age: bigint, country: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint, country: string ... 4 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "// Never use the inferSchema on prod as it will only read sampleset and infer the schema\n",
    "val ordersDf = spark.read.format(\"json\")\n",
    "               .option(\"path\", \"/user/itv002768/week11_practicals/players-201019-002101.json\")\n",
    "               .option(\"mode\", \"DROPMALFORMED\")\n",
    "               .load()\n",
    "ordersDf.printSchema()\n",
    "ordersDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d68dc6",
   "metadata": {},
   "source": [
    "### Reading a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d4251f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- registration_dttm: timestamp (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- cc: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- birthdate: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n",
      "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "|  registration_dttm| id|first_name|last_name|               email|gender|     ip_address|                 cc|             country| birthdate|   salary|               title|            comments|\n",
      "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "|2016-02-03 02:55:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|    1.197.201.2|   6759521864920116|           Indonesia|  3/8/1971| 49756.53|    Internal Auditor|               1E+02|\n",
      "|2016-02-03 12:04:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male| 218.111.175.34|                   |              Canada| 1/16/1968|150280.17|       Accountant IV|                    |\n",
      "|2016-02-02 20:09:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|   7.161.136.94|   6767119071901597|              Russia|  2/1/1960|144972.51| Structural Engineer|                    |\n",
      "|2016-02-02 19:36:21|  4|    Denise|    Riley|    driley3@gmpg.org|Female|  140.35.109.83|   3576031598965625|               China|  4/8/1997| 90263.05|Senior Cost Accou...|                    |\n",
      "|2016-02-03 00:05:31|  5|    Carlos|    Burns|cburns4@miitbeian...|      | 169.113.235.40|   5602256255204850|        South Africa|          |     null|                    |                    |\n",
      "|2016-02-03 02:22:34|  6|   Kathryn|    White|  kwhite5@google.com|Female| 195.131.81.179|   3583136326049310|           Indonesia| 2/25/1983| 69227.11|   Account Executive|                    |\n",
      "|2016-02-03 03:33:08|  7|    Samuel|   Holmes|sholmes6@foxnews.com|  Male| 232.234.81.197|   3582641366974690|            Portugal|12/18/1987| 14247.62|Senior Financial ...|                    |\n",
      "|2016-02-03 01:47:06|  8|     Harry|   Howell| hhowell7@eepurl.com|  Male|   91.235.51.73|                   |Bosnia and Herzeg...|  3/1/1962|186469.43|    Web Developer IV|                    |\n",
      "|2016-02-02 22:52:53|  9|      Jose|   Foster|   jfoster8@yelp.com|  Male|   132.31.53.61|                   |         South Korea| 3/27/1992|231067.84|Software Test Eng...|               1E+02|\n",
      "|2016-02-03 13:29:47| 10|     Emily|  Stewart|estewart9@opensou...|Female| 143.28.251.245|   3574254110301671|             Nigeria| 1/28/1997| 27234.28|     Health Coach IV|                    |\n",
      "|2016-02-02 19:10:42| 11|     Susan|  Perkins| sperkinsa@patch.com|Female|    180.85.0.62|   3573823609854134|              Russia|          |210001.95|                    |                    |\n",
      "|2016-02-03 13:04:34| 12|     Alice|    Berry|aberryb@wikipedia...|Female| 246.225.12.189|   4917830851454417|               China| 8/12/1968| 22944.53|    Quality Engineer|                    |\n",
      "|2016-02-03 13:48:17| 13|    Justin|    Berry|jberryc@usatoday.com|  Male|   157.7.146.43|6331109912871813274|              Zambia| 8/15/1975| 44165.46|Structural Analys...|                    |\n",
      "|2016-02-03 16:46:52| 14|     Kathy| Reynolds|kreynoldsd@redcro...|Female|  81.254.172.13|   5537178462965976|Bosnia and Herzeg...| 6/27/1970|286592.99|           Librarian|                    |\n",
      "|2016-02-03 03:53:23| 15|   Dorothy|   Hudson|dhudsone@blogger.com|Female|       8.59.7.0|   3542586858224170|               Japan|12/20/1989|157099.71|  Nurse Practicioner|<script>alert('hi...|\n",
      "|2016-02-02 19:44:01| 16|     Bruce|   Willis|bwillisf@bluehost...|  Male|239.182.219.189|   3573030625927601|              Brazil|          |239100.65|                    |                    |\n",
      "|2016-02-02 19:57:45| 17|     Emily|  Andrews|eandrewsg@cornell...|Female| 29.231.180.172|     30271790537626|              Russia| 4/13/1990|116800.65|        Food Chemist|                    |\n",
      "|2016-02-03 11:44:24| 18|   Stephen|  Wallace|swallaceh@netvibe...|  Male|  152.49.213.62|   5433943468526428|             Ukraine| 1/15/1978|248877.99|Account Represent...|                    |\n",
      "|2016-02-03 06:45:54| 19|  Clarence|   Lawson|clawsoni@vkontakt...|  Male| 107.175.15.152|   3544052814080964|              Russia|          |177122.99|                    |                    |\n",
      "|2016-02-03 05:30:36| 20|   Rebecca|     Bell| rbellj@bandcamp.com|Female|172.215.104.127|                   |               China|          |137251.19|                    |                    |\n",
      "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@f6d9291\n",
       "spark = org.apache.spark.sql.SparkSession@2b38cf\n",
       "ordersDf = [registration_dttm: timestamp, id: int ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[registration_dttm: timestamp, id: int ... 11 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "val ordersDf = spark.read.format(\"parquet\") // If you'll not mention then also it will take as parquet by default\n",
    "               .option(\"path\", \"/user/itv002768/week11_practicals/users-201019-002101.parquet\")\n",
    "               .load()\n",
    "ordersDf.printSchema()\n",
    "ordersDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7d09b",
   "metadata": {},
   "source": [
    "### Ways to define the schema\n",
    "\n",
    "There are three options:\n",
    "\n",
    "- Infer Schema - Not preferred for production.\n",
    "\n",
    "- Implicit Schema - Try to read a file where schema is associated with it e.g parquet, avro, json etc.\n",
    "\n",
    "- Explicit schema - Manually defining the schema.\n",
    "\n",
    "### Explicit Schema\n",
    "* Do it programatically.\n",
    "Here, we'll create a struct type and it is applicable to one row.\n",
    "\n",
    "```scala\n",
    "val ordersSchema = StructType(List(\n",
    "StructField(\"orderid\", <datatype>),\n",
    "StructField(\"orderdata\", <datatype>),\n",
    ".\n",
    ".\n",
    "))\n",
    "```\n",
    "\n",
    "StructFields will be equal to the number of columns\n",
    "\n",
    "```\n",
    "Scala     Spark\n",
    "---------------\n",
    "Int       IntegetType\n",
    "Long      LongType\n",
    "Float     FloatType\n",
    "Double    DoubleType\n",
    "String    StringType\n",
    "Timestamp TimestampType\n",
    "```\n",
    "In StructField(\"order_id\", IntegerType), First param is column name, second param is data type and the third param is of bool type that tells us if the field is nullable or not. By default it is true and if we'll pass false then it means that column should only contain the non-null values.\n",
    "\n",
    "Below is the code for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ce5e85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_statue: string (nullable = true)\n",
      "\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_statue|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@33ee22bb\n",
       "spark = org.apache.spark.sql.SparkSession@2b38cf\n",
       "ordersSchema = StructType(StructField(order_id,IntegerType,true), StructField(order_date,TimestampType,true), StructField(order_customer_id,IntegerType,true), StructField(order_statue,StringType,true))\n",
       "ordersDf = [order_id: int, order_date: timestamp ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: timestamp ... 2 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.types.StructType\n",
    "import org.apache.spark.sql.types.StructField\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "import org.apache.spark.sql.types.StringType\n",
    "import org.apache.spark.sql.types.TimestampType\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "val ordersSchema = StructType(List(\n",
    "StructField(\"order_id\", IntegerType),\n",
    "StructField(\"order_date\", TimestampType),\n",
    "StructField(\"order_customer_id\", IntegerType),\n",
    "StructField(\"order_statue\", StringType)\n",
    "))\n",
    "\n",
    "val ordersDf = spark.read.option(\"header\", true)\n",
    "               .schema(ordersSchema)\n",
    "               .csv(\"/user/itv002768/week11_practicals/orders-201019-002101.csv\")\n",
    "ordersDf.printSchema()\n",
    "ordersDf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310cde89",
   "metadata": {},
   "source": [
    "* DDL String\n",
    "\n",
    "val ordersSchemaDDL = \"order_id Int, order_date String, cust_id Int, order_status int\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c08c25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- cust_id: integer (nullable = true)\n",
      " |-- order_status: integer (nullable = true)\n",
      "\n",
      "+--------+----------+-------+------------+\n",
      "|order_id|order_date|cust_id|order_status|\n",
      "+--------+----------+-------+------------+\n",
      "|    null|      null|   null|        null|\n",
      "|    null|      null|   null|        null|\n",
      "|    null|      null|   null|        null|\n",
      "|    null|      null|   null|        null|\n",
      "|    null|      null|   null|        null|\n",
      "|    null|      null|   null|        null|\n",
      "|    null|      null|   null|        null|\n",
      "|    null|      null|   null|        null|\n",
      "|    null|      null|   null|        null|\n",
      "|    null|      null|   null|        null|\n",
      "|    null|      null|   null|        null|\n",
      "|    null|      null|   null|        null|\n",
      "|    null|      null|   null|        null|\n",
      "|    null|      null|   null|        null|\n",
      "|    null|      null|   null|        null|\n",
      "|    null|      null|   null|        null|\n",
      "|    null|      null|   null|        null|\n",
      "|    null|      null|   null|        null|\n",
      "|    null|      null|   null|        null|\n",
      "|    null|      null|   null|        null|\n",
      "+--------+----------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@771dcad8\n",
       "spark = org.apache.spark.sql.SparkSession@2b38cf\n",
       "ordersSchemaDDL = order_id Int, order_date String, cust_id Int, order_status Int\n",
       "ordersDf = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "val ordersSchemaDDL = \"order_id Int, order_date String, cust_id Int, order_status Int\"\n",
    "\n",
    "val ordersDf = spark.read.option(\"header\", true)\n",
    "               .schema(ordersSchemaDDL)\n",
    "               .csv(\"/user/itv002768/week11_practicals/orders-201019-002101.csv\")\n",
    "ordersDf.printSchema()\n",
    "ordersDf.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
