{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b1d715",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "* [Writing output to sink](#Writing-output-to-sink)\n",
    "* [repartition](#repartition)\n",
    "* [partitionBy](#partitionBy)\n",
    "* [maxRecordsPerFile](#maxRecordsPerFile)\n",
    "* [SparkSQL](#SparkSQL)\n",
    "* [Saving the data as a table](#Saving-the-data-as-a-table)\n",
    "* [bucketBy](#bucketBy)\n",
    "* [How to refer a column in a dataframe?](#How-to-refer-a-column-in-a-dataframe?)\n",
    "* [Column Expressions](#Column-Expressions)\n",
    "* [User Defined Functions in structured APIs](#User-Defined-Functions-in-structured-APIs)\n",
    "  * [Column Object expression UDF](#Column-Object-expression-UDF)\n",
    "  * [sql/string expression UDF](#sql/string-expression-UDF)\n",
    "  * [Listing the functions in spark catalog](#Listing-the-functions-in-spark-catalog)\n",
    "* [Aggregations](#Aggregations)\n",
    "  * [Simple Aggregations](#Simple-Aggregations)\n",
    "  * [Window aggregations](#Window-aggregations)\n",
    "* [Joins on dataframes](#Joins-on-dataframes)\n",
    "* [Showcasing how your code can lead to ambiguous problem](#Showcasing-how-your-code-can-lead-to-ambiguous-problem)\n",
    "* [How to deal with NULLs](#How-to-deal-with-NULLs)\n",
    "* [Internals of a normal join operations](#Internals-of-a-normal-join-operations)\n",
    "* [Broadcast Join](#Broadcast-Join)\n",
    "* [Practical - grouping on loglevel and month](#Practical---grouping-on-loglevel-and-month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae2947d",
   "metadata": {},
   "source": [
    "### Writing output to sink\n",
    "\n",
    "we've to provide a file format and if we don't provide any format it'll be parquet format.\n",
    "\n",
    "Savemodes:\n",
    "* append - Appending the file in existing directory.\n",
    "\n",
    "* Overwrite - First delete the directory if exist then create a new one.\n",
    "\n",
    "* errorIfExist - If the output directory exist it will give error.\n",
    "\n",
    "* ignore - If the output directory exist then it will ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d4295b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@1589c060\n",
       "spark = org.apache.spark.sql.SparkSession@3a759967\n",
       "ordersDf = [order_id: int, order_date: timestamp ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: timestamp ... 2 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "// Never use the inferSchema on prod as it will only read sampleset and infer the schema\n",
    "val ordersDf = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week11_practicals/orders-201019-002101.csv\")\n",
    "\n",
    "ordersDf.write\n",
    ".format(\"csv\")\n",
    ".mode(\"Overwrite\")\n",
    ".option(\"path\",\"/user/itv002768/week11_practicals/output\")\n",
    ".save()\n",
    "/*\n",
    "[itv002768@g02 ~]$ hadoop fs -ls /user/itv002768/week11_practicals/output\n",
    "Found 2 items\n",
    "-rw-r--r--   3 itv002768 supergroup          0 2022-09-08 12:31 /user/itv002768/week11_practicals/output/_SUCCESS\n",
    "-rw-r--r--   3 itv002768 supergroup    3551029 2022-09-08 12:31 /user/itv002768/week11_practicals/output/part-00000-0df75f10-3b4e-471c-93c5-7ba9f4470d14-c000.csv\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbd31062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@71408836\n",
       "spark = org.apache.spark.sql.SparkSession@3a759967\n",
       "ordersDf = [order_id: int, order_date: timestamp ... 2 more fields]\n",
       "ordersRep = [order_id: int, order_date: timestamp ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: timestamp ... 2 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "// Never use the inferSchema on prod as it will only read sampleset and infer the schema\n",
    "val ordersDf = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week11_practicals/orders-201019-002101.csv\")\n",
    "\n",
    "// For partitioning\n",
    "val ordersRep = ordersDf.repartition(4)\n",
    "\n",
    "ordersRep.write\n",
    ".format(\"csv\")\n",
    ".mode(\"Overwrite\")\n",
    ".option(\"path\",\"/user/itv002768/week11_practicals/output\")\n",
    ".save()\n",
    "/*\n",
    "[itv002768@g02 ~]$ hadoop fs -ls /user/itv002768/week11_practicals/output\n",
    "Found 5 items\n",
    "-rw-r--r--   3 itv002768 supergroup          0 2022-09-08 12:39 /user/itv002768/week11_practicals/output/_SUCCESS\n",
    "-rw-r--r--   3 itv002768 supergroup     887823 2022-09-08 12:39 /user/itv002768/week11_practicals/output/part-00000-5211d04d-b6b4-426f-bf32-a10dffeefde1-c000.csv\n",
    "-rw-r--r--   3 itv002768 supergroup     887951 2022-09-08 12:39 /user/itv002768/week11_practicals/output/part-00001-5211d04d-b6b4-426f-bf32-a10dffeefde1-c000.csv\n",
    "-rw-r--r--   3 itv002768 supergroup     887291 2022-09-08 12:39 /user/itv002768/week11_practicals/output/part-00002-5211d04d-b6b4-426f-bf32-a10dffeefde1-c000.csv\n",
    "-rw-r--r--   3 itv002768 supergroup     887964 2022-09-08 12:39 /user/itv002768/week11_practicals/output/part-00003-5211d04d-b6b4-426f-bf32-a10dffeefde1-c000.csv\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cffacc4",
   "metadata": {},
   "source": [
    "For getting the number of partitions for a dataframe we cannot use the `getNumPartitions` property directly. We've to first convert the data frame to RDD.\n",
    "\n",
    "`repartition` tries to divide the file in equal parts and this requires full shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "602b73bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordersRep.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abee96f",
   "metadata": {},
   "source": [
    "Normally when we are writing a data from to a output directory then we have few options to control spark file layouts.\n",
    "\n",
    "1. How many files? \n",
    "* `repartition` - Not a preferred choice because it requires full shuffling and we don not know what data will go in what file.\n",
    "* partitioning and Bucketing\n",
    "* sorted data - using sortBy\n",
    "\n",
    "Note: Number of output files will be equal to the number of partitions in your dataframe.\n",
    "\n",
    "### repartition\n",
    "1. It can help with the parallelism.\n",
    "2. With a normal repartition you won't be able to skip some of the partitions for performance improvement or we can say partition pruning is not possible.\n",
    "\n",
    "### partitionBy\n",
    "1. This is equivlent to partitioning in HIVE.\n",
    "2. Provides partition pruning.\n",
    "\n",
    "Please go through the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eb296c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@7ddf2c18\n",
       "spark = org.apache.spark.sql.SparkSession@3a759967\n",
       "ordersDf = [order_id: int, order_date: timestamp ... 2 more fields]\n",
       "ordersRep = [order_id: int, order_date: timestamp ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: timestamp ... 2 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "// Never use the inferSchema on prod as it will only read sampleset and infer the schema\n",
    "val ordersDf = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week11_practicals/orders-201019-002101.csv\")\n",
    "\n",
    "// For partitioning\n",
    "val ordersRep = ordersDf.repartition(4)\n",
    "\n",
    "ordersRep.write\n",
    ".format(\"csv\")\n",
    ".partitionBy(\"order_status\")\n",
    ".mode(\"Overwrite\")\n",
    ".option(\"path\",\"/user/itv002768/week11_practicals/output\")\n",
    ".save()\n",
    "/*\n",
    "[itv002768@g02 ~]$ hadoop fs -ls /user/itv002768/week11_practicals/output\n",
    "Found 10 items\n",
    "-rw-r--r--   3 itv002768 supergroup          0 2022-09-08 12:57 /user/itv002768/week11_practicals/output/_SUCCESS\n",
    "drwxr-xr-x   - itv002768 supergroup          0 2022-09-08 12:57 /user/itv002768/week11_practicals/output/order_status=CANCELED\n",
    "drwxr-xr-x   - itv002768 supergroup          0 2022-09-08 12:57 /user/itv002768/week11_practicals/output/order_status=CLOSED\n",
    "drwxr-xr-x   - itv002768 supergroup          0 2022-09-08 12:57 /user/itv002768/week11_practicals/output/order_status=COMPLETE\n",
    "drwxr-xr-x   - itv002768 supergroup          0 2022-09-08 12:57 /user/itv002768/week11_practicals/output/order_status=ON_HOLD\n",
    "drwxr-xr-x   - itv002768 supergroup          0 2022-09-08 12:57 /user/itv002768/week11_practicals/output/order_status=PAYMENT_REVIEW\n",
    "drwxr-xr-x   - itv002768 supergroup          0 2022-09-08 12:57 /user/itv002768/week11_practicals/output/order_status=PENDING\n",
    "drwxr-xr-x   - itv002768 supergroup          0 2022-09-08 12:57 /user/itv002768/week11_practicals/output/order_status=PENDING_PAYMENT\n",
    "drwxr-xr-x   - itv002768 supergroup          0 2022-09-08 12:57 /user/itv002768/week11_practicals/output/order_status=PROCESSING\n",
    "drwxr-xr-x   - itv002768 supergroup          0 2022-09-08 12:57 /user/itv002768/week11_practicals/output/order_status=SUSPECTED_FRAUD\n",
    "[itv002768@g02 ~]$ hadoop fs -ls /user/itv002768/week11_practicals/output/order_status=CLOSED\n",
    "Found 4 items\n",
    "-rw-r--r--   3 itv002768 supergroup      78170 2022-09-08 12:57 /user/itv002768/week11_practicals/output/order_status=CLOSED/part-00000-934901fd-0af7-4b24-bd1f-92a2057ee9dc.c000.csv\n",
    "-rw-r--r--   3 itv002768 supergroup      76486 2022-09-08 12:57 /user/itv002768/week11_practicals/output/order_status=CLOSED/part-00001-934901fd-0af7-4b24-bd1f-92a2057ee9dc.c000.csv\n",
    "-rw-r--r--   3 itv002768 supergroup      76880 2022-09-08 12:57 /user/itv002768/week11_practicals/output/order_status=CLOSED/part-00002-934901fd-0af7-4b24-bd1f-92a2057ee9dc.c000.csv\n",
    "-rw-r--r--   3 itv002768 supergroup      77850 2022-09-08 12:57 /user/itv002768/week11_practicals/output/order_status=CLOSED/part-00003-934901fd-0af7-4b24-bd1f-92a2057ee9dc.c000.csv\n",
    "\n",
    "[itv002768@g02 ~]$ hadoop fs -head /user/itv002768/week11_practicals/output/order_status=CLOSED/part-00003-934901fd-0af7-4b24-bd1f-92a2057ee9dc.c000.csv\n",
    "13151,2013-10-13T00:00:00.000-04:00,5736\n",
    "2923,2013-08-10T00:00:00.000-04:00,6362\n",
    "7765,2013-09-10T00:00:00.000-04:00,12248\n",
    "33538,2014-02-16T00:00:00.000-05:00,10299\n",
    "57250,2014-07-21T00:00:00.000-04:00,4775\n",
    "68291,2014-03-30T00:00:00.000-04:00,4470\n",
    "38665,2014-03-20T00:00:00.000-04:00,8719\n",
    "42245,2014-04-11T00:00:00.000-04:00,8680\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7849b5a5",
   "metadata": {},
   "source": [
    "### maxRecordsPerFile\n",
    "* It divides number of files based on max records to be accomodated in a file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ed10862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@405c9c83\n",
       "spark = org.apache.spark.sql.SparkSession@3a759967\n",
       "ordersDf = [order_id: int, order_date: timestamp ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: timestamp ... 2 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "// Never use the inferSchema on prod as it will only read sampleset and infer the schema\n",
    "val ordersDf = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week11_practicals/orders-201019-002101.csv\")\n",
    "\n",
    "ordersDf.write\n",
    ".format(\"csv\")\n",
    ".option(\"maxRecordsPerFile\", 2000)\n",
    ".mode(\"Overwrite\")\n",
    ".option(\"path\",\"/user/itv002768/week11_practicals/output\")\n",
    ".save()\n",
    "\n",
    "/*\n",
    "[itv002768@g02 ~]$ hadoop fs -ls /user/itv002768/week11_practicals/output | wc -l\n",
    "37\n",
    "\n",
    "[itv002768@g02 ~]$ hadoop fs -cat /user/itv002768/week11_practicals/output/part-00000-e9a52b95-41f0-4065-8e6f-e0feab43003b-c032.csv | wc -l\n",
    "2000\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2c52d5",
   "metadata": {},
   "source": [
    "AVRO is an external format and is not supported be defaule like csv, json or parquet. You've to download a jar for spark-avro-version and load it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b81bfc",
   "metadata": {},
   "source": [
    "### SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20ef7cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@3794482c\n",
       "spark = org.apache.spark.sql.SparkSession@3a759967\n",
       "ordersDf = [order_id: int, order_date: timestamp ... 2 more fields]\n",
       "resultDf = [order_id: int, order_date: timestamp ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: timestamp ... 2 more fields]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "// Never use the inferSchema on prod as it will only read sampleset and infer the schema\n",
    "val ordersDf = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week11_practicals/orders-201019-002101.csv\")\n",
    "\n",
    "// creating a table orders out of the dataframe ordersDF\n",
    "ordersDf.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "val resultDf = spark.sql(\"select * from orders limit 10\")\n",
    "resultDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6771605d",
   "metadata": {},
   "source": [
    "### Saving the data as a table\n",
    "Sometimes there is a requirement to store the data as persistent table so that we can connect externale tools like tableu or powerbi for reporting purpose.\n",
    "\n",
    "Table has two parts:\n",
    "- Data - It is stored in spark warehouse directly.\n",
    "\n",
    "- Metadata/Schema - It is stored in catalog metastore. It is store in memory. In this case if we'll terminate the application it is gone. We can use Hive metastore to store spark metadata. Use `enableHiveSupport` function to store the metadata in hive.\n",
    "\n",
    "By default, table will be created in default database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "517858fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: org.apache.hadoop.security.AccessControlException Permission denied: user=itv002768, access=WRITE, inode=\"/user\":hdfs:supergroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:496)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:336)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:360)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:239)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1909)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1893)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1852)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3407)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1161)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:739)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:532)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1020)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:948)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2952)\n);",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: org.apache.hadoop.security.AccessControlException Permission denied: user=itv002768, access=WRITE, inode=\"/user\":hdfs:supergroup:drwxr-xr-x",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:496)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:336)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:360)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:239)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1909)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1893)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1852)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3407)",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1161)",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:739)",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:532)",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1020)",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:948)",
      "\tat java.security.AccessController.doPrivileged(Native Method)",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2952)",
      ");",
      "  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:106)",
      "  at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:183)",
      "  at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createDatabase(ExternalCatalogWithListener.scala:47)",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:211)",
      "  at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:70)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)",
      "  at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)",
      "  at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)",
      "  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)",
      "  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)",
      "  ... 42 elided",
      "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: org.apache.hadoop.security.AccessControlException Permission denied: user=itv002768, access=WRITE, inode=\"/user\":hdfs:supergroup:drwxr-xr-x",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:496)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:336)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:360)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:239)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1909)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1893)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1852)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3407)",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1161)",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:739)",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:532)",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1020)",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:948)",
      "\tat java.security.AccessController.doPrivileged(Native Method)",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2952)",
      ")",
      "  at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:312)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:314)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:314)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:314)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:277)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:215)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:214)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:260)",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:313)",
      "  at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:184)",
      "  at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:184)",
      "  at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:184)",
      "  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)",
      "  ... 59 more",
      "Caused by: org.apache.hadoop.hive.metastore.api.MetaException: Got exception: org.apache.hadoop.security.AccessControlException Permission denied: user=itv002768, access=WRITE, inode=\"/user\":hdfs:supergroup:drwxr-xr-x",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:496)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:336)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:360)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:239)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1909)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1893)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1852)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3407)",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1161)",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:739)",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:532)",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1020)",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:948)",
      "\tat java.security.AccessController.doPrivileged(Native Method)",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2952)",
      "  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_database_result$create_database_resultStandardScheme.read(ThriftHiveMetastore.java:14412)",
      "  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_database_result$create_database_resultStandardScheme.read(ThriftHiveMetastore.java:14380)",
      "  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_database_result.read(ThriftHiveMetastore.java:14314)",
      "  at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)",
      "  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_create_database(ThriftHiveMetastore.java:625)",
      "  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.create_database(ThriftHiveMetastore.java:612)",
      "  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)",
      "  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "  at java.lang.reflect.Method.invoke(Method.java:498)",
      "  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)",
      "  at com.sun.proxy.$Proxy56.createDatabase(Unknown Source)",
      "  at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)",
      "  ... 71 more"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "val spark = SparkSession.builder().config(sparkConfig)\n",
    "                        .config(\"spark.ui.port\", \"0\")\n",
    "                        .config(\"spark.sql.warehouse.dir\", \"/user/${username}/warehouse\").master(\"yarn\")\n",
    "                        .appName(\"${username} | spark_sql\")\n",
    "                        .enableHiveSupport().getOrCreate()\n",
    "\n",
    "// Never use the inferSchema on prod as it will only read sampleset and infer the schema\n",
    "val ordersDf = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week11_practicals/orders-201019-002101.csv\")\n",
    "spark.sql(\"create database tushar_retail\")\n",
    "\n",
    "ordersDf.write\n",
    ".format(\"csv\")\n",
    ".mode(SaveMode.Overwrite)\n",
    ".saveAsTable(\"tushar_retail.orders_11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbb20a6",
   "metadata": {},
   "source": [
    "### bucketBy\n",
    "\n",
    "It only works when we save the data as table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226cc9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "val spark = SparkSession.builder().config(sparkConfig)\n",
    "                        .config(\"spark.ui.port\", \"0\")\n",
    "                        .config(\"spark.sql.warehouse.dir\", \"/user/${username}/warehouse\").master(\"yarn\")\n",
    "                        .appName(\"${username} | spark_sql\")\n",
    "                        .enableHiveSupport().getOrCreate()\n",
    "\n",
    "// Never use the inferSchema on prod as it will only read sampleset and infer the schema\n",
    "val ordersDf = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week11_practicals/orders-201019-002101.csv\")\n",
    "spark.sql(\"create database tushar_retail\")\n",
    "\n",
    "ordersDf.write\n",
    ".format(\"csv\")\n",
    ".mode(SaveMode.Overwrite)\n",
    ".bucketBy(4, \"order_id\")\n",
    ".sortBy()\n",
    ".saveAsTable(\"tushar_retail.orders_11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a79b5c8",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "\n",
    "1. Low level transformations</BR>\n",
    "**map, filter, groupByKey etc.**\n",
    "\n",
    "We can perform these using raw rdds, also some of these are even possible with dataframes and datasets.\n",
    "\n",
    "2. High level transformations</BR>\n",
    "**select, where, groupby etc.**\n",
    "\n",
    "These are supported by dataframs and datasets.\n",
    "\n",
    "\n",
    "```\n",
    "1 2022-01-02   112122,CLOSED\n",
    "2 2022-01-03   112422,START\n",
    "3 2022-01-04   112222,CLOSED\n",
    "4 2022-01-05   112622,CLOSED\n",
    "```\n",
    "In case of an ustructured file, we'll load it as a raw rdd. Each row will be of string type and we'll use a map transformation that is a low level transformation.\n",
    "\n",
    "Here, we can use the regular expression in map and then we can use a case class for to associate a structure. Then we can convert the rdd to a dataset using `.toDS`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "266eb705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------+\n",
      "|order_id|customer_id|order_status|\n",
      "+--------+-----------+------------+\n",
      "|       1|     112122|      CLOSED|\n",
      "|       2|     112422|       START|\n",
      "|       3|     112222|      CLOSED|\n",
      "|       4|     112622|      CLOSED|\n",
      "+--------+-----------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "stringRegex = ^(\\S+) (\\S+)\\t(\\S+)\\,(\\S+)\n",
       "defined class Orders\n",
       "sparkConfig = org.apache.spark.SparkConf@df88ec3\n",
       "spark = org.apache.spark.sql.SparkSession@1b767de0\n",
       "input = /user/itv002768/week12_practicals/regex_example.txt MapPartitionsRDD[1] at textFile at <console>:36\n",
       "parserOutput = [order_id: int, customer_id: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "parser: (line: String)Orders\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, customer_id: int ... 1 more field]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val stringRegex = \"\"\"^(\\S+) (\\S+)\\t(\\S+)\\,(\\S+)\"\"\".r\n",
    "\n",
    "case class Orders(order_id: Int, customer_id: Int, order_status: String)\n",
    "\n",
    "def parser(line: String) = {\n",
    "    line match {\n",
    "        case stringRegex(order_id, date, customer_id, order_status) => \n",
    "        Orders(order_id.toInt, customer_id.toInt, order_status)\n",
    "    }\n",
    "}\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "val input = spark.sparkContext.textFile(\"/user/itv002768/week12_practicals/regex_example.txt\")\n",
    "\n",
    "import spark.implicits._\n",
    "val parserOutput = input.map(parser).toDS()\n",
    "parserOutput.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b000a",
   "metadata": {},
   "source": [
    "### How to refer a column in a dataframe?\n",
    "\n",
    "1. Column String\n",
    "\n",
    "This is the easiest way and you can access the column using the column name. Please refer the below code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b177e7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+---------------+\n",
      "|order_id|   order_status|\n",
      "+--------+---------------+\n",
      "|       1|         CLOSED|\n",
      "|       2|PENDING_PAYMENT|\n",
      "|       3|       COMPLETE|\n",
      "|       4|         CLOSED|\n",
      "|       5|       COMPLETE|\n",
      "|       6|       COMPLETE|\n",
      "|       7|       COMPLETE|\n",
      "|       8|     PROCESSING|\n",
      "|       9|PENDING_PAYMENT|\n",
      "|      10|PENDING_PAYMENT|\n",
      "|      11| PAYMENT_REVIEW|\n",
      "|      12|         CLOSED|\n",
      "|      13|PENDING_PAYMENT|\n",
      "|      14|     PROCESSING|\n",
      "|      15|       COMPLETE|\n",
      "|      16|PENDING_PAYMENT|\n",
      "|      17|       COMPLETE|\n",
      "|      18|         CLOSED|\n",
      "|      19|PENDING_PAYMENT|\n",
      "|      20|     PROCESSING|\n",
      "+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@1ade4393\n",
       "spark = org.apache.spark.sql.SparkSession@1b767de0\n",
       "ordersDf = [order_id: int, order_date: timestamp ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: timestamp ... 2 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "// Never use the inferSchema on prod as it will only read sampleset and infer the schema\n",
    "val ordersDf = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week11_practicals/orders-201019-002101.csv\")\n",
    "\n",
    "ordersDf.show()\n",
    "ordersDf.select(\"order_id\", \"order_status\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8e76f",
   "metadata": {},
   "source": [
    "2. Column Object\n",
    "\n",
    "You can address a column using the column or col function and it is generic that can used in pyspark, scala and spark.\n",
    "\n",
    "Scala specific - $\"order_id\" or 'order_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99d14e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-------------------+---------------+-----------------+\n",
      "|order_id|         order_date|   order_status|order_customer_id|\n",
      "+--------+-------------------+---------------+-----------------+\n",
      "|       1|2013-07-25 00:00:00|         CLOSED|            11599|\n",
      "|       2|2013-07-25 00:00:00|PENDING_PAYMENT|              256|\n",
      "|       3|2013-07-25 00:00:00|       COMPLETE|            12111|\n",
      "|       4|2013-07-25 00:00:00|         CLOSED|             8827|\n",
      "|       5|2013-07-25 00:00:00|       COMPLETE|            11318|\n",
      "|       6|2013-07-25 00:00:00|       COMPLETE|             7130|\n",
      "|       7|2013-07-25 00:00:00|       COMPLETE|             4530|\n",
      "|       8|2013-07-25 00:00:00|     PROCESSING|             2911|\n",
      "|       9|2013-07-25 00:00:00|PENDING_PAYMENT|             5657|\n",
      "|      10|2013-07-25 00:00:00|PENDING_PAYMENT|             5648|\n",
      "|      11|2013-07-25 00:00:00| PAYMENT_REVIEW|              918|\n",
      "|      12|2013-07-25 00:00:00|         CLOSED|             1837|\n",
      "|      13|2013-07-25 00:00:00|PENDING_PAYMENT|             9149|\n",
      "|      14|2013-07-25 00:00:00|     PROCESSING|             9842|\n",
      "|      15|2013-07-25 00:00:00|       COMPLETE|             2568|\n",
      "|      16|2013-07-25 00:00:00|PENDING_PAYMENT|             7276|\n",
      "|      17|2013-07-25 00:00:00|       COMPLETE|             2667|\n",
      "|      18|2013-07-25 00:00:00|         CLOSED|             1205|\n",
      "|      19|2013-07-25 00:00:00|PENDING_PAYMENT|             9488|\n",
      "|      20|2013-07-25 00:00:00|     PROCESSING|             9198|\n",
      "+--------+-------------------+---------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@2d1a8389\n",
       "spark = org.apache.spark.sql.SparkSession@1b767de0\n",
       "ordersDf = [order_id: int, order_date: timestamp ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: timestamp ... 2 more fields]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.functions.column\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "// Never use the inferSchema on prod as it will only read sampleset and infer the schema\n",
    "val ordersDf = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week11_practicals/orders-201019-002101.csv\")\n",
    "\n",
    "ordersDf.show()\n",
    "\n",
    "import spark.implicits._\n",
    "ordersDf.select(column(\"order_id\"), col(\"order_date\"), $\"order_status\", 'order_customer_id).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c4d117",
   "metadata": {},
   "source": [
    "### Column Expressions\n",
    "\n",
    "* We cannot mix column object with column string.</BR>\n",
    "* we cannot mix column object with column expression.</BR>\n",
    "* we cannot mix column string with column expression.</BR>\n",
    "There is a way to convert column expression to a column object by passing it to a function `expr`.</BR>\n",
    "\n",
    "using `selectExpr` we can use column string with column object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42f02d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-----------------------------+\n",
      "|order_id|concat(order_status, _STATUS)|\n",
      "+--------+-----------------------------+\n",
      "|1       |CLOSED_STATUS                |\n",
      "|2       |PENDING_PAYMENT_STATUS       |\n",
      "|3       |COMPLETE_STATUS              |\n",
      "|4       |CLOSED_STATUS                |\n",
      "|5       |COMPLETE_STATUS              |\n",
      "|6       |COMPLETE_STATUS              |\n",
      "|7       |COMPLETE_STATUS              |\n",
      "|8       |PROCESSING_STATUS            |\n",
      "|9       |PENDING_PAYMENT_STATUS       |\n",
      "|10      |PENDING_PAYMENT_STATUS       |\n",
      "|11      |PAYMENT_REVIEW_STATUS        |\n",
      "|12      |CLOSED_STATUS                |\n",
      "|13      |PENDING_PAYMENT_STATUS       |\n",
      "|14      |PROCESSING_STATUS            |\n",
      "|15      |COMPLETE_STATUS              |\n",
      "|16      |PENDING_PAYMENT_STATUS       |\n",
      "|17      |COMPLETE_STATUS              |\n",
      "|18      |CLOSED_STATUS                |\n",
      "|19      |PENDING_PAYMENT_STATUS       |\n",
      "|20      |PROCESSING_STATUS            |\n",
      "+--------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-----------------------------+\n",
      "|order_id|concat(order_status, _STATUS)|\n",
      "+--------+-----------------------------+\n",
      "|1       |CLOSED_STATUS                |\n",
      "|2       |PENDING_PAYMENT_STATUS       |\n",
      "|3       |COMPLETE_STATUS              |\n",
      "|4       |CLOSED_STATUS                |\n",
      "|5       |COMPLETE_STATUS              |\n",
      "|6       |COMPLETE_STATUS              |\n",
      "|7       |COMPLETE_STATUS              |\n",
      "|8       |PROCESSING_STATUS            |\n",
      "|9       |PENDING_PAYMENT_STATUS       |\n",
      "|10      |PENDING_PAYMENT_STATUS       |\n",
      "|11      |PAYMENT_REVIEW_STATUS        |\n",
      "|12      |CLOSED_STATUS                |\n",
      "|13      |PENDING_PAYMENT_STATUS       |\n",
      "|14      |PROCESSING_STATUS            |\n",
      "|15      |COMPLETE_STATUS              |\n",
      "|16      |PENDING_PAYMENT_STATUS       |\n",
      "|17      |COMPLETE_STATUS              |\n",
      "|18      |CLOSED_STATUS                |\n",
      "|19      |PENDING_PAYMENT_STATUS       |\n",
      "|20      |PROCESSING_STATUS            |\n",
      "+--------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@382f0585\n",
       "spark = org.apache.spark.sql.SparkSession@1b767de0\n",
       "ordersDf = [order_id: int, order_date: timestamp ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: timestamp ... 2 more fields]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.functions.column\n",
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "// Never use the inferSchema on prod as it will only read sampleset and infer the schema\n",
    "val ordersDf = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week11_practicals/orders-201019-002101.csv\")\n",
    "\n",
    "ordersDf.show()\n",
    "// We cannot mix column strings with column expressions the below line will give error\n",
    "//ordersDf.select(\"order_id\", \"concat(order_status, '_STATUS')\").show()\n",
    "ordersDf.select($\"order_id\", expr(\"concat(order_status, '_STATUS')\")).show(false)\n",
    "ordersDf.selectExpr(\"order_id\", \"concat(order_status, '_STATUS')\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8937ee",
   "metadata": {},
   "source": [
    "### User Defined Functions in structured APIs\n",
    "\n",
    "#### Column Object expression UDF\n",
    "\n",
    "In case ot datasets we've to create a function and the register it. Whenever we want to add a new column we can use `.withColumn`.\n",
    "\n",
    "When we register a function it is registered with the driver, the driver will serialize the function and send it to each executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "530419d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|    _c0|_c1|      _c2|\n",
      "+-------+---+---------+\n",
      "|  sumit| 30|bangalore|\n",
      "|  kapil| 32|hyderabad|\n",
      "|sathish| 16|  chennai|\n",
      "|   ravi| 39|bangalore|\n",
      "| kavita| 12|hyderabad|\n",
      "|  kavya| 19|   mysore|\n",
      "+-------+---+---------+\n",
      "\n",
      "+-------+---+---------+-----+\n",
      "|   name|age|     city|adult|\n",
      "+-------+---+---------+-----+\n",
      "|  sumit| 30|bangalore|    Y|\n",
      "|  kapil| 32|hyderabad|    Y|\n",
      "|sathish| 16|  chennai|    N|\n",
      "|   ravi| 39|bangalore|    Y|\n",
      "| kavita| 12|hyderabad|    N|\n",
      "|  kavya| 19|   mysore|    Y|\n",
      "+-------+---+---------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined class AgeInfo\n",
       "sparkConfig = org.apache.spark.SparkConf@7751241d\n",
       "spark = org.apache.spark.sql.SparkSession@1b767de0\n",
       "ageDf = [_c0: string, _c1: int ... 1 more field]\n",
       "ageDf_ = [name: string, age: int ... 1 more field]\n",
       "parseAgeFunction = UserDefinedFunction(<function1>,StringType,Some(List(IntegerType)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "ageCheck: (age: Int)String\n",
       "final_: o...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function1>,StringType,Some(List(IntegerType)))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.functions.column\n",
    "import org.apache.spark.sql.functions.expr\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "case class AgeInfo(name: String, age: Int, city: String)\n",
    "\n",
    "def ageCheck(age: Int)={\n",
    "    if(age>18) \"Y\" else \"N\"\n",
    "}\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "// Never use the inferSchema on prod as it will only read sampleset and infer the schema\n",
    "val ageDf = spark.read.option(\"header\", false)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week12_practicals/agedataset\")\n",
    "\n",
    "ageDf.show()\n",
    "\n",
    "val ageDf_ = ageDf.toDF(\"name\", \"age\", \"city\")\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "//converting it to a dataset\n",
    "ageDf_.as[AgeInfo]\n",
    "\n",
    "//Registering the function\n",
    "val parseAgeFunction = udf(ageCheck(_: Int): String)\n",
    "\n",
    "val final_ = ageDf_.withColumn(\"adult\", parseAgeFunction(column(\"age\")))\n",
    "\n",
    "final_.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d5fd3e",
   "metadata": {},
   "source": [
    "#### sql/string expression UDF\n",
    "\n",
    "Here, we'll try to register a function using sql. \n",
    "\n",
    "`spark.udf.register(\"parseAgeFunction\", ageCheck(_: Int): String)`\n",
    "\n",
    "We can also use this function witn `spark.sql` after creating the table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "276b311b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|    _c0|_c1|      _c2|\n",
      "+-------+---+---------+\n",
      "|  sumit| 30|bangalore|\n",
      "|  kapil| 32|hyderabad|\n",
      "|sathish| 16|  chennai|\n",
      "|   ravi| 39|bangalore|\n",
      "| kavita| 12|hyderabad|\n",
      "|  kavya| 19|   mysore|\n",
      "+-------+---+---------+\n",
      "\n",
      "+-------+---+---------+-----+\n",
      "|   name|age|     city|adult|\n",
      "+-------+---+---------+-----+\n",
      "|  sumit| 30|bangalore|    Y|\n",
      "|  kapil| 32|hyderabad|    Y|\n",
      "|sathish| 16|  chennai|    N|\n",
      "|   ravi| 39|bangalore|    Y|\n",
      "| kavita| 12|hyderabad|    N|\n",
      "|  kavya| 19|   mysore|    Y|\n",
      "+-------+---+---------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined class AgeInfo\n",
       "sparkConfig = org.apache.spark.SparkConf@69d1bd2\n",
       "spark = org.apache.spark.sql.SparkSession@1b767de0\n",
       "ageDf = [_c0: string, _c1: int ... 1 more field]\n",
       "ageDf_ = [name: string, age: int ... 1 more field]\n",
       "final_ = [name: string, age: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n",
       "ageCheck: (age: Int)String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[name: string, age: int ... 2 more fields]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.functions.column\n",
    "import org.apache.spark.sql.functions.expr\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "case class AgeInfo(name: String, age: Int, city: String)\n",
    "\n",
    "def ageCheck(age: Int)={\n",
    "    if(age>18) \"Y\" else \"N\"\n",
    "}\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "// Never use the inferSchema on prod as it will only read sampleset and infer the schema\n",
    "val ageDf = spark.read.option(\"header\", false)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week12_practicals/agedataset\")\n",
    "\n",
    "ageDf.show()\n",
    "\n",
    "val ageDf_ = ageDf.toDF(\"name\", \"age\", \"city\")\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "//converting it to a dataset\n",
    "ageDf_.as[AgeInfo]\n",
    "\n",
    "spark.udf.register(\"parseAgeFunction\", ageCheck(_: Int): String)\n",
    "\n",
    "val final_ = ageDf_.withColumn(\"adult\", expr(\"parseAgeFunction(age)\"))\n",
    "final_.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af51fd4f",
   "metadata": {},
   "source": [
    "#### Listing the functions in spark catalog\n",
    "\n",
    "If a function is registered using `spark.udf.register` only that will be shown in the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0058063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+--------------------+-----------+\n",
      "|      name|database|description|           className|isTemporary|\n",
      "+----------+--------+-----------+--------------------+-----------+\n",
      "|         !|    null|       null|org.apache.spark....|       true|\n",
      "|         %|    null|       null|org.apache.spark....|       true|\n",
      "|         &|    null|       null|org.apache.spark....|       true|\n",
      "|         *|    null|       null|org.apache.spark....|       true|\n",
      "|         +|    null|       null|org.apache.spark....|       true|\n",
      "|         -|    null|       null|org.apache.spark....|       true|\n",
      "|         /|    null|       null|org.apache.spark....|       true|\n",
      "|         <|    null|       null|org.apache.spark....|       true|\n",
      "|        <=|    null|       null|org.apache.spark....|       true|\n",
      "|       <=>|    null|       null|org.apache.spark....|       true|\n",
      "|         =|    null|       null|org.apache.spark....|       true|\n",
      "|        ==|    null|       null|org.apache.spark....|       true|\n",
      "|         >|    null|       null|org.apache.spark....|       true|\n",
      "|        >=|    null|       null|org.apache.spark....|       true|\n",
      "|         ^|    null|       null|org.apache.spark....|       true|\n",
      "|       abs|    null|       null|org.apache.spark....|       true|\n",
      "|      acos|    null|       null|org.apache.spark....|       true|\n",
      "|add_months|    null|       null|org.apache.spark....|       true|\n",
      "| aggregate|    null|       null|org.apache.spark....|       true|\n",
      "|       and|    null|       null|org.apache.spark....|       true|\n",
      "+----------+--------+-----------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+--------+-----------+---------+-----------+\n",
      "|            name|database|description|className|isTemporary|\n",
      "+----------------+--------+-----------+---------+-----------+\n",
      "|parseAgeFunction|    null|       null|     null|       true|\n",
      "+----------------+--------+-----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//to show all functions\n",
    "spark.catalog.listFunctions().show()\n",
    "\n",
    "spark.catalog.listFunctions().filter(x => x.name == \"parseAgeFunction\" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d6e7a",
   "metadata": {},
   "source": [
    "### Practical on orders data\n",
    "1. Create a scala list from sample data.\n",
    "2. from a scala list create a dataframe with cols orderid, orderdate, customerid, status.\n",
    "3. convert orderdate from timestamp to unixtimestamp.\n",
    "4. Create a new column newid and make sure it has unique ids.\n",
    "5. Drop duplicated based on columns(orderdate, customerid).\n",
    "6. drop orderid column.\n",
    "7. sort it based on orderdate\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5082d88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------------+-----+\n",
      "| orderdate|customerid|         status|newid|\n",
      "+----------+----------+---------------+-----+\n",
      "|1654920000|      8827|         CLOSED|    3|\n",
      "|1657512000|       256|PAYMENT_PENDING|    1|\n",
      "|1657512000|       115|         CLOSED|    0|\n",
      "+----------+----------+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@3cb496a3\n",
       "spark = org.apache.spark.sql.SparkSession@1b767de0\n",
       "myList = List((1,2022-07-11,115,CLOSED), (2,2022-07-11,256,PAYMENT_PENDING), (3,2022-07-11,115,COMPLETED), (4,2022-06-11,8827,CLOSED))\n",
       "ordersDf = [orderid: int, orderdate: string ... 2 more fields]\n",
       "ordersDfUnix = [orderdate: bigint, customerid: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[orderdate: bigint, customerid: int ... 2 more fields]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.functions.column\n",
    "import org.apache.spark.sql.functions.expr\n",
    "import org.apache.spark.sql.types.DateType\n",
    "\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "// create a scala list\n",
    "val myList = List(\n",
    "    (1, \"2022-07-11\", 115, \"CLOSED\"),\n",
    "    (2, \"2022-07-11\", 256, \"PAYMENT_PENDING\"),\n",
    "    (3, \"2022-07-11\", 115, \"COMPLETED\"),\n",
    "    (4, \"2022-06-11\", 8827, \"CLOSED\")\n",
    ")\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "// create a DF out of list\n",
    "val ordersDf = spark.createDataFrame(myList)\n",
    "            .toDF(\"orderid\", \"orderdate\", \"customerid\", \"status\")\n",
    "\n",
    "\n",
    "// Convert orderdate to unixtimestamp\n",
    "// create a new column newid having only unique ids\n",
    "// Drop the duplicates\n",
    "// Drop column orderid\n",
    "// sort based on orderdate\n",
    "val ordersDfUnix = ordersDf.withColumn(\"orderdate\", unix_timestamp(col(\"orderdate\").cast(DateType)))\n",
    "                .withColumn(\"newid\", monotonically_increasing_id)\n",
    "                .dropDuplicates(\"orderdate\", \"customerid\")\n",
    "                .drop(\"orderid\")\n",
    "                .sort(\"orderdate\")\n",
    "ordersDfUnix.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f14dc",
   "metadata": {},
   "source": [
    "### Aggregations\n",
    "### Simple Aggregations\n",
    "\n",
    "* After doing the aggregations when we get the single record e.g Total number of records, sum of all quantities.\n",
    "\n",
    "\n",
    "In the below code:\n",
    "- totalNumberOfRows\n",
    "- totalQuantity\n",
    "- avgUnitPrice\n",
    "- numUniqueInvoices\n",
    "\n",
    "we'll use the column expression, string expression and spark sql.\n",
    "\n",
    "\n",
    "2. Grouped Aggregations\n",
    "\n",
    "* How many Items are there in each invoice number\n",
    "- group data based on country and invoice number(total quantity for each group, sum of invoice value)\n",
    "\n",
    "3. window aggregated\n",
    "\n",
    "* Sale happened in last n days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f856a2da",
   "metadata": {},
   "source": [
    "Each line item corrosponds to a different kind of thing.\n",
    "\n",
    "\n",
    "```\n",
    "[itv002768@g02 ~]$ hadoop fs -head /user/itv002768/week12_practicals/order_data-201025-223502.csv\n",
    "﻿InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country\n",
    "536378,,PACK OF 60 DINOSAUR CAKE CASES,24,01-12-2010 9.37,0.55,14688,United Kingdom\n",
    "536378,,PACK OF 60 PINK PAISLEY CAKE CASES,24,01-12-2010 9.37,0.55,14688,United Kingdom\n",
    "536378,84991,60 TEATIME FAIRY CAKE CASES,24,01-12-2010 9.37,0.55,14688,United Kingdom\n",
    "536378,84519A,TOMATO CHARLIE+LOLA COASTER SET,6,01-12-2010 9.37,2.95,14688,United Kingdom\n",
    "536378,85183B,CHARLIE & LOLA WASTEPAPER BIN FLORA,48,01-12-2010 9.37,1.25,14688,United Kingdom\n",
    "536378,85071B,RED CHARLIE+LOLA PERSONAL DOORSIGN,96,01-12-2010 9.37,0.38,14688,United Kingdom\n",
    "536378,21931,JUMBO STORAGE BAG SUKI,10,01-12-2010 9.37,1.95,14688,United Kingdom\n",
    "536378,21929,JUMBO BAG PINK VINTAGE PAISLEY,10,01-12-2010 9.37,1.95,14688,United Kingdom\n",
    "536380,22961,JAM MAKING SET PRINTED,24,01-12-2010 9.41,1.45,17809,United Kingdom\n",
    "536381,22139,RETROSPOT TEA SET CERAMIC 11 PC ,23,01-12-2010 9.41,4.25,15311,United Kingdom\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9f2e7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-----------------+-----------------+\n",
      "|totalRowCount|totalQuantity|     avgUnitPrice|numUniqueInvoices|\n",
      "+-------------+-------------+-----------------+-----------------+\n",
      "|       541782|      5175855|4.611565323321927|            25858|\n",
      "+-------------+-------------+-----------------+-----------------+\n",
      "\n",
      "+--------------+---------+-------------+------------------+\n",
      "|       Country|InvoiceNo|TotalQuantity|      InvoiceValue|\n",
      "+--------------+---------+-------------+------------------+\n",
      "|United Kingdom|   536446|          329|            440.89|\n",
      "|United Kingdom|   536508|          216|            155.52|\n",
      "|United Kingdom|   537811|           74|            268.86|\n",
      "|United Kingdom|   538895|          370|            247.38|\n",
      "|United Kingdom|   540453|          341|302.44999999999993|\n",
      "|United Kingdom|   541291|          217|305.81000000000006|\n",
      "|United Kingdom|   542551|           -1|               0.0|\n",
      "|United Kingdom|   542576|           -1|               0.0|\n",
      "|United Kingdom|   542628|            9|            132.35|\n",
      "|United Kingdom|   542886|          199| 320.5099999999998|\n",
      "|United Kingdom|   542907|           75|            313.85|\n",
      "|United Kingdom|   543131|          134|             164.1|\n",
      "|United Kingdom|   543189|          102|            153.94|\n",
      "|United Kingdom|   543265|           -4|               0.0|\n",
      "|        Cyprus|   544574|          173|            320.69|\n",
      "|United Kingdom|   545077|           24|             10.08|\n",
      "|United Kingdom|   545300|          116|            323.16|\n",
      "|United Kingdom|   545347|           72| 76.32000000000001|\n",
      "|United Kingdom|   545418|           10|              85.0|\n",
      "|United Kingdom|   545897|          577|1762.2200000000018|\n",
      "+--------------+---------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+-------------+-----------------+-----------------+\n",
      "|totalRowCount|totalQuantity|     avgUnitPrice|numUniqueInvoices|\n",
      "+-------------+-------------+-----------------+-----------------+\n",
      "|       541782|      5175855|4.611565323321927|            25858|\n",
      "+-------------+-------------+-----------------+-----------------+\n",
      "\n",
      "+--------------+---------+-------------+------------------+\n",
      "|       Country|InvoiceNo|totalQuantity|      InvoiceValue|\n",
      "+--------------+---------+-------------+------------------+\n",
      "|United Kingdom|   536446|          329|            440.89|\n",
      "|United Kingdom|   536508|          216|            155.52|\n",
      "|United Kingdom|   537811|           74|            268.86|\n",
      "|United Kingdom|   538895|          370|            247.38|\n",
      "|United Kingdom|   540453|          341|302.44999999999993|\n",
      "|United Kingdom|   541291|          217|305.81000000000006|\n",
      "|United Kingdom|   542551|           -1|               0.0|\n",
      "|United Kingdom|   542576|           -1|               0.0|\n",
      "|United Kingdom|   542628|            9|            132.35|\n",
      "|United Kingdom|   542886|          199| 320.5099999999998|\n",
      "|United Kingdom|   542907|           75|            313.85|\n",
      "|United Kingdom|   543131|          134|             164.1|\n",
      "|United Kingdom|   543189|          102|            153.94|\n",
      "|United Kingdom|   543265|           -4|               0.0|\n",
      "|        Cyprus|   544574|          173|            320.69|\n",
      "|United Kingdom|   545077|           24|             10.08|\n",
      "|United Kingdom|   545300|          116|            323.16|\n",
      "|United Kingdom|   545347|           72| 76.32000000000001|\n",
      "|United Kingdom|   545418|           10|              85.0|\n",
      "|United Kingdom|   545897|          577|1762.2200000000018|\n",
      "+--------------+---------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-------------+-----------------+-------------------------+\n",
      "|count(1)|sum(Quantity)|   avg(UnitPrice)|count(DISTINCT InvoiceNo)|\n",
      "+--------+-------------+-----------------+-------------------------+\n",
      "|  541782|      5175855|4.611565323321927|                    25858|\n",
      "+--------+-------------+-----------------+-------------------------+\n",
      "\n",
      "+--------------+---------+-------------+-------------------------------------------+\n",
      "|       Country|InvoiceNo|sum(Quantity)|sum((CAST(Quantity AS DOUBLE) * UnitPrice))|\n",
      "+--------------+---------+-------------+-------------------------------------------+\n",
      "|United Kingdom|   536446|          329|                                     440.89|\n",
      "|United Kingdom|   536508|          216|                                     155.52|\n",
      "|United Kingdom|   537811|           74|                                     268.86|\n",
      "|United Kingdom|   538895|          370|                                     247.38|\n",
      "|United Kingdom|   540453|          341|                         302.44999999999993|\n",
      "|United Kingdom|   541291|          217|                         305.81000000000006|\n",
      "|United Kingdom|   542551|           -1|                                        0.0|\n",
      "|United Kingdom|   542576|           -1|                                        0.0|\n",
      "|United Kingdom|   542628|            9|                                     132.35|\n",
      "|United Kingdom|   542886|          199|                          320.5099999999998|\n",
      "|United Kingdom|   542907|           75|                                     313.85|\n",
      "|United Kingdom|   543131|          134|                                      164.1|\n",
      "|United Kingdom|   543189|          102|                                     153.94|\n",
      "|United Kingdom|   543265|           -4|                                        0.0|\n",
      "|        Cyprus|   544574|          173|                                     320.69|\n",
      "|United Kingdom|   545077|           24|                                      10.08|\n",
      "|United Kingdom|   545300|          116|                                     323.16|\n",
      "|United Kingdom|   545347|           72|                          76.32000000000001|\n",
      "|United Kingdom|   545418|           10|                                       85.0|\n",
      "|United Kingdom|   545897|          577|                         1762.2200000000018|\n",
      "+--------------+---------+-------------+-------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@6dbbb929\n",
       "spark = org.apache.spark.sql.SparkSession@1b767de0\n",
       "stockOrdersDf = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[InvoiceNo: string, StockCode: string ... 6 more fields]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.functions.column\n",
    "import org.apache.spark.sql.functions.expr\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "val stockOrdersDf = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week12_practicals/order_data-201025-223502.csv\")\n",
    "// Col Expression\n",
    "// Simple aggregations\n",
    "stockOrdersDf.select(\n",
    "    count(\"*\").as(\"totalRowCount\"),\n",
    "    sum(\"Quantity\").as(\"totalQuantity\"),\n",
    "    avg(\"UnitPrice\").as(\"avgUnitPrice\"),\n",
    "    countDistinct(\"InvoiceNo\").as(\"numUniqueInvoices\")\n",
    ").show()\n",
    "\n",
    "// Grouped aggregations\n",
    "stockOrdersDf.groupBy(\"Country\", \"InvoiceNo\")\n",
    "    .agg(sum(\"Quantity\").as(\"TotalQuantity\"),\n",
    "         sum(expr(\"Quantity * UnitPrice\")).as(\"InvoiceValue\")\n",
    ").show()\n",
    "\n",
    "// string expressions\n",
    "// Simple aggregations\n",
    "stockOrdersDf.selectExpr(\n",
    "    \"count(*) as totalRowCount\",\n",
    "    \"sum(Quantity) as totalQuantity\",\n",
    "    \"avg(UnitPrice) as avgUnitPrice\",\n",
    "    \"count(Distinct(InvoiceNo)) as numUniqueInvoices\"\n",
    ").show()\n",
    "\n",
    "stockOrdersDf.groupBy(\"Country\", \"InvoiceNo\").agg(\n",
    "    expr(\"sum(Quantity) as totalQuantity\"),\n",
    "    expr(\"sum(Quantity * UnitPrice) as InvoiceValue\")\n",
    ").show()\n",
    "\n",
    "// spark sql\n",
    "// Simple aggregations\n",
    "stockOrdersDf.createOrReplaceTempView(\"stock_sales\")\n",
    "spark.sql(\"select count(*), sum(Quantity), avg(UnitPrice), count(distinct(InvoiceNo)) from stock_sales\").show()\n",
    "\n",
    "// grouped aggregations\n",
    "spark.sql(\"select Country, InvoiceNo, sum(Quantity), sum(Quantity * UnitPrice) from stock_sales group by 1,2\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2883e085",
   "metadata": {},
   "source": [
    "### Window aggregations\n",
    "```\n",
    "[itv002768@g02 ~]$ hadoop fs -put windowdata-201025-223502.csv /user/itv002768/week12_practicals\n",
    "[itv002768@g02 ~]$ hadoop fs -head /user/itv002768/week12_practicals/windowdata-201025-223502.csv\n",
    "Spain,49,1,67,174.72\n",
    "Germany,48,11,1795,3309.75\n",
    "Lithuania,48,3,622,1598.06\n",
    "Germany,49,12,1852,4521.39\n",
    "Bahrain,51,1,54,205.74\n",
    "Iceland,49,1,319,711.79\n",
    "India,51,5,95,276.84\n",
    "Australia,50,2,133,387.95\n",
    "Italy,49,1,-2,-17.0\n",
    "India,49,5,1280,3284.1\n",
    "Spain,50,2,400,1049.01\n",
    "United Kingdom,51,200,28782,75103.46\n",
    "```\n",
    "\n",
    "partition column - country</BR>\n",
    "ordering column -  weeknum</BR>\n",
    "window size - 1st row till current row</BR>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f7c537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+-------------+------------+------------------+\n",
      "|country|weeknum|numinvoices|totalquantity|invoicevalue|     running_total|\n",
      "+-------+-------+-----------+-------------+------------+------------------+\n",
      "| Sweden|     50|          3|         3714|      2646.3|            2646.3|\n",
      "|Germany|     48|         11|         1795|     3309.75|           3309.75|\n",
      "|Germany|     49|         12|         1852|     4521.39|           7831.14|\n",
      "|Germany|     50|         15|         1973|     5065.79|          12896.93|\n",
      "|Germany|     51|          5|         1103|     1665.91|          14562.84|\n",
      "| France|     48|          4|         1299|     2808.16|           2808.16|\n",
      "| France|     49|          9|         2303|     4527.01|           7335.17|\n",
      "| France|     50|          6|          529|      537.32|           7872.49|\n",
      "| France|     51|          5|          847|     1702.87|           9575.36|\n",
      "|Belgium|     48|          1|          528|       346.1|             346.1|\n",
      "|Belgium|     50|          2|          285|      625.16|            971.26|\n",
      "|Belgium|     51|          2|          942|      838.65|1809.9099999999999|\n",
      "|Finland|     50|          1|         1254|       892.8|             892.8|\n",
      "|  India|     48|          7|         2822|     3147.23|           3147.23|\n",
      "|  India|     49|          5|         1280|      3284.1|           6431.33|\n",
      "|  India|     50|          5|         1184|     2321.78|           8753.11|\n",
      "|  India|     51|          5|           95|      276.84|           9029.95|\n",
      "|  Italy|     48|          1|          164|       427.8|             427.8|\n",
      "|  Italy|     49|          1|           -2|       -17.0|             410.8|\n",
      "|  Italy|     51|          1|          131|       383.7|             794.5|\n",
      "+-------+-------+-----------+-------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@6ff31b1b\n",
       "spark = org.apache.spark.sql.SparkSession@2166e24d\n",
       "input = [Spain: string, 49: int ... 3 more fields]\n",
       "invoiceDf = [country: string, weeknum: int ... 3 more fields]\n",
       "mywindow = org.apache.spark.sql.expressions.WindowSpec@5332c238\n",
       "final_ = [country: stri...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[country: stri..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.functions.column\n",
    "import org.apache.spark.sql.functions.expr\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "val input = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week12_practicals/windowdata-201025-223502.csv\")\n",
    "val invoiceDf = input.toDF(\"country\", \"weeknum\", \"numinvoices\", \"totalquantity\", \"invoicevalue\")\n",
    "\n",
    "\n",
    "val mywindow = Window.partitionBy(\"country\").orderBy(\"weeknum\")\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "val final_ = invoiceDf.withColumn(\"running_total\", sum(\"invoicevalue\").over(mywindow))\n",
    "final_.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60d6bc2",
   "metadata": {},
   "source": [
    "### Joins on dataframes\n",
    "There are two kind of joins which we can perform:\n",
    "* Simple join or Shuffle sort merge join\n",
    "\n",
    "* Broadcast join\n",
    "\n",
    "* Inner join - Matching records from both the tables\n",
    "* outer join - mathcing records + non-matching records from left table + non-matching records from right table\n",
    "* left join - matching records + non-matching records from left table\n",
    "* right join - matching records + non-matching records from right table\n",
    "\n",
    "Data on which we've to perform the joins:\n",
    "```\n",
    "[itv002768@g02 ~]$ hadoop fs -head /user/itv002768/week12_practicals/orders-201025-223502.csv\n",
    "order_id,order_date,order_customer_id,order_status\n",
    "1,2013-07-25 00:00:00.0,11599,CLOSED\n",
    "2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT\n",
    "3,2013-07-25 00:00:00.0,12111,COMPLETE\n",
    "4,2013-07-25 00:00:00.0,8827,CLOSED\n",
    "5,2013-07-25 00:00:00.0,11318,COMPLETE\n",
    "6,2013-07-25 00:00:00.0,7130,COMPLETE\n",
    "7,2013-07-25 00:00:00.0,4530,COMPLETE\n",
    "8,2013-07-25 00:00:00.0,2911,PROCESSING\n",
    "9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT\n",
    "10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT\n",
    "11,2013-07-25 00:00:00.0,918,PAYMENT_REVIEW\n",
    "12,2013-07-25 00:00:00.0,1837,CLOSED\n",
    "13,2013-07-25 00:00:00.0,9149,PENDING_PAYMENT\n",
    "14,2013-07-25 00:00:00.0,9842,PROCESSING\n",
    "15,2013-07-25 00:00:00.0,2568,COMPLETE\n",
    "16,2013-07-25 00:00:00.0,7276,PENDING_PAYMENT\n",
    "17,2013-07-25 00:00:00.0,2667,COMPLETE\n",
    "18,2013-07-25 00:00:00.0,1205,CLOSED\n",
    "19,2013-07-25 00:00:00.0,9488,PENDING_PAYMENT\n",
    "20,2013-07-25 00:00:00.0,9198,PROCESSING\n",
    "21,2013-07-25 00:00:00.0,2711,PENDING\n",
    "22,2013-07-25 00:00:00.0,333,COMPLETE\n",
    "23,2013-07-25 00:00:00.0,4367,PENDING_PAYMENT\n",
    "24,2013-07-25 00:00:00.0,11441,CL[itv002768@g02 ~]$ hadoop fs -head /user/itv002768/week12_practicals/customers-201025-223502.csv\n",
    "customer_id,customer_fname,customer_lname,customer_email,customer_password,customer_street,customer_city,customer_state,customer_zipcode\n",
    "1,Richard,Hernandez,XXXXXXXXX,XXXXXXXXX,6303 Heather Plaza,Brownsville,TX,78521\n",
    "2,Mary,Barrett,XXXXXXXXX,XXXXXXXXX,9526 Noble Embers Ridge,Littleton,CO,80126\n",
    "3,Ann,Smith,XXXXXXXXX,XXXXXXXXX,3422 Blue Pioneer Bend,Caguas,PR,00725\n",
    "4,Mary,Jones,XXXXXXXXX,XXXXXXXXX,8324 Little Common,San Marcos,CA,92069\n",
    "5,Robert,Hudson,XXXXXXXXX,XXXXXXXXX,10 Crystal River Mall ,Caguas,PR,00725\n",
    "6,Mary,Smith,XXXXXXXXX,XXXXXXXXX,3151 Sleepy Quail Promenade,Passaic,NJ,07055\n",
    "7,Melissa,Wilcox,XXXXXXXXX,XXXXXXXXX,9453 High Concession,Caguas,PR,00725\n",
    "8,Megan,Smith,XXXXXXXXX,XXXXXXXXX,3047 Foggy Forest Plaza,Lawrence,MA,01841\n",
    "9,Mary,Perez,XXXXXXXXX,XXXXXXXXX,3616 Quaking Street,Caguas,PR,00725\n",
    "10,Melissa,Smith,XXXXXXXXX,XXXXXXXXX,8598 Harvest Beacon Plaza,Stafford,VA,22554\n",
    "11,Mary,Huffman,XXXXXXXXX,XXXXXXXXX,3169 Stony Woods,Caguas,PR,00725\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b881b4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|customer_id|customer_fname|customer_lname|customer_email|customer_password|     customer_street|customer_city|customer_state|customer_zipcode|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|          1|       Richard|     Hernandez|     XXXXXXXXX|        XXXXXXXXX|  6303 Heather Plaza|  Brownsville|            TX|           78521|\n",
      "|          2|          Mary|       Barrett|     XXXXXXXXX|        XXXXXXXXX|9526 Noble Embers...|    Littleton|            CO|           80126|\n",
      "|          3|           Ann|         Smith|     XXXXXXXXX|        XXXXXXXXX|3422 Blue Pioneer...|       Caguas|            PR|             725|\n",
      "|          4|          Mary|         Jones|     XXXXXXXXX|        XXXXXXXXX|  8324 Little Common|   San Marcos|            CA|           92069|\n",
      "|          5|        Robert|        Hudson|     XXXXXXXXX|        XXXXXXXXX|10 Crystal River ...|       Caguas|            PR|             725|\n",
      "|          6|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|3151 Sleepy Quail...|      Passaic|            NJ|            7055|\n",
      "|          7|       Melissa|        Wilcox|     XXXXXXXXX|        XXXXXXXXX|9453 High Concession|       Caguas|            PR|             725|\n",
      "|          8|         Megan|         Smith|     XXXXXXXXX|        XXXXXXXXX|3047 Foggy Forest...|     Lawrence|            MA|            1841|\n",
      "|          9|          Mary|         Perez|     XXXXXXXXX|        XXXXXXXXX| 3616 Quaking Street|       Caguas|            PR|             725|\n",
      "|         10|       Melissa|         Smith|     XXXXXXXXX|        XXXXXXXXX|8598 Harvest Beac...|     Stafford|            VA|           22554|\n",
      "|         11|          Mary|       Huffman|     XXXXXXXXX|        XXXXXXXXX|    3169 Stony Woods|       Caguas|            PR|             725|\n",
      "|         12|   Christopher|         Smith|     XXXXXXXXX|        XXXXXXXXX|5594 Jagged Ember...|  San Antonio|            TX|           78227|\n",
      "|         13|          Mary|       Baldwin|     XXXXXXXXX|        XXXXXXXXX|7922 Iron Oak Gar...|       Caguas|            PR|             725|\n",
      "|         14|     Katherine|         Smith|     XXXXXXXXX|        XXXXXXXXX|5666 Hazy Pony Sq...|  Pico Rivera|            CA|           90660|\n",
      "|         15|          Jane|          Luna|     XXXXXXXXX|        XXXXXXXXX|    673 Burning Glen|      Fontana|            CA|           92336|\n",
      "|         16|       Tiffany|         Smith|     XXXXXXXXX|        XXXXXXXXX|      6651 Iron Port|       Caguas|            PR|             725|\n",
      "|         17|          Mary|      Robinson|     XXXXXXXXX|        XXXXXXXXX|     1325 Noble Pike|       Taylor|            MI|           48180|\n",
      "|         18|        Robert|         Smith|     XXXXXXXXX|        XXXXXXXXX|2734 Hazy Butterf...|     Martinez|            CA|           94553|\n",
      "|         19|     Stephanie|      Mitchell|     XXXXXXXXX|        XXXXXXXXX|3543 Red Treasure...|       Caguas|            PR|             725|\n",
      "|         20|          Mary|         Ellis|     XXXXXXXXX|        XXXXXXXXX|      4703 Old Route|West New York|            NJ|            7093|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-------------------+-----------------+---------------+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|customer_id|customer_fname|customer_lname|customer_email|customer_password|     customer_street|customer_city|customer_state|customer_zipcode|\n",
      "+--------+-------------------+-----------------+---------------+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|      11599|          Mary|        Malone|     XXXXXXXXX|        XXXXXXXXX|8708 Indian Horse...|      Hickory|            NC|           28601|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|        256|         David|     Rodriguez|     XXXXXXXXX|        XXXXXXXXX|7605 Tawny Horse ...|      Chicago|            IL|           60625|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|      12111|         Amber|        Franco|     XXXXXXXXX|        XXXXXXXXX|8766 Clear Prairi...|   Santa Cruz|            CA|           95060|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|       8827|         Brian|        Wilson|     XXXXXXXXX|        XXXXXXXXX|   8396 High Corners|  San Antonio|            TX|           78240|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|      11318|          Mary|         Henry|     XXXXXXXXX|        XXXXXXXXX|3047 Silent Ember...|       Caguas|            PR|             725|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|       7130|         Alice|         Smith|     XXXXXXXXX|        XXXXXXXXX|      8852 Iron Port|     Brooklyn|            NY|           11237|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|       4530|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|1073 Green Leaf G...|        Miami|            FL|           33161|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|       2911|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|9166 Golden Necta...|       Caguas|            PR|             725|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|       5657|          Mary|         James|     XXXXXXXXX|        XXXXXXXXX|  1389 Dusty Circuit|     Lakewood|            OH|           44107|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|       5648|        Joshua|         Smith|     XXXXXXXXX|        XXXXXXXXX|864 Iron Spring S...|      Memphis|            TN|           38111|\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|        918|        Nathan|         Smith|     XXXXXXXXX|        XXXXXXXXX|    9627 Honey Trail|       Caguas|            PR|             725|\n",
      "|      12|2013-07-25 00:00:00|             1837|         CLOSED|       1837|          Mary|          Vega|     XXXXXXXXX|        XXXXXXXXX|  4312 Bright Corner|       Caguas|            PR|             725|\n",
      "|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|       9149|        Ronald|     Whitehead|     XXXXXXXXX|        XXXXXXXXX|6789 Round Robin ...|    Santa Ana|            CA|           92705|\n",
      "|      14|2013-07-25 00:00:00|             9842|     PROCESSING|       9842|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|454 Lazy Branch F...|       Caguas|            PR|             725|\n",
      "|      15|2013-07-25 00:00:00|             2568|       COMPLETE|       2568|         Maria|         Smith|     XXXXXXXXX|        XXXXXXXXX|   3544 Fallen Mount|      Memphis|            TN|           38127|\n",
      "|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|       7276|        Pamela|         Smith|     XXXXXXXXX|        XXXXXXXXX|    9243 Old Gardens|       Caguas|            PR|             725|\n",
      "|      17|2013-07-25 00:00:00|             2667|       COMPLETE|       2667|         Tammy|         Smith|     XXXXXXXXX|        XXXXXXXXX|   8906 Rustic Mall |   Sun Valley|            CA|           91352|\n",
      "|      18|2013-07-25 00:00:00|             1205|         CLOSED|       1205|          Mary|        Powell|     XXXXXXXXX|        XXXXXXXXX|9299 Quiet Pionee...|        Miami|            FL|           33126|\n",
      "|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|       9488|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|    9758 Foggy Range|      Hialeah|            FL|           33012|\n",
      "|      20|2013-07-25 00:00:00|             9198|     PROCESSING|       9198|         David|          Kerr|     XXXXXXXXX|        XXXXXXXXX|7312 Crystal Will...|Bowling Green|            KY|           42101|\n",
      "+--------+-------------------+-----------------+---------------+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------------+--------------+----------------+--------+----------+-----------------+------------+\n",
      "|customer_id|customer_fname|customer_lname|customer_email|customer_password|     customer_street|      customer_city|customer_state|customer_zipcode|order_id|order_date|order_customer_id|order_status|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------------+--------------+----------------+--------+----------+-----------------+------------+\n",
      "|       9060|       Matthew|         Patel|     XXXXXXXXX|        XXXXXXXXX|7190 Silver Horse...|          Henderson|            NV|           89014|    null|      null|             null|        null|\n",
      "|       8882|       Kenneth|         Smith|     XXXXXXXXX|        XXXXXXXXX| 6754 Iron Leaf Line|            Hickory|            NC|           28601|    null|      null|             null|        null|\n",
      "|          1|       Richard|     Hernandez|     XXXXXXXXX|        XXXXXXXXX|  6303 Heather Plaza|        Brownsville|            TX|           78521|    null|      null|             null|        null|\n",
      "|       4555|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|5455 Red Lagoon Maze|             Caguas|            PR|             725|    null|      null|             null|        null|\n",
      "|        219|          Mary|       Harrell|     XXXXXXXXX|        XXXXXXXXX|9016 Foggy Robin ...|             Denver|            CO|           80219|    null|      null|             null|        null|\n",
      "|       4927|       Carolyn|         Green|     XXXXXXXXX|        XXXXXXXXX|7550 Sleepy View ...|             Caguas|            PR|             725|    null|      null|             null|        null|\n",
      "|        469|         Randy|         Smith|     XXXXXXXXX|        XXXXXXXXX|252 Golden Goose ...|South San Francisco|            CA|           94080|    null|      null|             null|        null|\n",
      "|       6072|       Stephen|         Smith|     XXXXXXXXX|        XXXXXXXXX|849 Noble Apple P...|             Malden|            MA|            2148|    null|      null|             null|        null|\n",
      "|       1187|       Dorothy|       Vazquez|     XXXXXXXXX|        XXXXXXXXX| 363 Green Goose Run|            Danbury|            CT|            6810|    null|      null|             null|        null|\n",
      "|       6613|        Ashley|         Smith|     XXXXXXXXX|        XXXXXXXXX|9847 Dusty Horse ...|             Caguas|            PR|             725|    null|      null|             null|        null|\n",
      "|       2073|         Donna|      Stephens|     XXXXXXXXX|        XXXXXXXXX|   9792 Cozy Corners|          Sunnyvale|            CA|           94087|    null|      null|             null|        null|\n",
      "|       7011|         Kevin|         Smith|     XXXXXXXXX|        XXXXXXXXX|1915 Thunder Hick...|          Wyandotte|            MI|           48192|    null|      null|             null|        null|\n",
      "|       2450|         James|         Smith|     XXXXXXXXX|        XXXXXXXXX|4063 Little Creek...|             Newark|            DE|           19702|    null|      null|             null|        null|\n",
      "|       7552|          Carl|         Smith|     XXXXXXXXX|        XXXXXXXXX|    9966 Cinder Loop|             Howell|            MI|           48843|    null|      null|             null|        null|\n",
      "|       8778|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|4015 Tawny Rise C...|             Caguas|            PR|             725|    null|      null|             null|        null|\n",
      "|       8243|          Gary|        Walker|     XXXXXXXXX|        XXXXXXXXX|2447 Stony Barn S...|           New York|            NY|           10128|    null|      null|             null|        null|\n",
      "|       1481|         Grace|         Smith|     XXXXXXXXX|        XXXXXXXXX|2171 Clear Lake Isle|             Caguas|            PR|             725|    null|      null|             null|        null|\n",
      "|       8343|          Mary|        Bolton|     XXXXXXXXX|        XXXXXXXXX|   7302 Sunny Valley|             Caguas|            PR|             725|    null|      null|             null|        null|\n",
      "|       2096|          Jose|        Tanner|     XXXXXXXXX|        XXXXXXXXX|8976 Old Hickory ...|              Bronx|            NY|           10467|    null|      null|             null|        null|\n",
      "|       8575|          Mary|       Mueller|     XXXXXXXXX|        XXXXXXXXX|9714 Emerald Bear...|             Caguas|            PR|             725|    null|      null|             null|        null|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------------+--------------+----------------+--------+----------+-----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@79537bae\n",
       "spark = org.apache.spark.sql.SparkSession@2166e24d\n",
       "customerDetails = [customer_id: int, customer_fname: string ... 7 more fields]\n",
       "orderInfo = [order_id: int, order_date: timestamp ... 2 more fields]\n",
       "innerJoinDf = [order_id: int, order_date: timestamp ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "leftJoinDf: org.apache.sp...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: timestamp ... 11 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.functions.column\n",
    "import org.apache.spark.sql.functions.expr\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "val customerDetails = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week12_practicals/customers-201025-223502.csv\")\n",
    "customerDetails.show()\n",
    "\n",
    "val orderInfo = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week12_practicals/orders-201025-223502.csv\")\n",
    "orderInfo.show()\n",
    "// join condition inside we can also put the join condition in a variable\n",
    "val innerJoinDf = orderInfo.join(customerDetails, orderInfo.col(\"order_customer_id\")===customerDetails.col(\"customer_id\"), \"inner\")\n",
    "innerJoinDf.show()\n",
    "\n",
    "//Left join\n",
    "val leftJoinDf = customerDetails.join(orderInfo, customerDetails.col(\"customer_id\")===orderInfo.col(\"order_customer_id\"), \"left\").sort(\"order_id\")\n",
    "leftJoinDf.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df605a95",
   "metadata": {},
   "source": [
    "### Showcasing how your code can lead to ambiguous problem\n",
    "\n",
    "There are two ways to solve the ambiguity:\n",
    "* Rename ambiguous column so that ambiguity won't come</BR>\n",
    "`.withColumnRenamed(\"olcclumnName\",\"newcolumnName\")`\n",
    "\n",
    "* After join we can drop one of the columns</BR>\n",
    "`.drop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f05d0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|customer_id|customer_fname|customer_lname|customer_email|customer_password|     customer_street|customer_city|customer_state|customer_zipcode|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|          1|       Richard|     Hernandez|     XXXXXXXXX|        XXXXXXXXX|  6303 Heather Plaza|  Brownsville|            TX|           78521|\n",
      "|          2|          Mary|       Barrett|     XXXXXXXXX|        XXXXXXXXX|9526 Noble Embers...|    Littleton|            CO|           80126|\n",
      "|          3|           Ann|         Smith|     XXXXXXXXX|        XXXXXXXXX|3422 Blue Pioneer...|       Caguas|            PR|             725|\n",
      "|          4|          Mary|         Jones|     XXXXXXXXX|        XXXXXXXXX|  8324 Little Common|   San Marcos|            CA|           92069|\n",
      "|          5|        Robert|        Hudson|     XXXXXXXXX|        XXXXXXXXX|10 Crystal River ...|       Caguas|            PR|             725|\n",
      "|          6|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|3151 Sleepy Quail...|      Passaic|            NJ|            7055|\n",
      "|          7|       Melissa|        Wilcox|     XXXXXXXXX|        XXXXXXXXX|9453 High Concession|       Caguas|            PR|             725|\n",
      "|          8|         Megan|         Smith|     XXXXXXXXX|        XXXXXXXXX|3047 Foggy Forest...|     Lawrence|            MA|            1841|\n",
      "|          9|          Mary|         Perez|     XXXXXXXXX|        XXXXXXXXX| 3616 Quaking Street|       Caguas|            PR|             725|\n",
      "|         10|       Melissa|         Smith|     XXXXXXXXX|        XXXXXXXXX|8598 Harvest Beac...|     Stafford|            VA|           22554|\n",
      "|         11|          Mary|       Huffman|     XXXXXXXXX|        XXXXXXXXX|    3169 Stony Woods|       Caguas|            PR|             725|\n",
      "|         12|   Christopher|         Smith|     XXXXXXXXX|        XXXXXXXXX|5594 Jagged Ember...|  San Antonio|            TX|           78227|\n",
      "|         13|          Mary|       Baldwin|     XXXXXXXXX|        XXXXXXXXX|7922 Iron Oak Gar...|       Caguas|            PR|             725|\n",
      "|         14|     Katherine|         Smith|     XXXXXXXXX|        XXXXXXXXX|5666 Hazy Pony Sq...|  Pico Rivera|            CA|           90660|\n",
      "|         15|          Jane|          Luna|     XXXXXXXXX|        XXXXXXXXX|    673 Burning Glen|      Fontana|            CA|           92336|\n",
      "|         16|       Tiffany|         Smith|     XXXXXXXXX|        XXXXXXXXX|      6651 Iron Port|       Caguas|            PR|             725|\n",
      "|         17|          Mary|      Robinson|     XXXXXXXXX|        XXXXXXXXX|     1325 Noble Pike|       Taylor|            MI|           48180|\n",
      "|         18|        Robert|         Smith|     XXXXXXXXX|        XXXXXXXXX|2734 Hazy Butterf...|     Martinez|            CA|           94553|\n",
      "|         19|     Stephanie|      Mitchell|     XXXXXXXXX|        XXXXXXXXX|3543 Red Treasure...|       Caguas|            PR|             725|\n",
      "|         20|          Mary|         Ellis|     XXXXXXXXX|        XXXXXXXXX|      4703 Old Route|West New York|            NJ|            7093|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:00|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|       9198|     PROCESSING|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-----------+--------------+\n",
      "|order_id|customer_id|customer_fname|\n",
      "+--------+-----------+--------------+\n",
      "|       1|      11599|          Mary|\n",
      "|       2|        256|         David|\n",
      "|       3|      12111|         Amber|\n",
      "|       4|       8827|         Brian|\n",
      "|       5|      11318|          Mary|\n",
      "|       6|       7130|         Alice|\n",
      "|       7|       4530|          Mary|\n",
      "|       8|       2911|          Mary|\n",
      "|       9|       5657|          Mary|\n",
      "|      10|       5648|        Joshua|\n",
      "|      11|        918|        Nathan|\n",
      "|      12|       1837|          Mary|\n",
      "|      13|       9149|        Ronald|\n",
      "|      14|       9842|          Mary|\n",
      "|      15|       2568|         Maria|\n",
      "|      16|       7276|        Pamela|\n",
      "|      17|       2667|         Tammy|\n",
      "|      18|       1205|          Mary|\n",
      "|      19|       9488|          Mary|\n",
      "|      20|       9198|         David|\n",
      "+--------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@3ad445d4\n",
       "spark = org.apache.spark.sql.SparkSession@2166e24d\n",
       "customerDetails = [customer_id: int, customer_fname: string ... 7 more fields]\n",
       "orderInfo = [order_id: int, order_date: timestamp ... 2 more fields]\n",
       "innerJoinDf = [order_id: int, customer_id: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, customer_id: int ... 1 more field]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.functions.column\n",
    "import org.apache.spark.sql.functions.expr\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "val customerDetails = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week12_practicals/customers-201025-223502.csv\")\n",
    "customerDetails.show()\n",
    "\n",
    "val orderInfo = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week12_practicals/orders-201025-223502.csv\").withColumnRenamed(\"order_customer_id\", \"customer_id\")\n",
    "orderInfo.show()\n",
    "\n",
    "\n",
    "val innerJoinDf = orderInfo.join(customerDetails, orderInfo.col(\"customer_id\")===customerDetails.col(\"customer_id\"), \"inner\")\n",
    "    .drop(orderInfo.col(\"customer_id\"))\n",
    "    .select(\"order_id\", \"customer_id\", \"customer_fname\")\n",
    "innerJoinDf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c6feb4",
   "metadata": {},
   "source": [
    "### How to deal with NULLs\n",
    "\n",
    "Whenever order_id is null show -1 for this we can use `coalesce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff9bb018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|customer_id|customer_fname|customer_lname|customer_email|customer_password|     customer_street|customer_city|customer_state|customer_zipcode|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|          1|       Richard|     Hernandez|     XXXXXXXXX|        XXXXXXXXX|  6303 Heather Plaza|  Brownsville|            TX|           78521|\n",
      "|          2|          Mary|       Barrett|     XXXXXXXXX|        XXXXXXXXX|9526 Noble Embers...|    Littleton|            CO|           80126|\n",
      "|          3|           Ann|         Smith|     XXXXXXXXX|        XXXXXXXXX|3422 Blue Pioneer...|       Caguas|            PR|             725|\n",
      "|          4|          Mary|         Jones|     XXXXXXXXX|        XXXXXXXXX|  8324 Little Common|   San Marcos|            CA|           92069|\n",
      "|          5|        Robert|        Hudson|     XXXXXXXXX|        XXXXXXXXX|10 Crystal River ...|       Caguas|            PR|             725|\n",
      "|          6|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|3151 Sleepy Quail...|      Passaic|            NJ|            7055|\n",
      "|          7|       Melissa|        Wilcox|     XXXXXXXXX|        XXXXXXXXX|9453 High Concession|       Caguas|            PR|             725|\n",
      "|          8|         Megan|         Smith|     XXXXXXXXX|        XXXXXXXXX|3047 Foggy Forest...|     Lawrence|            MA|            1841|\n",
      "|          9|          Mary|         Perez|     XXXXXXXXX|        XXXXXXXXX| 3616 Quaking Street|       Caguas|            PR|             725|\n",
      "|         10|       Melissa|         Smith|     XXXXXXXXX|        XXXXXXXXX|8598 Harvest Beac...|     Stafford|            VA|           22554|\n",
      "|         11|          Mary|       Huffman|     XXXXXXXXX|        XXXXXXXXX|    3169 Stony Woods|       Caguas|            PR|             725|\n",
      "|         12|   Christopher|         Smith|     XXXXXXXXX|        XXXXXXXXX|5594 Jagged Ember...|  San Antonio|            TX|           78227|\n",
      "|         13|          Mary|       Baldwin|     XXXXXXXXX|        XXXXXXXXX|7922 Iron Oak Gar...|       Caguas|            PR|             725|\n",
      "|         14|     Katherine|         Smith|     XXXXXXXXX|        XXXXXXXXX|5666 Hazy Pony Sq...|  Pico Rivera|            CA|           90660|\n",
      "|         15|          Jane|          Luna|     XXXXXXXXX|        XXXXXXXXX|    673 Burning Glen|      Fontana|            CA|           92336|\n",
      "|         16|       Tiffany|         Smith|     XXXXXXXXX|        XXXXXXXXX|      6651 Iron Port|       Caguas|            PR|             725|\n",
      "|         17|          Mary|      Robinson|     XXXXXXXXX|        XXXXXXXXX|     1325 Noble Pike|       Taylor|            MI|           48180|\n",
      "|         18|        Robert|         Smith|     XXXXXXXXX|        XXXXXXXXX|2734 Hazy Butterf...|     Martinez|            CA|           94553|\n",
      "|         19|     Stephanie|      Mitchell|     XXXXXXXXX|        XXXXXXXXX|3543 Red Treasure...|       Caguas|            PR|             725|\n",
      "|         20|          Mary|         Ellis|     XXXXXXXXX|        XXXXXXXXX|      4703 Old Route|West New York|            NJ|            7093|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:00|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|       9198|     PROCESSING|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-----------+--------------+\n",
      "|order_id|customer_id|customer_fname|\n",
      "+--------+-----------+--------------+\n",
      "|      -1|       9060|       Matthew|\n",
      "|      -1|       8882|       Kenneth|\n",
      "|      -1|          1|       Richard|\n",
      "|      -1|       4555|          Mary|\n",
      "|      -1|        219|          Mary|\n",
      "|      -1|       4927|       Carolyn|\n",
      "|      -1|        469|         Randy|\n",
      "|      -1|       6072|       Stephen|\n",
      "|      -1|       1187|       Dorothy|\n",
      "|      -1|       6613|        Ashley|\n",
      "|      -1|       2073|         Donna|\n",
      "|      -1|       7011|         Kevin|\n",
      "|      -1|       2450|         James|\n",
      "|      -1|       7552|          Carl|\n",
      "|      -1|       8778|          Mary|\n",
      "|      -1|       8243|          Gary|\n",
      "|      -1|       1481|         Grace|\n",
      "|      -1|       8343|          Mary|\n",
      "|      -1|       2096|          Jose|\n",
      "|      -1|       8575|          Mary|\n",
      "+--------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@5a7632b0\n",
       "spark = org.apache.spark.sql.SparkSession@2166e24d\n",
       "customerDetails = [customer_id: int, customer_fname: string ... 7 more fields]\n",
       "orderInfo = [order_id: int, order_date: timestamp ... 2 more fields]\n",
       "innerJoinDf = [order_id: int, customer_id: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, customer_id: int ... 1 more field]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.functions.column\n",
    "import org.apache.spark.sql.functions.expr\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "val customerDetails = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week12_practicals/customers-201025-223502.csv\")\n",
    "customerDetails.show()\n",
    "\n",
    "val orderInfo = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week12_practicals/orders-201025-223502.csv\").withColumnRenamed(\"order_customer_id\", \"customer_id\")\n",
    "orderInfo.show()\n",
    "\n",
    "\n",
    "val innerJoinDf = customerDetails.join(orderInfo, customerDetails.col(\"customer_id\")===orderInfo.col(\"customer_id\"), \"left\")\n",
    "    .drop(orderInfo.col(\"customer_id\"))\n",
    "    .select(\"order_id\", \"customer_id\", \"customer_fname\")\n",
    "    .sort(\"order_id\")\n",
    "    .withColumn(\"order_id\", expr(\"coalesce(order_id, -1)\"))\n",
    "innerJoinDf.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a246c53e",
   "metadata": {},
   "source": [
    "### Internals of a normal join operations\n",
    "\n",
    "**Please refer to Dataframe Session-21**</BR>\n",
    "Let's say we have a three node cluster and we are reading two file orders and customers which are divided into two parts each. In this case it'll go to three executors on three nodes. Now suppose if we'll join these two df then there is a possibility that join column id in one dataframe on one node will have its join mate on different executor. In this case shuffling will happen.\n",
    "\n",
    "Once the join start it will try to join in the same executor and output is written to exchange. **exchange** is nothing but a buffer in an executor. From this exchange spark can read it and do the shuffle.\n",
    "\n",
    "All the records with same key will go to the same reduce exchange.\n",
    "\n",
    "### Broadcast Join\n",
    "\n",
    "**Please refer to Dataframe Session-22**</BR>\n",
    "This doesn't require a shuffle.\n",
    "\n",
    "When to use which type of join?</BR>\n",
    "* Whenever we're joining two large dataframed then it'll invoke a simple join and shuffle will be required.\n",
    "* If one dataframe is large and one dataframe in smaller in that case we can go with broadcast join. That small dataframe(fullcopy) will be sent to all the executors.\n",
    "* If both the datasets are small we can use the programming language like python. Spark is not required here.\n",
    "\n",
    "Broadcast dataframe should be small enough that it should fit into the executor's memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b98d1f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|customer_id|customer_fname|customer_lname|customer_email|customer_password|     customer_street|customer_city|customer_state|customer_zipcode|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|          1|       Richard|     Hernandez|     XXXXXXXXX|        XXXXXXXXX|  6303 Heather Plaza|  Brownsville|            TX|           78521|\n",
      "|          2|          Mary|       Barrett|     XXXXXXXXX|        XXXXXXXXX|9526 Noble Embers...|    Littleton|            CO|           80126|\n",
      "|          3|           Ann|         Smith|     XXXXXXXXX|        XXXXXXXXX|3422 Blue Pioneer...|       Caguas|            PR|             725|\n",
      "|          4|          Mary|         Jones|     XXXXXXXXX|        XXXXXXXXX|  8324 Little Common|   San Marcos|            CA|           92069|\n",
      "|          5|        Robert|        Hudson|     XXXXXXXXX|        XXXXXXXXX|10 Crystal River ...|       Caguas|            PR|             725|\n",
      "|          6|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|3151 Sleepy Quail...|      Passaic|            NJ|            7055|\n",
      "|          7|       Melissa|        Wilcox|     XXXXXXXXX|        XXXXXXXXX|9453 High Concession|       Caguas|            PR|             725|\n",
      "|          8|         Megan|         Smith|     XXXXXXXXX|        XXXXXXXXX|3047 Foggy Forest...|     Lawrence|            MA|            1841|\n",
      "|          9|          Mary|         Perez|     XXXXXXXXX|        XXXXXXXXX| 3616 Quaking Street|       Caguas|            PR|             725|\n",
      "|         10|       Melissa|         Smith|     XXXXXXXXX|        XXXXXXXXX|8598 Harvest Beac...|     Stafford|            VA|           22554|\n",
      "|         11|          Mary|       Huffman|     XXXXXXXXX|        XXXXXXXXX|    3169 Stony Woods|       Caguas|            PR|             725|\n",
      "|         12|   Christopher|         Smith|     XXXXXXXXX|        XXXXXXXXX|5594 Jagged Ember...|  San Antonio|            TX|           78227|\n",
      "|         13|          Mary|       Baldwin|     XXXXXXXXX|        XXXXXXXXX|7922 Iron Oak Gar...|       Caguas|            PR|             725|\n",
      "|         14|     Katherine|         Smith|     XXXXXXXXX|        XXXXXXXXX|5666 Hazy Pony Sq...|  Pico Rivera|            CA|           90660|\n",
      "|         15|          Jane|          Luna|     XXXXXXXXX|        XXXXXXXXX|    673 Burning Glen|      Fontana|            CA|           92336|\n",
      "|         16|       Tiffany|         Smith|     XXXXXXXXX|        XXXXXXXXX|      6651 Iron Port|       Caguas|            PR|             725|\n",
      "|         17|          Mary|      Robinson|     XXXXXXXXX|        XXXXXXXXX|     1325 Noble Pike|       Taylor|            MI|           48180|\n",
      "|         18|        Robert|         Smith|     XXXXXXXXX|        XXXXXXXXX|2734 Hazy Butterf...|     Martinez|            CA|           94553|\n",
      "|         19|     Stephanie|      Mitchell|     XXXXXXXXX|        XXXXXXXXX|3543 Red Treasure...|       Caguas|            PR|             725|\n",
      "|         20|          Mary|         Ellis|     XXXXXXXXX|        XXXXXXXXX|      4703 Old Route|West New York|            NJ|            7093|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:00|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|       9198|     PROCESSING|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-------------------+-----------+---------------+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|order_id|         order_date|customer_id|   order_status|customer_id|customer_fname|customer_lname|customer_email|customer_password|     customer_street|customer_city|customer_state|customer_zipcode|\n",
      "+--------+-------------------+-----------+---------------+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|       1|2013-07-25 00:00:00|      11599|         CLOSED|      11599|          Mary|        Malone|     XXXXXXXXX|        XXXXXXXXX|8708 Indian Horse...|      Hickory|            NC|           28601|\n",
      "|       2|2013-07-25 00:00:00|        256|PENDING_PAYMENT|        256|         David|     Rodriguez|     XXXXXXXXX|        XXXXXXXXX|7605 Tawny Horse ...|      Chicago|            IL|           60625|\n",
      "|       3|2013-07-25 00:00:00|      12111|       COMPLETE|      12111|         Amber|        Franco|     XXXXXXXXX|        XXXXXXXXX|8766 Clear Prairi...|   Santa Cruz|            CA|           95060|\n",
      "|       4|2013-07-25 00:00:00|       8827|         CLOSED|       8827|         Brian|        Wilson|     XXXXXXXXX|        XXXXXXXXX|   8396 High Corners|  San Antonio|            TX|           78240|\n",
      "|       5|2013-07-25 00:00:00|      11318|       COMPLETE|      11318|          Mary|         Henry|     XXXXXXXXX|        XXXXXXXXX|3047 Silent Ember...|       Caguas|            PR|             725|\n",
      "|       6|2013-07-25 00:00:00|       7130|       COMPLETE|       7130|         Alice|         Smith|     XXXXXXXXX|        XXXXXXXXX|      8852 Iron Port|     Brooklyn|            NY|           11237|\n",
      "|       7|2013-07-25 00:00:00|       4530|       COMPLETE|       4530|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|1073 Green Leaf G...|        Miami|            FL|           33161|\n",
      "|       8|2013-07-25 00:00:00|       2911|     PROCESSING|       2911|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|9166 Golden Necta...|       Caguas|            PR|             725|\n",
      "|       9|2013-07-25 00:00:00|       5657|PENDING_PAYMENT|       5657|          Mary|         James|     XXXXXXXXX|        XXXXXXXXX|  1389 Dusty Circuit|     Lakewood|            OH|           44107|\n",
      "|      10|2013-07-25 00:00:00|       5648|PENDING_PAYMENT|       5648|        Joshua|         Smith|     XXXXXXXXX|        XXXXXXXXX|864 Iron Spring S...|      Memphis|            TN|           38111|\n",
      "|      11|2013-07-25 00:00:00|        918| PAYMENT_REVIEW|        918|        Nathan|         Smith|     XXXXXXXXX|        XXXXXXXXX|    9627 Honey Trail|       Caguas|            PR|             725|\n",
      "|      12|2013-07-25 00:00:00|       1837|         CLOSED|       1837|          Mary|          Vega|     XXXXXXXXX|        XXXXXXXXX|  4312 Bright Corner|       Caguas|            PR|             725|\n",
      "|      13|2013-07-25 00:00:00|       9149|PENDING_PAYMENT|       9149|        Ronald|     Whitehead|     XXXXXXXXX|        XXXXXXXXX|6789 Round Robin ...|    Santa Ana|            CA|           92705|\n",
      "|      14|2013-07-25 00:00:00|       9842|     PROCESSING|       9842|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|454 Lazy Branch F...|       Caguas|            PR|             725|\n",
      "|      15|2013-07-25 00:00:00|       2568|       COMPLETE|       2568|         Maria|         Smith|     XXXXXXXXX|        XXXXXXXXX|   3544 Fallen Mount|      Memphis|            TN|           38127|\n",
      "|      16|2013-07-25 00:00:00|       7276|PENDING_PAYMENT|       7276|        Pamela|         Smith|     XXXXXXXXX|        XXXXXXXXX|    9243 Old Gardens|       Caguas|            PR|             725|\n",
      "|      17|2013-07-25 00:00:00|       2667|       COMPLETE|       2667|         Tammy|         Smith|     XXXXXXXXX|        XXXXXXXXX|   8906 Rustic Mall |   Sun Valley|            CA|           91352|\n",
      "|      18|2013-07-25 00:00:00|       1205|         CLOSED|       1205|          Mary|        Powell|     XXXXXXXXX|        XXXXXXXXX|9299 Quiet Pionee...|        Miami|            FL|           33126|\n",
      "|      19|2013-07-25 00:00:00|       9488|PENDING_PAYMENT|       9488|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|    9758 Foggy Range|      Hialeah|            FL|           33012|\n",
      "|      20|2013-07-25 00:00:00|       9198|     PROCESSING|       9198|         David|          Kerr|     XXXXXXXXX|        XXXXXXXXX|7312 Crystal Will...|Bowling Green|            KY|           42101|\n",
      "+--------+-------------------+-----------+---------------+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@76942a71\n",
       "spark = org.apache.spark.sql.SparkSession@2166e24d\n",
       "customerDetails = [customer_id: int, customer_fname: string ... 7 more fields]\n",
       "orderInfo = [order_id: int, order_date: timestamp ... 2 more fields]\n",
       "innerJoinDf = [order_id: int, order_date: timestamp ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: timestamp ... 11 more fields]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.functions.column\n",
    "import org.apache.spark.sql.functions.expr\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "val customerDetails = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week12_practicals/customers-201025-223502.csv\")\n",
    "customerDetails.show()\n",
    "\n",
    "val orderInfo = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week12_practicals/orders-201025-223502.csv\").withColumnRenamed(\"order_customer_id\", \"customer_id\")\n",
    "orderInfo.show()\n",
    "\n",
    "// Doing a broadcast join\n",
    "val innerJoinDf = orderInfo.join(broadcast(customerDetails), customerDetails.col(\"customer_id\")===orderInfo.col(\"customer_id\"), \"inner\")\n",
    "innerJoinDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63993fde",
   "metadata": {},
   "source": [
    "### Practical - grouping on loglevel and month\n",
    "\n",
    "The following file contains the log level and the timestamp of each log.\n",
    "\n",
    "Our task is to count the log level for each month.\n",
    "```\n",
    "[itv002768@g02 ~]$ hadoop fs -head /user/itv002768/week12_practicals/biglog-201105-152517.txt\n",
    "level,datetime\n",
    "DEBUG,2015-2-6 16:24:07\n",
    "WARN,2016-7-26 18:54:43\n",
    "INFO,2012-10-18 14:35:19\n",
    "DEBUG,2012-4-26 14:26:50\n",
    "DEBUG,2013-9-28 20:27:13\n",
    "INFO,2017-8-20 13:17:27\n",
    "INFO,2015-4-13 09:28:17\n",
    "DEBUG,2015-7-17 00:49:27\n",
    "DEBUG,2014-7-26 02:33:09\n",
    "INFO,2016-1-13 09:51:57\n",
    "DEBUG,2015-1-14 08:55:30\n",
    "DEBUG,2016-1-20 03:47:06\n",
    "DEBUG,2013-7-8 21:00:50\n",
    "DEBUG,2012-5-22 11:43:57\n",
    "DEBUG,2013-3-20 06:14:50\n",
    "```\n",
    "\n",
    "I'll load the hardcoded data and once I feel its confident enought to do the required operations I'll use the original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f5ddbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+--------+-----+-----+-----+-----+-----+------+---------+-------+--------+--------+\n",
      "|level|January|February|March|April|  May| June| July|August|September|October|November|December|\n",
      "+-----+-------+--------+-----+-----+-----+-----+-----+------+---------+-------+--------+--------+\n",
      "| INFO|  29119|   28983|29095|29302|28900|29143|29300| 28993|    29038|  29018|   23301|   28874|\n",
      "|ERROR|   4054|    4013| 4122| 4107| 4086| 4059| 3976|  3987|     4161|   4040|    3389|    4106|\n",
      "| WARN|   8217|    8266| 8165| 8277| 8403| 8191| 8222|  8381|     8352|   8226|    6616|    8328|\n",
      "|FATAL|     94|      72|   70|   83|   60|   78|   98|    80|       81|     92|   16797|      94|\n",
      "|DEBUG|  41961|   41734|41652|41869|41785|41774|42085| 42147|    41433|  41936|   33366|   41749|\n",
      "+-----+-------+--------+-----+-----+-----+-----+-----+------+---------+-------+--------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined class Logging\n",
       "sparkConfig = org.apache.spark.SparkConf@3b5309dc\n",
       "spark = org.apache.spark.sql.SparkSession@4b8f727c\n",
       "logDetails = [level: string, datetime: timestamp]\n",
       "result = [level: string, month: string ... 1 more field]\n",
       "months = List(January, February, March, April, May, June, July, August, September, October, November, De...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n",
       "mapper: (line: String)Logging\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List(January, February, March, April, May, June, July, August, September, October, November, De..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.functions.column\n",
    "import org.apache.spark.sql.functions.expr\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "\n",
    "case class Logging(level: String, datetime: String)\n",
    "\n",
    "def mapper(line: String): Logging = {\n",
    "    val fields = line.split(\",\")\n",
    "    val logging: Logging = Logging(fields(0), fields(1))\n",
    "    return logging\n",
    "}\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application-1\")\n",
    "sparkConfig.set(\"spark.master\", \"local[2]\")\n",
    "\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "\n",
    "/*\n",
    "This is not required as we only did it for testing purpose\n",
    "\n",
    "import spark.implicits._\n",
    "val myList = List(\"DEBUG,2015-2-6 16:24:07\",\n",
    "\"WARN,2016-7-26 18:54:43\",\n",
    "\"INFO,2012-10-18 14:35:19\",\n",
    "\"DEBUG,2012-4-26 14:26:50\",\n",
    "\"DEBUG,2013-9-28 20:27:13\",\n",
    "\"INFO,2017-8-20 13:17:27\",\n",
    "\"INFO,2015-4-13 09:28:17\")\n",
    "val sampleDataRdd = spark.sparkContext.parallelize(myList)\n",
    "\n",
    "\n",
    "val rdd2 = sampleDataRdd.map(mapper)\n",
    "\n",
    "val logDetails = rdd2.toDF()\n",
    "*/\n",
    "val logDetails = spark.read.option(\"header\", true)\n",
    "               .option(\"inferSchema\", true)\n",
    "               .csv(\"/user/itv002768/week12_practicals/biglog-201105-152517.txt\")\n",
    "\n",
    "\n",
    "\n",
    "logDetails.createOrReplaceTempView(\"logging_table\")\n",
    "\n",
    "val result = spark.sql(\"\"\"\n",
    "select level,\n",
    "       date_format(datetime, 'MMMM') as month,\n",
    "       count(*) as total\n",
    "from logging_table\n",
    "group by level, month\n",
    "\"\"\")\n",
    "\n",
    "val months = List(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\")\n",
    "val result1 = spark.sql(\"\"\"\n",
    "select level,\n",
    "       date_format(datetime, 'MMMM') month\n",
    "from logging_table\n",
    "\"\"\").groupBy(\"level\").pivot(\"month\", months).count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
