{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66a86940",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "* [Introduction](#Introduction)\n",
    "* [Requirements for a DB](#Requirements-for-a-DB)\n",
    "* [Hbase Properties](#Hbase-Properties)\n",
    "* [Hbase vs RDBMS](#Hbase-vs-DBMS)\n",
    "* [Advantages of a Columnar store](#Advantages-of-a-Columnar-store)\n",
    "* [4-Dimentional Data Model](#4-Dimentional-Data-Model)\n",
    "* [Hbase Architecture](#Hbase-Architecture)\n",
    "* [HBase Read/Write Operations](#HBase-Read/Write-Operations)\n",
    "  - [HBase Write Operations](#HBase-Write-Operations)\n",
    "  - [Hbase Read Operations](#Hbase-Read-Operations)\n",
    "* [Compactions](#Compactions)\n",
    "* [Update/Delete in Hbase](#Update/Delete-in-Hbase)\n",
    "* [Practicals](#Practicals)\n",
    "* [CAP Theorem](#CAP-Theorem)\n",
    "  - [Consistency](#Consistency)\n",
    "  - [Availability](#Availability)\n",
    "  - [Partition Toleance](#Partition-Toleance)\n",
    "* [Cassandra](#Cassandra)\n",
    "* [Cassandra vs Hbase](#Cassandra-vs-Hbase)\n",
    "* [Hive-Hbase Integration](#Hive-Hbase-Integration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7653ccf5",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "**HBase** is a distributed database management system which run on top of Hadoop\n",
    "* Stores data IN HDFS.\n",
    "* Its sclable as the capacity is directly proportional to number of nodes.\n",
    "* Fault tolerant.\n",
    "\n",
    "\n",
    "### Requirements for a DB\n",
    "* Structured Manner - In the form of rows & columns.\n",
    "* Should give Random Access to your data - Indices\n",
    "* Low Latency - Taking less time in searching n number of rows from a set of rows.\n",
    "* ACID compliance\n",
    "  - Atomicity - All or Nothing(All the transactions that are interrelated should be executed or none should execute).\n",
    "  - Consistency - you can put some contraints in DB tables so that consistency will be maintained.\n",
    "  - Isolation - If multiple people are operating in a DB and doing some operations there should be a defined sequence in which that transaction should happen. Example is Locking mechanism in tables and row versioning.\n",
    "  - Durability - Whenever a system fails, it should not be in a state that it is not usable. In case of DB if some changes are done like updated and inserts they should be persistent. In short, Avoiding data loss.\n",
    "  \n",
    "  Hadoop is not a database, it doesn't follow the above mentioned points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f53c4",
   "metadata": {},
   "source": [
    "### Hbase Properties\n",
    "**Structured**:\n",
    "Hbase is a Loose structured DB\n",
    "\n",
    "**Low Latency**:\n",
    "Provides real-time access using row based indices called as row keys.\n",
    "\n",
    "**Random Access**:\n",
    "Possible based on row keys\n",
    "\n",
    "**ACID Compliant**:\n",
    "To some extent at row level.\n",
    "\n",
    "It offers two thing, it provides you quick ***processing*** and ***searching***. But we are more concerned on searching based on row keys.\n",
    "\n",
    "### Hbase vs DBMS\n",
    "\n",
    "* DBMS stores the data in row manner while Hbase stores it in columnar fashion(NoSQL).\n",
    "* Normalization is there in DBMS, But in case of Hbase it is not preferred.\n",
    "* DBMS offers joins, group by, order by etc. But in NoSQL we can only CRUD operations. Why? Because data is denormalized.\n",
    "* DBMS are ACID compliant. However, Hbase provides Hbase compliant at row level. Whenever we trying to update multiple columns for a single row either all of them will be updated or None will be updated. In case of multiple rows update there is now gurantee.\n",
    "\n",
    "\n",
    "### Advantages of a Columnar store\n",
    "* If the data is Sparse it will save space. No key value pair is required if there are not values for a column.\n",
    "* Adding new column is easier since only a key value is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9238bd",
   "metadata": {},
   "source": [
    "### 4-Dimentional Data Model\n",
    "* **Row Key** - Unique identified for a record. Sorted in ascending order and stored as byte array.\n",
    "* **Column** - All Columns e.g - dept. grade, title, Name, SSN. Each column family is stored in a separate file and setup at schema definition time.\n",
    "* **Column family** - All columns grouped in a logical way (work(dept. grade, title) | Personal(Name, SSN)).\n",
    "* **Timestamp** - e.g Title as AVP on 23455676 and VP on 2445654 - Basically for versioning and latest version is shown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cdfd10",
   "metadata": {},
   "source": [
    "### Hbase Architecture\n",
    "Let's talk about how a table looks like in Hbase;\n",
    "\n",
    "Table - employee</br>\n",
    "Column Family - Column groups</br>\n",
    "personal - Name, Age, Address</br>\n",
    "professional - Designation, Department, Salary</br>\n",
    "\n",
    "Row_key = employee_id(1...10000)\n",
    "\n",
    "These are divided like this:</br>\n",
    "1-2500 (region 1)</br>\n",
    "2501 - 5000 (region 2)</br>\n",
    "5001 - 7500 (region 3)</br>\n",
    "7501 - 10000 (region 4)</br>\n",
    "\n",
    "Regions are handled by region server\n",
    "\n",
    "Region server 1 = Region 1 and Region 2</br>\n",
    "Region server 2 = Region 3 and Region 4</br>\n",
    "\n",
    "\n",
    "* If we have 4 data nodes we will have 4 region servers running on each DB. Good practice is **1:1** for **region_server:data_node**.\n",
    "* Each region server can hold **multiple regions** and each region holds data in ascending order based on row keys.\n",
    "* In each region there is a **memstore** for every column family. Whenever we want to write new records it is stored in memstore and keeps every record in a memory till a threshold defined then it will flush it to the disc and a new file is created. File that is created is called as HFile(In HDFS).\n",
    "* **WAL(Write Ahead Log)** - Consider that data is still in memstore and server crashes then there is a possibility of data loss. This is tackeled by WAL. Before Inserts are done to memstore it is done to a WAL so that we can replay the logs from WAL since it is on disc(HDFS). One per region server.\n",
    "* **Block Cache** - We can also call it read cache. Whenever we read the data it is cached in memory so that it can directly fetch the data from memory for next read. One per region server.\n",
    "* **Zookeeper** - It is a coordinating service for various distributed systems. It is an open source distributed coordinator. Every server sends a heartbeat to zookeeper so that zookeeper will keep track. Also holds location of metatable(Table having the mapping of row_key, region and region servers). This metatable is present on one of the region servers. Zookeeper holds the track of the region server which is holding this metatable.\n",
    "* **Hmaster** - Hbase has master-slave architecture. Hmaste is master and region servers are slaves. Hmaster assigns regions to region server. In case of some failure or load increases Hmaster will try to balance the load by assigning some of the regions loaded in region server to other regions. Hbase an have one or more masters but only one Hmaster. Only one master is active as Hmaster at a time others are passive.\n",
    "\n",
    "**Hfiles**\n",
    "  1. Stores data in sorted key-value pairs.\n",
    "  2. These are immutable, once created they cannot be modified.\n",
    "  3. Large in size and depends on memstore size.\n",
    "  4. Stores data in set of blocks, So that reading is easy based on block indices.\n",
    "  5. Binary search is applied within the block to search the data since the data is stored in sorted manner.\n",
    "\n",
    "**MetaTable**\n",
    "  1. A datastructure that stores the location of the regions along with the region servers.\n",
    "  2. It helps user identify the region server and its coooesponding regions, where the specific range of key-value pairs are stored.\n",
    "  3. Meta table is stored in one of the region server and its location is in zookeeper.\n",
    "  \n",
    "  \n",
    "### HBase Read/Write Operations\n",
    "Steps involved are as follows\n",
    "\n",
    "1. Contact zookeeper to fetch the location of metatable(If cliend doesn't have the latest cached version of meta table).\n",
    "2. Client queries the meta table to find the location fo region server for a specific key-value pair.\n",
    "3. Client cached the region information and metatable information for future interactions.\n",
    "4. Client can now contact to the region server specifie in step 2. This region server assigns the request to a specific region for Read/Write operations.\n",
    "\n",
    "#### HBase Write Operations\n",
    "\n",
    "1. Data needs to be written to WAL.\n",
    "2. Once the data is written to WAL, It is written to memstore. Once the memestore is full data is flushed to HDFS in the form of Hfile.\n",
    "3. Finally an acknowledgement is sent back to client.\n",
    "\n",
    "#### Hbase Read Operations\n",
    "1. The region server first checks the block cache that stores the recently accessed data.\n",
    "2. If the data is not there in block cache, It checks for the required data in memstore.\n",
    "3. If data is not there in memstore then the only way is to read the Hfile from disc. Hfile containing a particular key-value pair is identified.\n",
    "\n",
    "Once the Hfile is identified instead of reading the entire file, The **Data Block** Index if Hfile is scanned to get the data-block with the key-value pair. A binary search in this data block finally returns the data or null if the data is not present.\n",
    "\n",
    "### Compactions\n",
    "Flushes from memstore in the form of Hfiles create multiple files. Specially, incase of heavy incoming writes which leads to two major problems:\n",
    "\n",
    "1. The read efficiency gets low. Because of som many files more disc seeks are required.\n",
    "2. It leads to dirty data. A large number of Hfiles and data redundency\n",
    "\n",
    "To solve the above problems Compaction is there. Compaction is a process of combining the many small Hfiles into one.\n",
    "\n",
    "Again these are of two types;\n",
    "1. Minor - Hbase picks some smaller files are writes them on few large Hfiles.\n",
    "2. Major - All smaller files are written in single large Hfile. This is resource intensive because it merges lot of Hfiles into one. So Hadoop admins runs this when the traffic is low.\n",
    "\n",
    "### Update/Delete in Hbase\n",
    "**Updates** are done using timestamp(versioning). We only get to see the latest version that's how updates are done.\n",
    "\n",
    "**Delete**: Deletes in Hbase are special type of Updates where the value for which delete is requested is not deleted immediately. Instead these values are masked by assigning a tombstone marker to them. Every request to read this value returns NULL to the client because of tombstone marker. Client thinks it is deleted.\n",
    "\n",
    "This is done because Hfiles are immutable. All the values with tombstone marker are removed during the next compaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4666575",
   "metadata": {},
   "source": [
    "### Practicals\n",
    "\n",
    "```shell\n",
    "[itv002768@g02 ~]$ hbase shell\n",
    "SLF4J: Class path contains multiple SLF4J bindings.\n",
    "SLF4J: Found binding in [jar:file:/opt/hadoop-3.3.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: Found binding in [jar:file:/opt/hbase-2.3.4/lib/client-facing-thirdparty/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
    "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
    "HBase Shell\n",
    "Use \"help\" to get list of supported commands.\n",
    "Use \"exit\" to quit this interactive shell.\n",
    "For Reference, please visit: http://hbase.apache.org/2.0/book.html#shell\n",
    "Version 2.3.4, rafd5e4fc3cd259257229df3422f2857ed35da4cc, Thu Jan 14 21:32:25 UTC 2021\n",
    "Took 0.0005 seconds\n",
    "hbase(main):001:0>\n",
    "```\n",
    "\n",
    "**If services are not running**\n",
    "\n",
    "service --status-all</br>\n",
    "and check status for \"hbase master\" and \"hbase region server\"\n",
    "\n",
    "for starting the service - service hbase-master restart | service hbase-regionserver restart\n",
    "\n",
    "After restarting the services start a new session of hbase shell\n",
    "\n",
    "\n",
    "```shell\n",
    "hbase(main):001:0> create 'students', 'personal_details', 'contact_details', 'marks'\n",
    "Created table students\n",
    "Took 1.6689 seconds\n",
    "=> Hbase::Table - students\n",
    "hbase(main):002:0> list 'students'\n",
    "TABLE\n",
    "students\n",
    "1 row(s)\n",
    "Took 0.0108 seconds\n",
    "=> [\"students\"]\n",
    "\n",
    "## put 'TABLE_NAME', 'COLUMN_FAMILY:KEY', 'VALUE' \n",
    "hbase(main):003:0> put 'students','student1','personal_details:name', 'Tushar'\n",
    "Took 0.4423 seconds\n",
    "hbase(main):004:0> put 'students','student1','personal_details:email', 'tushar5353@gmail.com'\n",
    "Took 0.0047 seconds\n",
    "\n",
    "hbase(main):006:0> scan 'students'\n",
    "ROW                                      COLUMN+CELL\n",
    " student1                                column=personal_details:email, timestamp=2022-07-26T13:06:11.017, value=tushar5353@gmail.com\n",
    " student1                                column=personal_details:name, timestamp=2022-07-26T13:05:46.913, value=Tushar\n",
    "1 row(s)\n",
    "Took 0.0608 seconds\n",
    "\n",
    "# To get a subset of a table let's say a particular row key\n",
    "hbase(main):008:0> get 'students', 'student1'\n",
    "COLUMN                                   CELL\n",
    " personal_details:email                  timestamp=2022-07-26T13:06:11.017, value=tushar5353@gmail.com\n",
    " personal_details:name                   timestamp=2022-07-26T13:05:46.913, value=Tushar\n",
    "1 row(s)\n",
    "Took 0.0775 seconds\n",
    "\n",
    "# To get info for a column family\n",
    "hbase(main):009:0> get 'students', 'student1', {COLUMN => 'personal_details'}\n",
    "COLUMN                                   CELL\n",
    " personal_details:email                  timestamp=2022-07-26T13:06:11.017, value=tushar5353@gmail.com\n",
    " personal_details:name                   timestamp=2022-07-26T13:05:46.913, value=Tushar\n",
    "1 row(s)\n",
    "Took 0.0114 seconds\n",
    "\n",
    "#select name from students where id='student1'\n",
    "hbase(main):010:0> get 'students', 'student1', {COLUMN => 'personal_details:name'}\n",
    "COLUMN                                   CELL\n",
    " personal_details:name                   timestamp=2022-07-26T13:05:46.913, value=Tushar\n",
    "1 row(s)\n",
    "Took 0.0063 seconds\n",
    "\n",
    "# Delete email from students where id='student1'\n",
    "hbase(main):012:0> delete 'students', 'student1', 'personal_details:email'\n",
    "Took 0.0218 seconds\n",
    "\n",
    "hbase(main):015:0* describe\n",
    "describe             describe_namespace\n",
    "hbase(main):015:0* describe 'students'\n",
    "Table students is ENABLED\n",
    "students\n",
    "COLUMN FAMILIES DESCRIPTION\n",
    "{NAME => 'contact_details', BLOOMFILTER => 'ROW', IN_MEMORY => 'false', VERSIONS => '1', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', COMPRES\n",
    "SION => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}\n",
    "\n",
    "{NAME => 'marks', BLOOMFILTER => 'ROW', IN_MEMORY => 'false', VERSIONS => '1', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', COMPRESSION => 'N\n",
    "ONE', TTL => 'FOREVER', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}\n",
    "\n",
    "{NAME => 'personal_details', BLOOMFILTER => 'ROW', IN_MEMORY => 'false', VERSIONS => '1', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', COMPRE\n",
    "SSION => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}\n",
    "\n",
    "3 row(s)\n",
    "Quota is disabled\n",
    "Took 0.0703 seconds\n",
    "\n",
    "hbase(main):016:0> exists 'students'\n",
    "Table students does exist\n",
    "Took 0.0178 seconds\n",
    "=> true\n",
    "\n",
    "\n",
    "# To drop a table we need to disable it first\n",
    "# Because data is present in memstore and is not flushed, When we disable the table the content of memstore is flushed to disc and then we can drop a table\n",
    "hbase(main):017:0> drop 'students'\n",
    "ERROR: Table students is enabled. Disable it first.\n",
    "\n",
    "For usage try 'help \"drop\"'\n",
    "\n",
    "Took 0.0121 seconds\n",
    "\n",
    "hbase(main):018:0> disable 'students'\n",
    "Took 0.7219 seconds\n",
    "hbase(main):019:0> drop 'students'\n",
    "Took 0.3693 seconds\n",
    "\n",
    "hbase(main):025:0> create 'census_itv002768', 'personal', 'professional'\n",
    "Created table census_itv002768\n",
    "Took 1.1288 seconds\n",
    "=> Hbase::Table - census_itv002768\n",
    "\n",
    "# use LIMIT => 1 to limit the rows\n",
    "# use STARTROW => 3, ENDROW => 5 will give you row 3rd and 4th\n",
    "\n",
    "[itv002768@g02 ~]$ hadoop fs -ls -h /hbase/data/default/xyz_itv001044/d3c8336f4005a4b64916ea030eb1aafe/cf1\n",
    "Found 1 items\n",
    "-rw-r--r--   3 hbase supergroup      4.8 K 2021-09-27 12:27 /hbase/data/default/xyz_itv001044/d3c8336f4005a4b64916ea030eb1aafe/cf1/dffdd255d0004a97954d9d79809875d6 ----- Hfile\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405ae39",
   "metadata": {},
   "source": [
    "### CAP Theorem\n",
    "\n",
    "CAP theorem applies to the distributed systems that store data.</br>\n",
    "It stands for:</br>\n",
    "**C - Consistency**</br>\n",
    "**A - Availability**</br>\n",
    "**P - Partition Tolerance**</br>\n",
    "CAP theorum says out of above three we can only get 2. We can only chose two of them and it's upto our requirement we have to build the system.</br>\n",
    "Three Combinations that are possible:</br>\n",
    "**CA**(Consistency and Availability)</br>\n",
    "**AP**(Availability and Partition tolerance)</br>\n",
    "**CP**(Consistency and Partition tolerance)</br>\n",
    "\n",
    "\n",
    "#### Consistency\n",
    "\n",
    "Each node will hold the latest value. Whenever we request for a value we'll get the latest one and thats guranteed it cannot give us the old value as result.</br>\n",
    "Let's say you are sending a messge to someone on whatsapp. If that person is offline then He'll not get any message or you can say He should not get old or garbage data. He should always get the latest one.\n",
    "\n",
    "#### Availability\n",
    "System should always give a response. Even if it not sure of having latest value it should give response. There is no gurantee of latest value.\n",
    "\n",
    "\n",
    "#### Partition Toleance\n",
    "\n",
    "A system will continue to operate even there is a network failure.\n",
    "\n",
    "\n",
    "Consistency and Availability is provided by RDBMS\n",
    "\n",
    "When we talk about distributed system we have to make sure that partition tolerance is there. So the tradeoff will always be between Availability and Consistency that means we've to chose one out of two. So all the distrubited systems falls under two categories AP and CP.\n",
    "\n",
    "NoSQL systems: The data is stored in distributed manner across a cluster of interconnected machines and provide network partitioning. There are two flavoures of NoSQL databases that provide two set of gurantees:\n",
    "\n",
    "1. Consistency and Partitioning Tolerance (Hbase and MongoDB)\n",
    "2. Availability and partition Tolerance (Cassendra and DynamoDB)\n",
    "\n",
    "If we chose Availability over consistency then the system processes the query and provide some information. Even if the data is not latest. Need immediate results even if not the latest. e.g hotel booking app.\n",
    "\n",
    "Let's try to prove CAP theorem by contradiction.\n",
    "\n",
    "For time being let's assume all three are possible. A distributed Data store posses all three.\n",
    "\n",
    "Same object is there on two nodes that are connected by a network. Let's say there is a network issue and machine not able to contact each other. Some user is updating the value in machine one to V1 from V0. Another user contacted the machine2 He'll recieve the old value. This is availability but not consistency. To get the latest value network should be fixed between two machines. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2fd635",
   "metadata": {},
   "source": [
    "## Cassandra\n",
    "Cassandra is a distributed column oriented database and is highly performant and highly scalable. We use Hbase when we require transactional activities or need quick retrieval.\n",
    "\n",
    "* CAP in case of Cassandra\n",
    "\n",
    "Hbase - CP(Consistency and Partition tolerance)\n",
    "Cassandra - AP(Availibility and Partition tolerance)\n",
    "\n",
    "Linkedin - Consider you've 100 likes in your post. After one more like it should show 101 likes. It will show someone liked it but won't increase the counter. After sometime it will update it. In this case we can use cassandra, It wont't provide you the consistency but will provide you the eventual consistency. but won't give you error or something.\n",
    "\n",
    "\n",
    "* How a cassandra cluster looks like?\n",
    "\n",
    "There is no master node, All nodes are peers. It is a decentralized architecture. In case of Hbase we run on hadoop cluster and it also provide master-slave architecture where master is hmaster and slave is region server.\n",
    "\n",
    "For communication among the peers it uses gossip protocol to know what is present on another machine.\n",
    "\n",
    "In master-slave architecture there can be downtimes if master fails(secondary master can also go down risk is always involved). But in case of Cassandra there is no master, In this way this model is highly Available.\n",
    "\n",
    "* Tunable Read/Write Consistency:</br>\n",
    "By default cassandra compromises on Consistency to be highly Available.\n",
    "- Client will send the request to get the value of A\n",
    "- Request will got to one of the machines(e.g Node5)\n",
    "- Node 5 will go and talk to Node 1 to get the result\n",
    "- Note 5 will return the result to client even though if the result is latest or not\n",
    "\n",
    "Cassandra provides you tunable consistency. e.g result if n nodes agree on same result. Or Quorum level(multiple machines should have same result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a47032",
   "metadata": {},
   "source": [
    "### Cassandra vs Hbase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d113d69b",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "  .reveal p {\n",
    "    text-align: left;\n",
    "  }\n",
    "  .reveal ul {\n",
    "    display: block;\n",
    "  }\n",
    "  .reveal ol {\n",
    "    display: block;\n",
    "  }  \n",
    "</style>\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Cassandra</th>\n",
    "<th>Hbase</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    " \n",
    "* NoSQl distributed DB and hold the data in columnar fashion.\n",
    "* Highly Sclable.\n",
    "* Can be used with transactions with Update, inserts and reads with low latency.\n",
    "* Cassandra is having Decentralize architecture, there is no master.\n",
    "* It is highly available as it doesn't have dependence on master.\n",
    "* It provides AP(Availability and Parittion tolerance) and eventual consistency.\n",
    "* It provides tunable consistancy.\n",
    "* Cassandra has a separate cluster for keeping data.\n",
    "* It has its own query languate(CQL).\n",
    "   \n",
    "</td>\n",
    "<td>\n",
    "    \n",
    "* NoSQl distributed DB and hold the data in columnar fashion.\n",
    "* Highly Sclable.\n",
    "* Can be used with transactions with Update, inserts and reads with low latency.\n",
    "* It has master-slave architecture.\n",
    "* It provides CP(Consistency and Partition Tolerance).\n",
    "* It runs on top of Hadoop Cluster, Data is kept in HDFS.\n",
    "* Perfect choice if you're working on Hadoop.\n",
    "\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7598d1",
   "metadata": {},
   "source": [
    "\n",
    "* Sometimes Syntax looks hard in that case you can use apache phoenix. It provides you a wrappper on top of Hbase so that you can write simplq SQL to query Hbase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03bd78",
   "metadata": {},
   "source": [
    "## Hive-Hbase Integration\n",
    "\n",
    "We'll Create a table which we can access from both Hive and Hbase. We want to access this table in Hive and do some aggregations or any processing. We will access this table from Hbase when quick searches are required or Insert/Update.\n",
    "In the above cases we can create Hbase table managed by hive.\n",
    "\n",
    "##### Use case of Hive-Hbase table(a.k.a Hbase table managed by hive)\n",
    "- On Hbase table if we want to do any processing like groupby, aggregation or any kind of map-reduce activity its better to create hbase table managed by Hive\n",
    "- If you are doing some processing on hive and after the processing is done you want to dump the data in Hive for quick searches.\n",
    "\n",
    "##### Practical\n",
    "- We'll have one dataset kv1.txt and we'll be creating a hive table based on structure of kv1.txt file.\n",
    "```shell\n",
    "[itv002768@g02 week7_dataset]$ head kv1-200927-183907.txt\n",
    "238\u0001val_238\n",
    "86\u0001val_86\n",
    "311\u0001val_311\n",
    "27\u0001val_27\n",
    "165\u0001val_165\n",
    "409\u0001val_409\n",
    "255\u0001val_255\n",
    "278\u0001val_278\n",
    "98\u0001val_98\n",
    "484\u0001val_484\n",
    "[itv002768@g02 week7_dataset]$ hive\n",
    "SLF4J: Class path contains multiple SLF4J bindings.\n",
    "SLF4J: Found binding in [jar:file:/opt/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: Found binding in [jar:file:/opt/hadoop-3.3.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
    "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
    "Hive Session ID = d06750c2-9909-4904-a934-f20c6baa9a37\n",
    "\n",
    "Logging initialized using configuration in file:/opt/apache-hive-3.1.2-bin/conf/hive-log4j2.properties Async: true\n",
    "Hive Session ID = ad30fbd4-1caa-490c-9bd6-0912ad7df46d\n",
    "Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
    "hive> create table pokes(foo int, bar string);\n",
    "FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. AlreadyExistsException(message:Table hive.default.pokes already exists)\n",
    "hive> use tushar_test;\n",
    "OK\n",
    "Time taken: 0.129 seconds\n",
    "hive> create table pokes(foo int, bar string);\n",
    "OK\n",
    "Time taken: 0.361 seconds\n",
    "```\n",
    "\n",
    "- Loading the file kv1.txt in the hive table created in previous step.\n",
    "\n",
    "```shell\n",
    "hive> load data local inpath '/home/itv002768/week7_dataset/kv1-200927-183907.txt' overwrite into table pokes;\n",
    "Loading data to table tushar_test.pokes\n",
    "OK\n",
    "Time taken: 2.215 seconds\n",
    "```\n",
    "\n",
    "- Verifying the data in the table.\n",
    "\n",
    "```shell\n",
    "hive> select * from pokes limit 10;\n",
    "OK\n",
    "238     val_238\n",
    "86      val_86\n",
    "311     val_311\n",
    "27      val_27\n",
    "165     val_165\n",
    "409     val_409\n",
    "255     val_255\n",
    "278     val_278\n",
    "98      val_98\n",
    "484     val_484\n",
    "Time taken: 15.371 seconds, Fetched: 10 row(s)\n",
    "```\n",
    "- Create a Hbase table managed by Hive which we can access from both Hive and Hbase.\n",
    "\n",
    "*CREATE TABLE hbase_table_1(key int, value string)</br>\n",
    "STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</br>\n",
    "WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,cf1:val\") **key corresponds to key in create table and val also**</br>\n",
    "TBLPROPERTIES (\"hbase.table.name\" = \"tushar_shared_table\") **In hive, table name will be hbase_table_1 and in hbase we can see it as 'tushar_shared_table' - optional(if not provided table name in hbase will be same as that of hive)**</br>*\n",
    "\n",
    "```shell\n",
    "hive> CREATE TABLE hbase_table_1(key int, value string)\n",
    "    > STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n",
    "    > WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,cf1:val\")\n",
    "    > TBLPROPERTIES (\"hbase.table.name\" = \"tushar_shared_table\");\n",
    "2022-07-29 07:52:22,113 INFO  [d49fb0d7-016f-4072-b6f0-79f1f1d9e3c2 main] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x100bba26 connecting to ZooKeeper ensemble=m01.itversity.com:2181,m02.itversity.com:2181,w01.itversity.com:2181\n",
    "2022-07-29 07:52:24,434 INFO  [d49fb0d7-016f-4072-b6f0-79f1f1d9e3c2 main] client.HBaseAdmin: Operation: CREATE, Table Name: default:tushar_shared_table completed\n",
    "OK\n",
    "Time taken: 2.588 seconds\n",
    "```\n",
    "\n",
    "\n",
    "- Load the data from normal Hive table to table created in previous table.\n",
    "\n",
    "```shell\n",
    "hive> INSERT OVERWRITE TABLE hbase_table_1 select * from pokes where foo=98;\n",
    "Query ID = itv002768_20220729075409_265ead69-b770-46ec-a72e-12dce8d6640d\n",
    "Total jobs = 1\n",
    "Launching Job 1 out of 1\n",
    "Number of reduce tasks is set to 0 since there\\'s no reduce operator\n",
    "2022-07-29 07:54:26,484 WARN  [d49fb0d7-016f-4072-b6f0-79f1f1d9e3c2 main] mapreduce.TableMapReduceUtil: The addDependencyJars(Configuration, Class<?>...) method has been deprecated since it is easy to use incorrectly. Most users should rely on addDependencyJars(Job) instead. See HBASE-8386 for more details.\n",
    "2022-07-29 07:54:26,486 WARN  [d49fb0d7-016f-4072-b6f0-79f1f1d9e3c2 main] mapreduce.TableMapReduceUtil: The addDependencyJars(Configuration, Class<?>...) method has been deprecated since it is easy to use incorrectly. Most users should rely on addDependencyJars(Job) instead. See HBASE-8386 for more details.\n",
    "2022-07-29 07:54:28,519 INFO  [d49fb0d7-016f-4072-b6f0-79f1f1d9e3c2 main] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x7fcbc336 connecting to ZooKeeper ensemble=m01.itversity.com:2181,m02.itversity.com:2181,w01.itversity.com:2181\n",
    "Starting Job = job_1658918988971_0779, Tracking URL = http://m02.itversity.com:19088/proxy/application_1658918988971_0779/\n",
    "Kill Command = /opt/hadoop/bin/mapred job  -kill job_1658918988971_0779\n",
    "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 0\n",
    "2022-07-29 07:54:49,596 Stage-2 map = 0%,  reduce = 0%\n",
    "2022-07-29 07:54:54,873 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.87 sec\n",
    "MapReduce Total cumulative CPU time: 2 seconds 870 msec\n",
    "Ended Job = job_1658918988971_0779\n",
    "MapReduce Jobs Launched:\n",
    "Stage-Stage-2: Map: 1   Cumulative CPU: 2.87 sec   HDFS Read: 20143 HDFS Write: 0 SUCCESS\n",
    "Total MapReduce CPU Time Spent: 2 seconds 870 msec\n",
    "OK\n",
    "Time taken: 47.877 seconds\n",
    "hive> select * from pokes where foo=98;\n",
    "OK\n",
    "98      val_98\n",
    "98      val_98\n",
    "Time taken: 14.213 seconds, Fetched: 2 row(s)\n",
    "hive> select * from hbase_table_1 where key=98;\n",
    "OK\n",
    "2022-07-29 07:56:08,108 INFO  [d49fb0d7-016f-4072-b6f0-79f1f1d9e3c2 main] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x45482f82 connecting to ZooKeeper ensemble=m01.itversity.com:2181,m02.itversity.com:2181,w01.itversity.com:2181\n",
    "2022-07-29 07:56:08,195 INFO  [d49fb0d7-016f-4072-b6f0-79f1f1d9e3c2 main] mapreduce.RegionSizeCalculator: Calculating region sizes for table \"tushar_shared_table\".\n",
    "2022-07-29 07:56:08,754 INFO  [d49fb0d7-016f-4072-b6f0-79f1f1d9e3c2 main] client.ConnectionImplementation: Closing zookeeper sessionid=0x3017a13950e0bfd\n",
    "2022-07-29 07:56:08,790 INFO  [d49fb0d7-016f-4072-b6f0-79f1f1d9e3c2 main] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x1d4c6e32 connecting to ZooKeeper ensemble=m01.itversity.com:2181,m02.itversity.com:2181,w01.itversity.com:2181\n",
    "2022-07-29 07:56:08,810 INFO  [d49fb0d7-016f-4072-b6f0-79f1f1d9e3c2 main] mapreduce.TableInputFormatBase: Input split length: 0 bytes.\n",
    "2022-07-29 07:56:08,849 INFO  [d49fb0d7-016f-4072-b6f0-79f1f1d9e3c2 main] client.ConnectionImplementation: Closing zookeeper sessionid=0x200000102340024\n",
    "98      val_98\n",
    "Time taken: 9.482 seconds, Fetched: 1 row(s)\n",
    "\n",
    "hbase(main):004:0> scan \"tushar_shared_table\"\n",
    "ROW                                      COLUMN+CELL\n",
    " 98                                      column=cf1:val, timestamp=2022-07-29T07:54:54.096, value=val_98\n",
    "1 row(s)\n",
    "Took 0.1556 seconds\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
