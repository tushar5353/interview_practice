{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bfaf9a",
   "metadata": {},
   "source": [
    "Table Of Contents\n",
    "=================\n",
    "* What is spark?\n",
    "  - [Compute Engine](#Compute-Engine)\n",
    "  - [In-memory](#In-memory)\n",
    "  - [General Purpose](#General-Purpose)  \n",
    "* [RDD](#RDD)(Resilient Distributed Dataset) </BR>\n",
    "  Properies of RDD\n",
    "  - [Resilient](#Resilient)\n",
    "  - [Immutable](#Immutable)\n",
    "  - [Lazy Transformations](#Lazy-Transformations)\n",
    "* [Practicals - Spark](#Practicals)\n",
    "  - [Find Word Count in a file](#Find-Word-Count-in-a-file)\n",
    "  - [Find Top Customers](#Find-Top-Customers)\n",
    "  - [Find Movie Ratings](#Find-Movie-Ratings)\n",
    "  - [Find Average Linkedin Connections](#Find-Average-Linkedin-Connections)\n",
    "* [Practicals - pyspark](#Practicals---pyspark)\n",
    "  - [Find Word Count in a file](#Find-Word-Count-in-a-file-pyspark)\n",
    "  - [Find Top Customers](#Find-Top-Customers-pyspark)\n",
    "  - [Find Movie Ratings](#Find-Movie-Ratings-pyspark)\n",
    "  - [Find Average Linkedin Connections](#Find-Average-Linkedin-Connections-pyspark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5079c9bc",
   "metadata": {},
   "source": [
    "What is Apache Spark?\n",
    "Apache spark is a general puropose in-mempory compute engine.\n",
    "\n",
    "### Compute Engine\n",
    "\n",
    "Hadoop provides three things:\n",
    "* hdfs - storage\n",
    "* mapReduce - computation\n",
    "* YARN - resource manager\n",
    "\n",
    "In the above scenerio spark takes place of mapReduce and is an alternative of compute engine. It's not like it will be used in place of hadoop ecosystem.\n",
    "\n",
    "Spark alone cannot do anything, It requires tow things to work:\n",
    "1. Storage\n",
    "2. Resource Manager\n",
    "\n",
    "In this way it is a plug and play compute engine. But it is not bundled by default with hadoop ecosystem.\n",
    "Spark is flexible enought so that it can use local storage, s3, google cloud storage, hdfs etc.\n",
    "\n",
    "For resource manager it can use YARN, mesos, Kubernates.\n",
    "\n",
    "### In-memory\n",
    "Normally in production there won't be a single mapReduce job there will be a chain of mapreduce jobs. e.g mr1, mr2 ..... mr5.\n",
    "\n",
    "And these mr jobs will take the inputs form hdfs that is again reading the data from discs and writing the data to discs. So for each map reduce job two disc I/O is required. one for read and one for write. This is a kind of bottleneck with mapreduce where lot of disc I/Os are required.\n",
    "\n",
    "In case of spark, we'll load the data in some variable V1 that is in memory, do the processing and output will be assigned to variable V2 that is again in a memory. Same goes with the chain of variables. So the data is read from the disc in the very starting and finally in the end for writing the output data. So only two disc I/Os are required. That's why it is said to be 10 to 100 times faster than mapReduce.\n",
    "\n",
    "### General Purpose\n",
    "In hadoop, if you want to clean the data we'll use pig, for querying we'll use hive, for ML its mahout for each of the thing we've to learn different tools and all of them are bound to Map and Reduce. Whether if the problem fits in Map Reduce or not all the above tools will use mapReduce.\n",
    "\n",
    "In spark, we have to learn just only style of writing the code. Things like cleaning, data ingestion, querying etc can happen with a single thing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca975073",
   "metadata": {},
   "source": [
    "## RDD\n",
    "\n",
    "The basic unit that holds the data in spark is called as **RDD(Resilient Distributed Dataset)**.\n",
    "\n",
    "Let's say we have created a list with some elements in it in scala, it will be kept in memory on a system. But if there are too many elements or you can say the data is too much we can put it in a List and distribute it among the n number of systems. Roughly RDD is in-memory distributed collection.\n",
    "\n",
    "Basic Program flow in Spark(pseudoc-code)\n",
    "\n",
    "```\n",
    "rdd1 = load file1 from hdfs\n",
    "rdd2 = rdd1.map // not literaly map, just transformation of data\n",
    "rdd3 = rdd2.filter // Everytime we do some operation we create a new rdd\n",
    "rdd3.collect() // if rdd2 is the final answer collect it\n",
    "\n",
    "No calcullation will ot happen on first three lines, everything will happen on the last line\n",
    "All the steps will be added in a stack or a flow and when we call collect() things will start\n",
    "executing.\n",
    "\n",
    "We can call this flow as DAG(Directed acylic Graph), DAG is nothing but an execution plan.\n",
    "```\n",
    "\n",
    "There are two kinds of operations in Spark:\n",
    "1. Transformations\n",
    "In the above pseudo code, first three lines are transformations and all the transforamtions are lazy. Every transformation is added to a DAG and once you call collect it will start executing.\n",
    "\n",
    "2. Actions\n",
    "In the above pseudo code, last line(collect()) is an action.\n",
    "\n",
    "\n",
    "Let's say we have a four node hadoop cluster and we are having a dataset 500 MB. Based on default block size four blocks will be made with 1 block on each node. \n",
    "\n",
    "In spark when we say load the data in one variable rdd1 from files then the data will be loaded in memory(RAM) of each node. We call this as partition, so we have four partitions for four blocks. These all the partitions together are known as RDD, in-memory distributed across nodes. If we loose an RDD we can again recover it that's why it is **Resilient**.\n",
    "\n",
    "### Resilient\n",
    "**How RDDs are Resilient?**</BR>\n",
    "Resilient is the ability to quickly recover.\n",
    "\n",
    "Consider we have rdd1 and we're doing some operation and generatiog rdd2 in same way rddd3. IF rdd3 is lost then how can we recover it.\n",
    "\n",
    "In this case we already know that how rdd3 is created. It will search for it's parent rdd using lineage graph and it will quickly apply the transformation to it to regenerate the rdd3. So, RDD provides fault tolerance throught Lineage Graph that keeps a track of transformations to be executed after an action has been called.\n",
    "\n",
    "In HDFS we get this using replication, but in case of RDD we cannot do this because memory is not cheap.\n",
    "\n",
    "### Immutable\n",
    "**RDDs are Immutable</BR>**\n",
    "Once we load the data in RDD, it cannot be changed. Why? Because if we want to recover a child RDD we have to use the parent RDD, if the data is getting overwritten in same RDD we cannot do that. Immutability and Lineage gives us the power to regenerate RDD during failure.\n",
    "\n",
    "### Lazy Transformations\n",
    "**Why transformations are Lazy?</BR>**\n",
    "Assume that transformations are not lazy. Now, consider you have 1GB file in HDFS.\n",
    "\n",
    "```\n",
    "rdd1 = load file1 in hdfs\n",
    "rdd1.printl(line1)\n",
    "```\n",
    "\n",
    "In the above case, to print the first line we loaded the whole 1GB file in memory.\n",
    "\n",
    "Now consider that spark is using lazy transformations(fact). Nothing will happen, An entry to the DAG is loaded. In this case spark knows that user is trying to print the first line so only first line will be loaded in the memory.\n",
    "\n",
    "Another example:\n",
    "\n",
    "File1 is in HDFS wil 1M lines\n",
    "\n",
    "```\n",
    "rdd1 = load file1 from hdfs\n",
    "rdd2 = rdd1.map\n",
    "rdd3 = rdd2.filter\n",
    "rdd2.collect\n",
    "```\n",
    "\n",
    "In the above example rdd1 will be loaded with 1M lines or RDD is filled with data(RDD is materialized). In second line 1M lines will be processed by MAP and produces an ouput of 1M lines.In line 3 we are applying the filter and there is a possibility that we'll get only 5 records on filter. It means we are interested in those five records. So for those five records we are loading all the 1M lines in the memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c59f3d",
   "metadata": {},
   "source": [
    "## Practicals\n",
    "\n",
    "### Find Word Count in a file\n",
    "We've to find the frequency of words which resides in hdfs.\n",
    "\n",
    "Create a file file1 and put some text in it\n",
    "\n",
    "Move file in hdfs\n",
    "\n",
    "```bash\n",
    "[itv002768@g02 ~]$ hadoop fs -mkdir spark_wordcount_test\n",
    "[itv002768@g02 ~]$ hadoop fs -put file1 spark_wordcount_test\n",
    "[itv002768@g02 ~]$ hadoop fs -ls spark_wordcount_test\n",
    "Found 1 items\n",
    "-rw-r--r--   3 itv002768 supergroup       1673 2022-08-07 06:03 spark_wordcount_test/file1\n",
    "\n",
    "[itv002768@g02 ~]$ hadoop fs -head spark_wordcount_test/file1\n",
    "this is very interesting\n",
    "this is very interesting\n",
    "this is very interesting\n",
    "this is very interesting\n",
    "this is very interesting\n",
    "this is very interesting\n",
    "this is very interesting\n",
    "this is very interesting\n",
    "this is very interesting\n",
    "\n",
    "[itv002768@g02 ~]$ spark-shell\n",
    "Multiple versions of Spark are installed but SPARK_MAJOR_VERSION is not set\n",
    "Spark2 will be picked by default\n",
    "SLF4J: Class path contains multiple SLF4J bindings.\n",
    "SLF4J: Found binding in [jar:file:/opt/spark-2.4.7-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: Found binding in [jar:file:/opt/hadoop-3.3.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
    "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "22/08/07 06:12:57 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
    "22/08/07 06:13:02 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
    "Spark context Web UI available at http://g02.itversity.com:4040\n",
    "Spark context available as 'sc' (master = yarn, app id = application_1658918988971_4431).\n",
    "Spark session available as 'spark'.\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
    "      /_/\n",
    "\n",
    "Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_282)\n",
    "Type in expressions to have them evaluated.\n",
    "Type :help for more information.\n",
    "\n",
    "scala>\n",
    "```\n",
    "In the above commands we can see the following line</BR>\n",
    "Spark context available as 'sc' (master = yarn, app id = application_1658918988971_4431).\n",
    "\n",
    "sc is the entry point to the cluster, if you do not use sc then it won't use the cluster. It gives you the capability to run the code on cluster.\n",
    "\n",
    "Basic unit which holds the data in spark is know as RDD. Our file is in hdfs so first step would be to load the data in rdd\n",
    "\n",
    "Here, I'm using spark kernel to run the commands you can also use spark-shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc21c896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "rdd1 = /user/itv002768/spark_wordcount_test/file1 MapPartitionsRDD[1] at textFile at <console>:27\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "/user/itv002768/spark_wordcount_test/file1 MapPartitionsRDD[1] at textFile at <console>:27"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.textFile(\"/user/itv002768/spark_wordcount_test/file1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7919efe",
   "metadata": {},
   "source": [
    "FlatMap takes each line as input and whatever we'll mention it'll do that. Like below we are splitting the words on space. It will create an array of words.\n",
    "\n",
    "Array(Array(line1), Array(line2), Array(line3))\n",
    "\n",
    "But flatMap will flatten the 2D array and we'll have a single Array containing all the words in the input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a6c6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd2 = MapPartitionsRDD[2] at flatMap at <console>:27\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[2] at flatMap at <console>:27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd2 = rdd1.flatMap(x => x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "689435bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd3 = MapPartitionsRDD[3] at map at <console>:27\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[3] at map at <console>:27"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd3 = rdd2.map(x => (x,1)) // for each word it will create a mapping with one e.g (this, 1), (is, 1) ....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0137a6a7",
   "metadata": {},
   "source": [
    "How reduceByKey works:\n",
    "\n",
    "First it will put together all the key/value pairs sorted by keys. It always works on two rows at a time.\n",
    "\n",
    "Here, x is first row and y is second row. On the right side it will add the values(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e549a006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd4 = ShuffledRDD[4] at reduceByKey at <console>:27\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[4] at reduceByKey at <console>:27"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd4 = rdd3.reduceByKey((x, y) => x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f287dab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((his,1), (this,25), (is,26), (am,34), (interestingt,1), (\"\",3), (there,34), (going,34), (very,26), (interesting,25), (not,34), (it,34), (i,34), (isort,34))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152ac4cb",
   "metadata": {},
   "source": [
    "**finding the wordcount using pyspark**\n",
    "\n",
    "```python\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
    "      /_/\n",
    "\n",
    "Using Python version 2.7.5 (default, Nov 16 2020 22:23:17)\n",
    "SparkSession available as 'spark'.\n",
    ">>> rdd1 = spark.textFile(\"/user/itv002768/spark_wordcount_test/file1\")\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "AttributeError: 'SparkSession' object has no attribute 'textFile'\n",
    ">>> rdd1 = sc.textFile(\"/user/itv002768/spark_wordcount_test/file1\")\n",
    ">>> rdd2 = rdd1.flatMap(lambda x: x.split(\" \"))\n",
    ">>> rdd3 = rdd2.map(lambda x: (x,1))\n",
    ">>> rdd4 = rdd3.reduceByKey(lambda x, y:  x+y)\n",
    ">>> rdd4.collect()\n",
    "[(u'', 3), (u'very', 26), (u'i', 34), (u'is', 26), (u'am', 34), (u'this', 25), (u'isort', 34), (u'not', 34), (u'interestingt', 1), (u'going', 34), (u'his', 1), (u'there', 34), (u'it', 34), (u'interesting', 25)]\n",
    "\n",
    "\"\"\"\n",
    "Instead of writing collect, we use saveAsTextFile to save the output in a file.\n",
    "\n",
    "rdd4.saveAsTextFile('/user/itv002768/spark_wordcount_test_result')\n",
    "\"\"\"\n",
    "\n",
    ">>> rdd4.saveAsTextFile('/user/itv002768/spark_wordcount_test_result')\n",
    ">>>\n",
    "[itv002768@g02 ~]$ hadoop fs -ls spark_wordcount_test_result\n",
    "Found 3 items\n",
    "-rw-r--r--   3 itv002768 supergroup          0 2022-08-07 07:05 spark_wordcount_test_result/_SUCCESS\n",
    "-rw-r--r--   3 itv002768 supergroup        121 2022-08-07 07:05 spark_wordcount_test_result/part-00000\n",
    "-rw-r--r--   3 itv002768 supergroup         75 2022-08-07 07:05 spark_wordcount_test_result/part-00001\n",
    "[itv002768@g02 ~]$ hadoop fs -cat spark_wordcount_test_result/*\n",
    "(u'', 3)\n",
    "(u'i', 34)\n",
    "(u'is', 26)\n",
    "(u'am', 34)\n",
    "(u'this', 25)\n",
    "(u'not', 34)\n",
    "(u'isort', 34)\n",
    "(u'very', 26)\n",
    "(u'interestingt', 1)\n",
    "(u'going', 34)\n",
    "(u'his', 1)\n",
    "(u'there', 34)\n",
    "(u'it', 34)\n",
    "(u'interesting', 25)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a475f5",
   "metadata": {},
   "source": [
    "Below code is used to read the file from local path and process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7823d048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "rdd1 = /user/itv002768/spark_wordcount_test/file1 MapPartitionsRDD[1] at textFile at <console>:27\n",
       "rdd2 = MapPartitionsRDD[2] at flatMap at <console>:28\n",
       "rdd3 = MapPartitionsRDD[3] at map at <console>:29\n",
       "rdd4 = ShuffledRDD[4] at reduceByKey at <console>:30\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array((his,1), (this,25), (is,26), (am,34), (interestingt,1), (\"\",3), (there,34), (going,34), (very,26), (interesting,25), (not,34), (it,34), (i,34), (isort,34))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.textFile(\"/user/itv002768/spark_wordcount_test/file1\")\n",
    "val rdd2 = rdd1.flatMap(x => x.split(\" \"))\n",
    "val rdd3 = rdd2.map(x => (x,1))\n",
    "val rdd4 = rdd3.reduceByKey((x, y) => x+y)\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e969ebd",
   "metadata": {},
   "source": [
    "We'll again run the same code example but this time using scala ide.\n",
    "\n",
    "Please refer the Video **Spark Fundamental Practical - 2** for detailed information since ide settings related to versions and jars are also there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6182b2",
   "metadata": {},
   "source": [
    "```scala\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.log4j.Level\n",
    "import org.apache.log4j.Logger\n",
    "\n",
    "object wordcount {\n",
    "  def main(args: Array[String]){\n",
    "    System.setProperty(\"hadoop.home.dir\", \"C:/Hadoop\")\n",
    "    Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "    // local[*] - cluster is on local and use all the cpu cores\n",
    "    // wordcount - name of application, we can give any name\n",
    "    val sc = new SparkContext(\"local[*]\", \"wordcount\")\n",
    "    val input = sc.textFile(\"C:/Users/tushar.sharma/bigdata/spark_practice/search_data.txt\")\n",
    "    val allWords = input.flatMap(x => x.split(\" \"))\n",
    "    val wordsValue = allWords.map(x => (x,1))\n",
    "    val wordsFrequency = wordsValue.reduceByKey((x, y) => x+y)\n",
    "    wordsFrequency.collect.foreach(println)\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba2188",
   "metadata": {},
   "source": [
    "**Below program to convert all the words to lowercase and then count**\n",
    "\n",
    "```scala\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.log4j.Level\n",
    "import org.apache.log4j.Logger\n",
    "\n",
    "object wordcount {\n",
    "  def main(args: Array[String]){\n",
    "    System.setProperty(\"hadoop.home.dir\", \"C:/Hadoop\")\n",
    "    Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "    // local[*] - cluster is on local and use all the cpu cores\n",
    "    // wordcount - name of application, we can give any name\n",
    "    val sc = new SparkContext(\"local[*]\", \"wordcount\")\n",
    "    val input = sc.textFile(\"C:\\\\Users\\\\tushar.sharma\\\\bigdata\\\\spark_practice\\\\search_data.txt.txt\")\n",
    "    val allWords = input.flatMap(_.split(\" \"))\n",
    "    val lowerCaseWords = allWords.map(_.toLowerCase())\n",
    "    val wordsValue = lowerCaseWords.map(x => (x,1))\n",
    "    val wordsFrequency = wordsValue.reduceByKey((x, y) => x+y)\n",
    "    \n",
    "    wordsFrequency.collect.foreach(println)\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**In case if we want to get the top ten words, we've to sort on values of output</BR>\n",
    "But there is No such way where we can sort on keys but not sort on values\n",
    "In this case we'll replace key with value, sort by key and then show the results**\n",
    "\n",
    "```scala\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.log4j.Level\n",
    "import org.apache.log4j.Logger\n",
    "\n",
    "object wordcount {\n",
    "  def main(args: Array[String]){\n",
    "    System.setProperty(\"hadoop.home.dir\", \"C:/Hadoop\")\n",
    "    Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "    // local[*] - cluster is on local and use all the cpu cores\n",
    "    // wordcount - name of application, we can give any name\n",
    "    val sc = new SparkContext(\"local[*]\", \"wordcount\")\n",
    "    val input = sc.textFile(\"C:\\\\Users\\\\tushar.sharma\\\\bigdata\\\\spark_practice\\\\search_data.txt.txt\")\n",
    "    val allWords = input.flatMap(_.split(\" \"))\n",
    "    val lowerCaseWords = allWords.map(_.toLowerCase())\n",
    "    val wordsValue = lowerCaseWords.map(x => (x,1))\n",
    "    val wordsFrequency = wordsValue.reduceByKey((x, y) => x+y)\n",
    "    val replaceKeyWithVal = wordsFrequency.map(x => (x._2, x._1))\n",
    "    val finalCount_ = replaceKeyWithVal.sortByKey(false) // We want it in decending order that's why we gave false\n",
    "    val finalCount = finalCount_.map(x => (x._2, x._1))\n",
    "    finalCount.collect.foreach(println)\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**If you want to sort by second column of the touple use soryBy function**\n",
    "```scala\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.log4j.Level\n",
    "import org.apache.log4j.Logger\n",
    "\n",
    "object wordcount {\n",
    "  def main(args: Array[String]){\n",
    "    System.setProperty(\"hadoop.home.dir\", \"C:/Hadoop\")\n",
    "    Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "    // local[*] - cluster is on local and use all the cpu cores\n",
    "    // wordcount - name of application, we can give any name\n",
    "    val sc = new SparkContext(\"local[*]\", \"wordcount\")\n",
    "    val input = sc.textFile(\"C:\\\\Users\\\\tushar.sharma\\\\bigdata\\\\spark_practice\\\\search_data.txt.txt\")\n",
    "    val allWords = input.flatMap(_.split(\" \"))\n",
    "    val lowerCaseWords = allWords.map(_.toLowerCase())\n",
    "    val wordsValue = lowerCaseWords.map(x => (x,1))\n",
    "    val wordsFrequency = wordsValue.reduceByKey((x, y) => x+y).sortBy(x => x._2)\n",
    "    wordsFrequency.collect.foreach(println)\n",
    "  }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc739b",
   "metadata": {},
   "source": [
    "### Find Top Customers\n",
    "\n",
    "Below are the few lines from customerorders file, where columns are customer_id, product_id, amount_spend\n",
    "\n",
    "There can be repeated rows for same customer who baught some product with product_id x.\n",
    "\n",
    "Problem statement - Find the top ten customers who spend the most amount from shopping.\n",
    "\n",
    "**Note:** I've written and executed the code here, you can run the same code in IDE and make sure that the input files are in place.\n",
    "\n",
    "```bash\n",
    "[itv002768@g02 ~]$ hadoop fs -mkdir customerorders_practical\n",
    "[itv002768@g02 ~]$ hadoop fs -put customer\n",
    "customerorders-201008-180523.csv  customers.java\n",
    "[itv002768@g02 ~]$ hadoop fs -put customerorders-201008-180523.csv customerorders_practical\n",
    "[itv002768@g02 ~]$ hadoop fs -ls customerorders_practical\n",
    "Found 1 items\n",
    "-rw-r--r--   3 itv002768 supergroup     146855 2022-08-08 13:25 customerorders_practical/customerorders-201008-180523.csv\n",
    "[itv002768@g02 ~]$ hadoop fs -head customerorders_practical/customerorders-201008-180523.csv\n",
    "44,8602,37.19\n",
    "35,5368,65.89\n",
    "2,3391,40.64\n",
    "47,6694,14.98\n",
    "29,680,13.08\n",
    "91,8900,24.59\n",
    "70,3959,68.68\n",
    "85,1733,28.53\n",
    "53,9900,83.55\n",
    "14,1505,4.32\n",
    "51,3378,19.80\n",
    "42,6926,57.77\n",
    "2,4424,55.77\n",
    "79,9291,33.17\n",
    "50,3901,23.57\n",
    "20,6633,6.49\n",
    "15,6148,65.53\n",
    "44,8331,99.19\n",
    "5,3505,64.18\n",
    "48,5539,32.42\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1758ca0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Customers::\n",
      "(68,6375.45)\n",
      "(73,6206.199)\n",
      "(39,6193.1104)\n",
      "(54,6065.39)\n",
      "(71,5995.66)\n",
      "(2,5994.591)\n",
      "(97,5977.1895)\n",
      "(46,5963.111)\n",
      "(42,5696.8403)\n",
      "(59,5642.8906)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rawCustomersInfo = /user/itv002768/customerorders_practical/customerorders-201008-180523.csv MapPartitionsRDD[112] at textFile at <console>:33\n",
       "splitCust = MapPartitionsRDD[113] at map at <console>:36\n",
       "totalPurchase = MapPartitionsRDD[119] at sortBy at <console>:39\n",
       "finalInfo = Array((68,6375.45), (73,6206.199), (39,6193.1104), (54,6065.39), (71,5995.66), (2,5994.591), (97,5977.1895), (46,5963.111), (42,5696.8403), (59,5642.8906), (41,5637.619), (0,5524.9497), (8,5517.24), (85,5503.4307), (61,5497.48), (32,5496.0503), (58,5437.7305), (63,5415.15), (15,5413.5103), (6,5397.8794), (92,5379.281), (43,5368.83), (70,5368.2505), (72,5337.4395), (34,5330.7...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array((68,6375.45), (73,6206.199), (39,6193.1104), (54,6065.39), (71,5995.66), (2,5994.591), (97,5977.1895), (46,5963.111), (42,5696.8403), (59,5642.8906), (41,5637.619), (0,5524.9497), (8,5517.24), (85,5503.4307), (61,5497.48), (32,5496.0503), (58,5437.7305), (63,5415.15), (15,5413.5103), (6,5397.8794), (92,5379.281), (43,5368.83), (70,5368.2505), (72,5337.4395), (34,5330.7..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawCustomersInfo = sc.textFile(\"/user/itv002768/customerorders_practical/customerorders-201008-180523.csv\")\n",
    "// split on \",\" and take only 1st and third element of an array, and convert third element to float\n",
    "// Put these in a touple\n",
    "val splitCust = rawCustomersInfo.map(x => (x.split(\",\")(0), x.split(\",\")(2).toFloat ) )\n",
    "\n",
    "// Calculate the sum of amount and sort by amount in descending order\n",
    "val totalPurchase = splitCust.reduceByKey((x, y) => x+y).sortBy(x => x._2, false)\n",
    "val finalInfo = totalPurchase.collect\n",
    "val topTen = finalInfo.take(10)\n",
    "println(\"Top 10 Customers::\")\n",
    "for(info <- topTen){\n",
    "    println(info)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1816acd3",
   "metadata": {},
   "source": [
    "### Find Movie Ratings\n",
    "Below are the few lines from moviedata-201008-180523.data file where columns are separated by tab.\n",
    "\n",
    "column details : user_id    movie_id    rating_given    timestamp\n",
    "\n",
    "problem statement - How many times movies were rated 1 star, 2 stars ..... 5 starts\n",
    "\n",
    "```bash\n",
    "[itv002768@g02 ~]$ hadoop fs -mkdir movierating_practical\n",
    "[itv002768@g02 ~]$ hadoop fs -put moviedata-201008-180523.data movierating_practical\n",
    "[itv002768@g02 ~]$ hadoop fs -head movierating_practical/moviedata-201008-180523.data\n",
    "196     242     3       881250949\n",
    "186     302     3       891717742\n",
    "22      377     1       878887116\n",
    "244     51      2       880606923\n",
    "166     346     1       886397596\n",
    "298     474     4       884182806\n",
    "115     265     2       881171488\n",
    "253     465     5       891628467\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "154da350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawMovieRatingInfo = /user/itv002768/movierating_practical/moviedata-201008-180523.data MapPartitionsRDD[133] at textFile at <console>:31\n",
       "requiredCol = MapPartitionsRDD[134] at map at <console>:34\n",
       "final_ = ShuffledRDD[135] at reduceByKey at <console>:36\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array((4,34174), (2,11370), (5,21201), (3,27145), (1,6110))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawMovieRatingInfo = sc.textFile(\"/user/itv002768/movierating_practical/moviedata-201008-180523.data\")\n",
    "// split on \"\\t\" and take only third column\n",
    "// Put this column value in a touple along with 1 as second element\n",
    "val requiredCol = rawMovieRatingInfo.map(x => (x.split(\"\\t\")(2), 1) )\n",
    "// Sum it up\n",
    "val final_ = requiredCol.reduceByKey((x, y) => x+y)\n",
    "\n",
    "final_.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b07da5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawMovieRatingInfo = /user/itv002768/movierating_practical/moviedata-201008-180523.data MapPartitionsRDD[143] at textFile at <console>:30\n",
       "requiredCol = MapPartitionsRDD[144] at map at <console>:33\n",
       "final_ = Map(4 -> 34174, 5 -> 21201, 1 -> 6110, 2 -> 11370, 3 -> 27145)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Map(4 -> 34174, 5 -> 21201, 1 -> 6110, 2 -> 11370, 3 -> 27145)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawMovieRatingInfo = sc.textFile(\"/user/itv002768/movierating_practical/moviedata-201008-180523.data\")\n",
    "// split on \"\\t\" and take only third column\n",
    "val requiredCol = rawMovieRatingInfo.map(x => x.split(\"\\t\")(2))\n",
    "// It will result a map with key as rating and count as value\n",
    "val final_ = requiredCol.countByValue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad05b2b",
   "metadata": {},
   "source": [
    "**In the above example we used countByValue instead of map + reduceByKey</BR>\n",
    "The only difference is map + reduceByKey is a transformation and countByValue is and action</BR>\n",
    "map + reduceByKey -> rdd</BR>\n",
    "countByValue -> variable</BR>\n",
    "we don't have to use any collect when using countByValue</BR>\n",
    "Only use CountByValue if it is the final result because if you perform anything after using countByValue then operations will be performed on the local machine.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295f2eaf",
   "metadata": {},
   "source": [
    "### Find Average Linkedin Connections\n",
    "\n",
    "Below file contains the information related to linkedin users\n",
    "\n",
    "row_id, username, age, connections\n",
    "\n",
    "Problem statement - We've to find the average connection for various age groups\n",
    "\n",
    "```bash\n",
    "[itv002768@g02 ~]$ hadoop fs -mkdir week9_friendsavg_practical\n",
    "[itv002768@g02 ~]$ hadoop fs -put friendsdata-201008-180523.csv week9_friendsavg_practical\n",
    "\n",
    "[itv002768@g02 ~]$ hadoop fs -head week9_friendsavg_practical/friendsdata-201008-180523.csv\n",
    "0::Will::33::385\n",
    "1::Jean-Luc::26::2\n",
    "2::Hugh::55::221\n",
    "3::Deanna::40::465\n",
    "4::Quark::68::21\n",
    "5::Weyoun::59::318\n",
    "6::Gowron::37::220\n",
    "7::Will::54::307\n",
    "8::Jadzia::38::380\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "339a777b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawInfo = /user/itv002768/week9_friendsavg_practical/friendsdata-201008-180523.csv MapPartitionsRDD[47] at textFile at <console>:41\n",
       "requiredCol = MapPartitionsRDD[48] at map at <console>:42\n",
       "requiredInfo = MapPartitionsRDD[49] at mapValues at <console>:46\n",
       "final_ = ShuffledRDD[50] at reduceByKey at <console>:47\n",
       "final__ = MapPartitionsRDD[51] at map at <console>:48\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "parseLines: (info: String)(Int, Int)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array((34,245), (52,340), (56,306), (66,276), (22,206), (28,209), (54,278), (46,223), (48,281), (30,235), (50,254), (32,207), (36,246), (24,233), (62,220), (64,281), (..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseLines(info: String) = {\n",
    "    val splitLine = info.split(\"::\")\n",
    "    val age = splitLine(2).toInt\n",
    "    val connections = splitLine(3).toInt\n",
    "    (age, connections)\n",
    "}\n",
    "\n",
    "val rawInfo = sc.textFile(\"/user/itv002768/week9_friendsavg_practical/friendsdata-201008-180523.csv\")\n",
    "val requiredCol = rawInfo.map(parseLines)\n",
    "//val requiredInfo = requiredCol.map(x => (x._1, (x._2, 1)))\n",
    "\n",
    "// We can also use mapValues to take values only instead of using map in the above commented line\n",
    "val requiredInfo = requiredCol.mapValues(x => (x, 1))\n",
    "val final_ = requiredInfo.reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2))\n",
    "val final__ = final_.map(x => (x._1, x._2._1/x._2._2))\n",
    "\n",
    "final__.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293efc10",
   "metadata": {},
   "source": [
    "## Practicals - pyspark\n",
    "For the below practicals, I you want to run the code blocks switch the kernel to **Pyspark 3**\n",
    "\n",
    "### Find Word Count in a file pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a83802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3aaf987e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 361),\n",
       " ('big', 285),\n",
       " ('in', 171),\n",
       " ('training', 114),\n",
       " ('course', 105),\n",
       " ('hadoop', 100),\n",
       " ('online', 58),\n",
       " ('courses', 53),\n",
       " ('spark', 42),\n",
       " ('bangalore', 40)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()\n",
    "\n",
    "input_ = sc.textFile(\"/user/itv002768/week9_practical_search_words/search_data.txt\")\n",
    "all_words = input_.flatMap(lambda x: x.split(\" \"))\n",
    "small_letters = all_words.map(lambda x: x.lower())\n",
    "words_value = small_letters.map(lambda x: (x,1))\n",
    "words_frequency = words_value.reduceByKey(lambda x, y: x+y)\n",
    "final = words_frequency.collect()\n",
    "# top ten words\n",
    "sorted(final, key=lambda final: final[1], reverse = True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12d0f88",
   "metadata": {},
   "source": [
    "### Find Top Customers pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91126972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('68', 6375.450000000001),\n",
       " ('73', 6206.200000000001),\n",
       " ('39', 6193.110000000001),\n",
       " ('54', 6065.390000000001),\n",
       " ('71', 5995.660000000002),\n",
       " ('2', 5994.59),\n",
       " ('97', 5977.1900000000005),\n",
       " ('46', 5963.109999999999),\n",
       " ('42', 5696.840000000002),\n",
       " ('59', 5642.889999999999)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_customers_info = sc.textFile(\"/user/itv002768/customerorders_practical/customerorders-201008-180523.csv\")\n",
    "split_cust = raw_customers_info.map(lambda x: (x.split(\",\")[0], float(x.split(\",\")[2]) ) )\n",
    "total_purchase = split_cust.reduceByKey(lambda x, y: x+y)\n",
    "final_info = total_purchase.collect()\n",
    "sorted(final_info, key=lambda final_info: final_info[1], reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077edf03",
   "metadata": {},
   "source": [
    "### Find Movie Ratings pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e6df5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 6110), ('4', 34174), ('5', 21201), ('3', 27145), ('2', 11370)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_movie_rating_info = sc.textFile(\"/user/itv002768/movierating_practical/moviedata-201008-180523.data\")\n",
    "required_col = raw_movie_rating_info.map(lambda x: (x.split(\"\\t\")[2], 1) )\n",
    "final_ = required_col.reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "final_.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46beec82",
   "metadata": {},
   "source": [
    "### Find Average Linkedin Connections pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd69d310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(60, 202.71428571428572),\n",
       " (32, 207.9090909090909),\n",
       " (68, 269.6),\n",
       " (38, 193.53333333333333),\n",
       " (40, 250.8235294117647),\n",
       " (66, 276.44444444444446),\n",
       " (26, 242.05882352941177),\n",
       " (44, 282.1666666666667),\n",
       " (64, 281.3333333333333),\n",
       " (54, 278.0769230769231),\n",
       " (46, 223.69230769230768),\n",
       " (30, 235.8181818181818),\n",
       " (56, 306.6666666666667),\n",
       " (62, 220.76923076923077),\n",
       " (28, 209.1),\n",
       " (36, 246.6),\n",
       " (58, 116.54545454545455),\n",
       " (20, 165.0),\n",
       " (18, 343.375),\n",
       " (52, 340.6363636363636),\n",
       " (48, 281.4),\n",
       " (22, 206.42857142857142),\n",
       " (50, 254.6),\n",
       " (24, 233.8),\n",
       " (42, 303.5),\n",
       " (34, 245.5),\n",
       " (69, 235.2),\n",
       " (67, 214.625),\n",
       " (51, 302.14285714285717),\n",
       " (29, 215.91666666666666),\n",
       " (27, 228.125),\n",
       " (49, 184.66666666666666),\n",
       " (55, 295.53846153846155),\n",
       " (25, 197.45454545454547),\n",
       " (47, 233.22222222222223),\n",
       " (23, 246.3),\n",
       " (21, 350.875),\n",
       " (65, 298.2),\n",
       " (43, 230.57142857142858),\n",
       " (63, 384.0),\n",
       " (45, 309.53846153846155),\n",
       " (57, 258.8333333333333),\n",
       " (37, 249.33333333333334),\n",
       " (19, 213.27272727272728),\n",
       " (41, 268.55555555555554),\n",
       " (61, 256.22222222222223),\n",
       " (59, 220.0),\n",
       " (35, 211.625),\n",
       " (33, 325.3333333333333),\n",
       " (31, 267.25),\n",
       " (39, 169.28571428571428),\n",
       " (53, 222.85714285714286)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_lines(info):\n",
    "    split_line = info.split(\"::\")\n",
    "    age = int(split_line[2])\n",
    "    connections = int(split_line[3])\n",
    "    return (age, connections)\n",
    "\n",
    "raw_info = sc.textFile(\"/user/itv002768/week9_friendsavg_practical/friendsdata-201008-180523.csv\")\n",
    "required_col = raw_info.map(parse_lines)\n",
    "required_info = required_col.mapValues(lambda x: (x, 1))\n",
    "final_ = required_info.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "final__ = final_.map(lambda x: (x[0], x[1][0]/x[1][1]))\n",
    "\n",
    "final__.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
