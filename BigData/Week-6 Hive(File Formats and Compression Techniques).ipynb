{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf2149d",
   "metadata": {},
   "source": [
    "# Table Of Contents\n",
    "  * [File Format](#File-Format)\n",
    "    - [Row Based](#Row-Based)\n",
    "    - [Column Based](#Column-Based)\n",
    "    - [Text File Format](#Text-File-Format)\n",
    "  * [Specialized File Formats](#Specialized-File-Formats)\n",
    "    - [Avro](#Avro)\n",
    "    - [ORC](#ORC)\n",
    "    - [Parquet](#Parquet)\n",
    "  * [Comparision](#Comparision)\n",
    "  * [File Format Practicals](#File-Format-Practicals)\n",
    "  * [File Compression Techniques](#File-Compression-Techniques)\n",
    "    - [Snappy](#Snappy)\n",
    "    - [Lzo](#Lzo)\n",
    "    - [Gzip](#Gzip)\n",
    "    - [Bzip2](#Bzip2)\n",
    "  * [Few More Optimizations](#Few-More-Optimizations)\n",
    "    - [Vectorization](#Vectorization)\n",
    "    - [Changing the Hive engine](#Changing-the-Hive-engine)\n",
    "  * [Apache Thrift Server](#Apache-Thrift-Server)\n",
    "  * [MSCK repair](#MSCK-repair)\n",
    "  * [Miscellaneous](#Miscellaneous)\n",
    "  * [Slowly Changing Dimentions](#Slowly-Changing-Dimentions)\n",
    "\n",
    "#### How the data will be stored?\n",
    "\n",
    "In this tutorial we'll talk about the following two things:\n",
    "* File Format\n",
    "* Compression Techniques\n",
    "\n",
    "# File Format\n",
    "\n",
    "\n",
    "#### Why do we need different file formats? \n",
    "       OR\n",
    "#### why do we need it?\n",
    "1. We want to Save storags\n",
    "2. Fast Processing\n",
    "3. Less time for our I/O operations\n",
    "\n",
    "The type of file format we use can help us in achieveing the above 4 aspects.\n",
    "\n",
    "    There are a lot of choices available on file formats\n",
    "    * Some of them will give faster reads\n",
    "    * Some are good if we need faster writes\n",
    "    * Some are splittable(Most preferrable in case of BigData since parallel processing will be there)\n",
    "    * Schema Evaolution Support(Altering the schema after loading or at later stages)\n",
    "    * Advanced compression\n",
    "    * Compatibility with the technology\n",
    "    \n",
    "All the file formats are divided into two broad categories:\n",
    "### Row Based\n",
    "In this file format data is stored in row-wise fashion.</br>\n",
    "    1. New data is appended in the end of the line.</br>\n",
    "    2. Writing is easy since it is a sequential storage. </br>\n",
    "    3. In case of <i>SELECTS</i> If we want to get a subset of columns it has to read the entire record.</br>\n",
    "    4. Since there are different data types in a single file that's why compression won't be that great.\n",
    "    \n",
    "Example is any DBMS table and it is not a good choice for Data Warehousing.\n",
    "\n",
    "\n",
    "\n",
    "### Column Based\n",
    "In this file format Data is stored based on per column basis</br>\n",
    "    1. All the column Values are stored together.</br>\n",
    "    2. If we write a query requesting some columns then only files containing those columns will be read.</br>\n",
    "    3. Suitable for faster reads but writing is costlier than Row based.\n",
    "    4. Suitable for Data Warehousing.\n",
    "    5. Since same data type is stored in a same file tha't why it compression level will be great.\n",
    "    \n",
    "Example is \n",
    "\n",
    "\n",
    "## Text File Format\n",
    "\n",
    "<b>Note:</b> We'll not use it for our production use</br>\n",
    "Examples: json, csv, XML etc.</br>\n",
    "These are human readable\n",
    "\n",
    "* CSV File</br>\n",
    "1. Everything is stored in text form, even an integer. Let's say ther is a number 1234 and is stored as a string the above number will take 4 x 2 = 8 bytes. On the other hand if it would have been stored as an int it would have taken 4 bytes only.\n",
    "2. If we want to read the file and store it as an int then we've to convert it into an int before storing it. It'll take processing this leads to increase in time.\n",
    "3. I/O will increase.\n",
    "\n",
    "* XML/JSON</br>\n",
    "\n",
    "Whatever we talked for CSV also applied here\n",
    "1. These File are not splittable so these are not ideal choice for production use.\n",
    "\n",
    "## Specialized File Formats\n",
    "\n",
    "There are some special file formats that we'll mostly use in BigData technologies.</br>\n",
    "All the file formats are splittable and any compression can be used among them. Compression codec is stored in metadata and whenever any reader reads the data it gets it from the metadata.\n",
    "* Serializable - Convering the data in a form that ca be transferred over the network and can be stored in a file.\n",
    "* Deserialization  - Converting the serialized data to readable format.\n",
    "* Agnostic Compression - Can use any compression technique.\n",
    "\n",
    "\n",
    "#### Avro\n",
    "* Generalized file format supporting most of the programmeing languages. We can say it is language neutral.\n",
    "* It is a row based file format.\n",
    "* Supports faster writes but slower reads.\n",
    "* Schema for this file format is stored in JSON.\n",
    "* This format is self descirbing because metadata schema is kept in JSON form.\n",
    "* Actual data is stored in a compressed binary format which is quiet efficient in terms of storage.\n",
    "* Schema Evolution - There is no other file format better than AVRO in case of schema evolution. Adding/Remove/renaming the colums is very well supported.\n",
    "\n",
    "\n",
    "Avro can be best fit when storing the data in the landing zone of the data lake. When we do ETL operations this is best suited because all the columns will be read.\n",
    "\n",
    "\n",
    "\n",
    "#### ORC\n",
    "\n",
    "ORC stands for Optimized Row Columnar format\n",
    "* Column Based File Format(Writes are not efficient but reads are efficient)\n",
    "* Highly Efficient in terms of storage(Takes very less space)\n",
    "* Uses Lightweight compression (dictionary encoding/Bit Packing/Delta Encoding/Run Length encoding) along with generalized compression techniques(snappy/lzo/gzip).\n",
    "\n",
    "<b> Dictionary Encoding</b>: If there are many values for a key it will store only the distinct values and creates a mapping internally.</br>\n",
    "<b> Bit Packing</b>: It tris to store the data types in less bits.</br>\n",
    "<b> Delta Encoding</b>: If there is any increment in the data then Instead of storing the whole column it will actually store the data or the difference between the last and current row.</br>\n",
    "<b> Run length Encoding</b>: if the characters are repeated it will count the number of chars and stores the numbers. e.g - ssssstttcc -> s5t3c2</br>\n",
    "\n",
    "* ORC also provides predicate push down.\n",
    "<b> Predicate Push Down</b>: In where clause whatever conditions we mentioned these are called a predicated. It first run the predicates so that it will filter out the data that is mentioned in the where clause. It pushes the predicates at storage level\n",
    "\n",
    "* It is best fit for hive because it supports all the data types including complex data types that are there in HIVE. It is specially designed for HIVE.\n",
    "\n",
    "* Schema Evolution: it supports this but not as matured as that of Avro. Metadata in ORC is stored using protocol buffers which allows addition/removal of fields. But the support is not as good as Avro.\n",
    "* ORC file is divided into theee parts.\n",
    "  - **Header**: This only contains \"ORC\"\n",
    "  - **Body**: Actual Data is stored and it is made up of stripes(default: 250MB) and these stripes are further divide into blocks(default: 10K) of rows.\n",
    "    - Stripe also contains three parts:\n",
    "      * Set Of Indices - Max/Min and count of each column in every row group in that stripe\n",
    "      * Data broken into row groups\n",
    "      * Stripe Footer - Encoding Used to compress it\n",
    "  - **Tail** - Made up of two parts:\n",
    "    - File footer: Contains the metadata at file level, For each col it will tell min/max at file level as well as stripe level\n",
    "    - Postscript: Which compression technique we've used and other info that helps to understand the remaining file. This is never compressed.\n",
    "* **Indices**: These are at three levels:\n",
    "  1. File Level - Min/Max value for the entire file\n",
    "  2. Stripe Level - Min/Max for each Stripe\n",
    "  3. Row Level - Again inside stripe there is row level Min and Max\n",
    "  \n",
    "  ```text\n",
    "    Example:\n",
    "    There are 100000 records:\n",
    "    File Level - 1 to 100000\n",
    "    Stripe level:\n",
    "      Stripe1 - 1-50000\n",
    "        Row Level:\n",
    "        RL1: 1-10000\n",
    "        RL2: 50001 - 100000 and so on\n",
    "      Stripe1 - 500001 - 100000\n",
    "        Row Level:\n",
    "        RL1: 50001-60000\n",
    "        RL2: 60001 - 70000 and so on\n",
    "    ```\n",
    "* File Level and Stripe indices are kept in file footer\n",
    "* Row Level indices are kept in index data in stripe\n",
    "  \n",
    "<span style=\"color:red\">**Key Takeaways**</span>\n",
    "* Columnar based\n",
    "* Data is stored in very storage efficient ways because of many compression encodigs\n",
    "* Very well suited for HIVE\n",
    "* The predicates are pushed at the storage level so that we've to read less\n",
    "* Schema evolution is not great but Okay.\n",
    "\n",
    "  \n",
    "\n",
    "#### Parquet\n",
    "* Column based file format.\n",
    "* Writes are not efficient but reads are efficients.\n",
    "* Very good for handline the nested data.\n",
    "* Shares many design goals with ORC but it is more general purpose(wide compatibliity with many platforms)\n",
    "* Compression is Efficient.\n",
    "* It stores metadata at the end of the file that's why it is called self describing.\n",
    "* It can support schema Evolution to some extent - Only adding or deleting the columns in the end, Not from the middle.\n",
    "* There are three Parts:\n",
    "  * **Header** - contains a text \"PAR1\"\n",
    "  * **Row group** - Column chunks(Let's say we have 5 columns and 100K rows) -> Further divided into pages\n",
    "    * Each Column chunk contains 10k rows\n",
    "  * **Footer** - Contains three parts\n",
    "    * File Metadata\n",
    "    * Length of File Metadata\n",
    "    * Magic Number \"PAR1\"\n",
    "    \n",
    "<span style=\"color:red\">**Key Takeaways**</span>\n",
    "* Generic File Format and column based\n",
    "* Faster reads and slower writes\n",
    "* Compression is efficient but not as good as ORC\n",
    "* Supports schema evolution to some extent\n",
    "* Best suited for nested structures\n",
    "* Splittable\n",
    "\n",
    "\n",
    "\n",
    "####  Comparision\n",
    "\n",
    "<table align=left>\n",
    "<tr>\n",
    "<th>Avro</th>\n",
    "<th>Parquet</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "    \n",
    "* It is a row based file format.\n",
    "* Provides faster writes but slower reads.\n",
    "* Avro is quiet mature in schema evolution.\n",
    "* Doesn't provide the support for nested Data.\n",
    "    \n",
    "</td>\n",
    "<td>\n",
    "    \n",
    "\n",
    "* It is column based file format.\n",
    "* Provides faster reads but slower writes.\n",
    "* Parquet only supports schema evolution at the end.\n",
    "* It provides very good support for nested DS.\n",
    "    \n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<table align=left>\n",
    "<tr>\n",
    "<th>ORC</th>\n",
    "<th>Parquet</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "  \n",
    "* Cannot handle deeply nested data.\n",
    "* Good at predicate pushdown.\n",
    "* Also supports ACID properties to somw extent\n",
    "* Compression - Better than parquet.\n",
    "* Schema Evolution - Better than parquet.\n",
    "* Supported Platforms - Hive, Presto.\n",
    "  \n",
    "</td>\n",
    "<td>\n",
    "    \n",
    "* Can handle deeply nested data.\n",
    "* Compression - ORC is better.\n",
    "* Parquet only supports schema evolution at the end.\n",
    "* Supported Platforms - Impala, Arrow Drill, Spark.\n",
    "    \n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b4604e",
   "metadata": {},
   "source": [
    "## File Format Practicals\n",
    "\n",
    "\n",
    "* Create a Table orders_orc by mentioning **stored as orc**\n",
    "* Load the data using insert columns\n",
    "* Check the file\n",
    "\n",
    "```shell\n",
    "hive> CREATE TABLE `orders_orc`(\n",
    "    >   `id` bigint,\n",
    "    >   `product_id` string,\n",
    "    >   `customer_id` bigint,\n",
    "    >   `quantity` int,\n",
    "    >   `amount` double) stored as orc;\n",
    "OK\n",
    "Time taken: 0.058 seconds\n",
    "\n",
    "hive> dfs -ls warehouse/tushar_test.db/orders_orc;\n",
    "Found 1 items\n",
    "-rw-r--r--   3 itv002768 supergroup        658 2022-07-17 08:55 warehouse/tushar_test.db/orders_orc/000000_0\n",
    "\n",
    "hive> dfs -cat warehouse/tushar_test.db/orders_orc/000000_0;\n",
    "ORC\u0011\n",
    "\u0006\u0012\u0003P7\n",
    "\u0019\n",
    "\u0003\u0012\u0003\u0012\n",
    "\u0018��(P@��be\u0001!1f%\u0001.֤���\\!ւ���T    �\u00061\n",
    "\u0016\n",
    "\u0003\u0003\u0012    �\u0011\u0010�\u0011\u0018�4P+\n",
    "\u0013\n",
    "\u0003\u0012\n",
    " \u0003\u0012\u0002\u0010\u0006\u0018\n",
    "PB�R�bb`\u0010R�`���d\u0003\u0015A0\u001d",
    "��A\u0012����\u0010�\u0017n\u0002\u0003d\u0003d\u0010\u0003d\u0012!phonecamerabroom     F\u0002�P    F\u0002b $c`�\u0003�\u001c",
    "@\u0014C�͠��]�A\n",
    "�0\u0010C��\u001d",
    "K\u0005\u0011�\u0010\u0010����h\n",
    "                  �\\|\u001e",
    "��h�\u001a���pJ�%3YYq���\"\u001d",
    ";u:ig�,߮\u0018�7c��\u001e",
    "���b�``�\u0012�`\u0016���;�+0�\u0004�Ć\u001a\u0001\n",
    "                                                                                   \u0011|\\\u0012\u001c",
    "�J\u0002\\�IE���B�\u0005\u0019�y�\u0012\n",
    "@\u0019).A�zN�u�\u0002�\u0004%�L�Bl\\|@!6&\u00016    . ��K��YJ��\u0001\n",
    "                                            T\u001c",
    "\u0004�t�\u0016I0�k�\u0003P�(�\u0001�`\u0016��$���,��Q�K�M�Yɂ�G������U�)3E���(?�4�$\u001e",
    "��N.-.��M-\u0002q8\n",
    "K\u0013�J2K*��\u0012s�K�J��8X��\u001d",
    "J3\u00031�\u0001�\u0015\n",
    "                              s��\u0010G�    ^�I'x%6�`�೒�\u0012�bM*���\u0015b-���K�P�HY        \u0002�sr�\u0013\u0014X'(�e\u0002\u0014b��\u0003\n",
    "�q0     �Ip\u0001��V�\u001c",
    "�RҜ\n",
    "                    `���\u0003�8H�\u0019\\;\u001d",
    "��D\u001d",
    "&�y�\u0001\u0010\u0001\u0018��\u0010\"\u0002\n",
    "                                                  (u0��\u0003\u0003ORC\u0018hive>\n",
    "```\n",
    "\n",
    "**To get the info in readable format use the following command**\n",
    "\n",
    "```shell\n",
    "[itv002768@g02 ~]$ hive --orcfiledump /user/itv002768/warehouse/tushar_test.db/orders_orc/000000_0\n",
    "SLF4J: Class path contains multiple SLF4J bindings.\n",
    "SLF4J: Found binding in [jar:file:/opt/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: Found binding in [jar:file:/opt/hadoop-3.3.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
    "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
    "Processing data file /user/itv002768/warehouse/tushar_test.db/orders_orc/000000_0 [length: 658]\n",
    "Structure for /user/itv002768/warehouse/tushar_test.db/orders_orc/000000_0\n",
    "File Version: 0.12 with ORC_517\n",
    "Rows: 3\n",
    "Compression: ZLIB\n",
    "Compression size: 262144\n",
    "Type: struct<id:bigint,product_id:string,customer_id:bigint,quantity:int,amount:double>\n",
    "\n",
    "Stripe Statistics:\n",
    "  Stripe 1:\n",
    "    Column 0: count: 3 hasNull: false\n",
    "    Column 1: count: 3 hasNull: false bytesOnDisk: 14 min: 111111 max: 111113 sum: 333336\n",
    "    Column 2: count: 3 hasNull: false bytesOnDisk: 26 min: broom max: phone sum: 16\n",
    "    Column 3: count: 3 hasNull: false bytesOnDisk: 6 min: 1111 max: 1111 sum: 3333\n",
    "    Column 4: count: 3 hasNull: false bytesOnDisk: 7 min: 1 max: 3 sum: 5\n",
    "    Column 5: count: 3 hasNull: false bytesOnDisk: 21 min: 10.0 max: 5200.0 sum: 6410.0\n",
    "\n",
    "File Statistics:\n",
    "  Column 0: count: 3 hasNull: false\n",
    "  Column 1: count: 3 hasNull: false bytesOnDisk: 14 min: 111111 max: 111113 sum: 333336\n",
    "  Column 2: count: 3 hasNull: false bytesOnDisk: 26 min: broom max: phone sum: 16\n",
    "  Column 3: count: 3 hasNull: false bytesOnDisk: 6 min: 1111 max: 1111 sum: 3333\n",
    "  Column 4: count: 3 hasNull: false bytesOnDisk: 7 min: 1 max: 3 sum: 5\n",
    "  Column 5: count: 3 hasNull: false bytesOnDisk: 21 min: 10.0 max: 5200.0 sum: 6410.0\n",
    "\n",
    "Stripes:\n",
    "  Stripe: offset: 3 data: 74 rows: 3 tail: 70 index: 163\n",
    "    Stream: column 0 section ROW_INDEX start: 3 length 11\n",
    "    Stream: column 1 section ROW_INDEX start: 14 length 30\n",
    "    Stream: column 2 section ROW_INDEX start: 44 length 35\n",
    "    Stream: column 3 section ROW_INDEX start: 79 length 27\n",
    "    Stream: column 4 section ROW_INDEX start: 106 length 24\n",
    "    Stream: column 5 section ROW_INDEX start: 130 length 36\n",
    "    Stream: column 1 section DATA start: 166 length 14\n",
    "    Stream: column 2 section DATA start: 180 length 19\n",
    "    Stream: column 2 section LENGTH start: 199 length 7\n",
    "    Stream: column 3 section DATA start: 206 length 6\n",
    "    Stream: column 4 section DATA start: 212 length 7\n",
    "    Stream: column 5 section DATA start: 219 length 21\n",
    "    Encoding column 0: DIRECT\n",
    "    Encoding column 1: DIRECT_V2\n",
    "    Encoding column 2: DIRECT_V2\n",
    "    Encoding column 3: DIRECT_V2\n",
    "    Encoding column 4: DIRECT_V2\n",
    "    Encoding column 5: DIRECT\n",
    "\n",
    "File length: 658 bytes\n",
    "Padding length: 0 bytes\n",
    "Padding ratio: 0%\n",
    "```\n",
    "\n",
    "**To see the actual data**\n",
    "```shell\n",
    "[itv002768@g02 ~]$ hive --orcfiledump /user/itv002768/warehouse/tushar_test.db/orders_orc/000000_0 -d\n",
    "SLF4J: Class path contains multiple SLF4J bindings.\n",
    "SLF4J: Found binding in [jar:file:/opt/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: Found binding in [jar:file:/opt/hadoop-3.3.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
    "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
    "Processing data file /user/itv002768/warehouse/tushar_test.db/orders_orc/000000_0 [length: 658]\n",
    "{\"id\":111111,\"product_id\":\"phone\",\"customer_id\":1111,\"quantity\":3,\"amount\":1200}\n",
    "{\"id\":111112,\"product_id\":\"camera\",\"customer_id\":1111,\"quantity\":1,\"amount\":5200}\n",
    "{\"id\":111113,\"product_id\":\"broom\",\"customer_id\":1111,\"quantity\":1,\"amount\":10}\n",
    "```\n",
    "\n",
    "**Below are the commands for Parquet file**\n",
    "\n",
    "```shell\n",
    "\n",
    "hive> CREATE TABLE `orders_parquet`(\n",
    "    >   `id` bigint,\n",
    "    >   `product_id` string,\n",
    "    >   `customer_id` bigint,\n",
    "    >   `quantity` int,\n",
    "    >   `amount` double) stored as parquet;\n",
    "OK\n",
    "Time taken: 0.841 seconds\n",
    "hive> insert into orders_parquet select * from orders_orc;\n",
    "Query ID = itv002768_20220717090857_137e9a9f-2901-4d44-8068-fcf30d934594\n",
    "Total jobs = 3\n",
    "Launching Job 1 out of 3\n",
    "Number of reduce tasks determined at compile time: 1\n",
    "In order to change the average load for a reducer (in bytes):\n",
    "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
    "In order to limit the maximum number of reducers:\n",
    "  set hive.exec.reducers.max=<number>\n",
    "In order to set a constant number of reducers:\n",
    "  set mapreduce.job.reduces=<number>\n",
    "Starting Job = job_1650084714510_39755, Tracking URL = http://m02.itversity.com:19088/proxy/application_1650084714510_39755/\n",
    "Kill Command = /opt/hadoop/bin/mapred job  -kill job_1650084714510_39755\n",
    "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
    "2022-07-17 09:09:23,017 Stage-1 map = 0%,  reduce = 0%\n",
    "2022-07-17 09:09:28,292 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.74 sec\n",
    "2022-07-17 09:09:32,471 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.17 sec\n",
    "MapReduce Total cumulative CPU time: 4 seconds 170 msec\n",
    "Ended Job = job_1650084714510_39755\n",
    "Stage-4 is selected by condition resolver.\n",
    "Stage-3 is filtered out by condition resolver.\n",
    "Stage-5 is filtered out by condition resolver.\n",
    "Moving data to directory hdfs://m01.itversity.com:9000/user/itv002768/warehouse/tushar_test.db/orders_parquet/.hive-staging_hive_2022-07-17_09-08-57_887_3847282730333851488-1/-ext-10000\n",
    "Loading data to table tushar_test.orders_parquet\n",
    "MapReduce Jobs Launched:\n",
    "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.17 sec   HDFS Read: 20127 HDFS Write: 1499 SUCCESS\n",
    "Total MapReduce CPU Time Spent: 4 seconds 170 msec\n",
    "OK\n",
    "Time taken: 36.655 seconds\n",
    "\n",
    "\n",
    "hive> dfs -cat warehouse/tushar_test.db/orders_parquet/000000_0;\n",
    "PAR1\u0015\u0015<\u0015<,\u0015\u0006\u0015\u0015\u0006\u001c",
    "\u0018       �\u0001�\u0001\u0016(  �\u0001�\u0001\u0002\u0003��\u0001       �\u0001\u0015\u0015D\u0015D,\u0015\u0006\u0015\u0015\u0006\u001c",
    "6(\u0005phone\u0018\u0005broom\u0002\u0003\u0005phone\u0006camera\u0005broom\u0015\u0004\u0015\u0010\u0015\u0010L\u0015\u0002\u0015\u0004W\u0004\u0015\u0015\u0010\u0015\u0010,\u0015\u0006\u0015\u0004\u0015\u0006\u001c",
    "W\u0004W\u0004\u0016W\u0004W\u0004\u0002\u0003\u0003\u0015\u0004\u0015\u0010\u0015\u0010L\u0015\u0004\u0015\u0004\u0003\u0001\u0015\u0015\u0012\u0015\u0012,\u0015\u0006\u0015\u0004\u0015\u0006\u001c",
    "\u0018\u0004\u0003\u0018\u0004\u0001\u0016(\u0004\u0003\u0018\u0004\u0001\u0002\u0003\u0001\u0003\u0006\u0015\u0015<\u0015<,\u0015\u0006\u0015\u0015\u0006\u001c",
    "P�@$@\u0016P�@$@\u0002\u0003��@P�@$@\u0015\u0002\u0019lH\n",
    "                                                            hive_schema\u0015\n",
    "\u0015\u0004%\u0002\u0018\u0002id\u0015\n",
    "         %\u0002\u0018\n",
    "product_id%\u0015\u0004%\u0002\u0018\n",
    "                customer_id\u0015\u0002%\u0002quantity\u0015\n",
    "%\u0002\u0018\u0006amount\u0016\u0006\u0019\u001c",
    "\u0019\\\u001c",
    "\u0015\u0004\u0019\u0006\u0019\u0018\u0002id\u0015\u0016\u0006\u0016�\u0001\u0016�\u0001<\u0018   �\u0001�\u0001\u0016(  �\u0001�\u0001\u0019\u001c",
    "\u0015\u0015\u0015\u0002&�\u0001\u001c",
    "\u0015\n",
    "                                                               \u0019\u0006\u0019\u0018\n",
    "product_id\u0015\u0016\u0006\u0016�\u0001\u0016�\u0001&�\u0001<6(\u0005phone\u0018\u0005broom\u0019\u001c",
    "\u0015\u0015\u0015\u0002&�\u0002\u001c",
    "\u0015\u0004\u0019\u0004\u0006\u0019\u0018\n",
    "                                                       customer_id\u0015\u0016\u0006\u0016�\u0001\u0016�\u0001&�\u0002<W\u0004W\u0004\u0016W\u0004W\u0004\u0019,\u0015\u0004\u0015\u0004\u0015\u0002\u0015\u0015\u0004\u0015\u0002&�\u0003\u001c",
    "\u0015\u0002\u0019\u0004\u0006\u0019quantity\u0015\u0016\u0006\u0016�\u0001\u0016�\u0001&�\u0003<\u0018\u0004\u0003\u0018\u0004\u0001\u0016(\u0004\u0003\u0018\u0004\u0001\u0019,\u0015\u0004\u0015\u0004\u0015\u0002\u0015\u0015\u0004\u0015\u0002&�\u0005\u001c",
    "\u0015\n",
    "\u0019\u0006\u0019\u0018\u0006amount\u0015\u0016\u0006\u0016�\u0001\u0016�\u0001&�\u0005<P�@$@\u0016P�@$@\u0019\u001c",
    "\u0015\u0015\u0015\u0002\u0016�\u0006\u0016\u0006\u0019\u001c",
    "\u0018\u0010writer.time.zone\u0018America/Toronto\u0018Jparquet-mr version 1.10.0 (build 031a6654009e3b82020012a18434c582bd74c73a)\u0019\\\u001c",
    "\u001c",
    "\u001c",
    "\u001c",
    "\u001c",
    "�\u0002PAR1\n",
    "\n",
    "\n",
    "[itv002768@g02 ~]$ parquet-tools meta /user/itv002768/warehouse/tushar_test.db/orders_parquet/000000_0 -d\n",
    "Invalid arguments: unknown extra argument \"-d\"\n",
    "\n",
    "usage: parquet-meta [option...] <input>\n",
    "where option is one of:\n",
    "       --debug     Enable debug output\n",
    "    -h,--help      Show this help string\n",
    "       --no-color  Disable color output even if supported\n",
    "where <input> is the parquet file to print to stdout\n",
    "[itv002768@g02 ~]$ parquet-tools meta /user/itv002768/warehouse/tushar_test.db/orders_parquet/000000_0\n",
    "file:        hdfs://m01.itversity.com:9000/user/itv002768/warehouse/tushar_test.db/orders_parquet/000000_0\n",
    "creator:     parquet-mr version 1.10.0 (build 031a6654009e3b82020012a18434c582bd74c73a)\n",
    "extra:       writer.time.zone = America/Toronto\n",
    "\n",
    "file schema: hive_schema\n",
    "--------------------------------------------------------------------------------\n",
    "id:          OPTIONAL INT64 R:0 D:1\n",
    "product_id:  OPTIONAL BINARY O:UTF8 R:0 D:1\n",
    "customer_id: OPTIONAL INT64 R:0 D:1\n",
    "quantity:    OPTIONAL INT32 R:0 D:1\n",
    "amount:      OPTIONAL DOUBLE R:0 D:1\n",
    "\n",
    "row group 1: RC:3 TS:416 OFFSET:4\n",
    "--------------------------------------------------------------------------------\n",
    "id:           INT64 UNCOMPRESSED DO:0 FPO:4 SZ:91/91/1.00 VC:3 ENC:PLAIN,BIT_PACKED,RLE\n",
    "product_id:   BINARY UNCOMPRESSED DO:0 FPO:95 SZ:69/69/1.00 VC:3 ENC:PLAIN,BIT_PACKED,RLE\n",
    "customer_id:  INT64 UNCOMPRESSED DO:0 FPO:164 SZ:90/90/1.00 VC:3 ENC:PLAIN_DICTIONARY,BIT_PACKED,RLE\n",
    "quantity:     INT32 UNCOMPRESSED DO:0 FPO:254 SZ:75/75/1.00 VC:3 ENC:PLAIN_DICTIONARY,BIT_PACKED,RLE\n",
    "amount:       DOUBLE UNCOMPRESSED DO:0 FPO:329 SZ:91/91/1.00 VC:3 ENC:PLAIN,BIT_PACKED,RLE\n",
    "\n",
    "[itv002768@g02 ~]$ parquet-tools cat /user/itv002768/warehouse/tushar_test.db/orders_parquet/000000_0\n",
    "id = 111111\n",
    "product_id = phone\n",
    "customer_id = 1111\n",
    "quantity = 3\n",
    "amount = 1200.0\n",
    "\n",
    "id = 111112\n",
    "product_id = camera\n",
    "customer_id = 1111\n",
    "quantity = 1\n",
    "amount = 5200.0\n",
    "\n",
    "id = 111113\n",
    "product_id = broom\n",
    "customer_id = 1111\n",
    "quantity = 1\n",
    "amount = 10.0\n",
    "  \n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a8b1bd",
   "metadata": {},
   "source": [
    "* If we have to use JSON format then we have to use json serde. As there is no inbuilt support for json. For this we've to load the JAR\n",
    "* serde is a combination of two things, **ser**ialization and **de**serialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52306aa7",
   "metadata": {},
   "source": [
    "## File Compression Techniques\n",
    "\n",
    "* Compression will help you to save storage.\n",
    "* It helps us to process the data faster.\n",
    "* It will reduce the I/O cost.\n",
    "\n",
    "Compression and uncompression comes with a cost w.r.t Time taken to compress or uncompress and when we compare it with I/O gain we can actually neglect this additional time. Following are the four important compression techniques. However, some of the codecs are optimized for storage and some are for speed.\n",
    "\n",
    "There is a trade off, if we want file to be more compressed it will take more time to compress.\n",
    "\n",
    "### Snappy\n",
    "* Developed by google.\n",
    "* Snappy is a very fast compression codec.\n",
    "* However, In terms of compression it is not very good as it will give you ideal size after compression.\n",
    "* Suitable for quick compression.\n",
    "* In most projects this codec is used.\n",
    "* By default, it's not splittable. But if we use it with splittable file formats then no worries because file formats will take care of the things.\n",
    "* can be used with avro, orc and parquet - container based file formats and splittable.\n",
    "\n",
    "\n",
    "### Lzo\n",
    "* Optimized for speed just like snappy.\n",
    "* Lzo is inherently splittable so that we can use it with file formats which are not splittable.\n",
    "* Requires additional indexing step.\n",
    "* It requires separate installation for hadoop.\n",
    "\n",
    "### Gzip\n",
    "* Optimized for storage rather than speed(avg. 2.5 times than snappy).\n",
    "* Processing speed is slow.\n",
    "* It should be used with container based file formats Since it is not splittable.\n",
    "* If we are using the gzip we can reduce the block size so that we'll have more number of blocks and can achieve parallelism.\n",
    "\n",
    "### Bzip2\n",
    "* Optimized for storage and is very very slow.\n",
    "* Purpose is to compress the files to a great extent.\n",
    "* This is splittable and can be used with text, json and XML.\n",
    "* Compresses around 9% better than Gzip. However, this comes with some cost making it 10 times slower than Gzip.\n",
    "* Not an ideal choice for hadoop untill or unless our primary concern is to reduce the storage.\n",
    "* Used for Active archival purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eba9a3",
   "metadata": {},
   "source": [
    "## Few More Optimizations\n",
    "\n",
    "### Vectorization\n",
    "Whenever we run a query it will try to get the data row by row this takes time. To avoid this we can fetch the data in batches\n",
    "\n",
    "Vectorized query execution in Hive is a feature that grately reduces the CPU usage for typical query operations.\n",
    "In case of vectorized queries we process the 1024 rows at a time and it is configurable.\n",
    "\n",
    "* You must store your data in ORC format to use query Vectorization.\n",
    "* set hive.vectorized.execution.enabled = true (This is false by default).\n",
    "\n",
    "\n",
    "```shell\n",
    "hive> set hive.vectorized.execution.enabled=true;\n",
    "hive> create table vectorizedtable(state string, id int) stored as orc;\n",
    "OK\n",
    "Time taken: 0.804 seconds\n",
    "hive> insert into vectorizedtable values('Karnataka', 1);\n",
    "Query ID = itv002768_20220717133618_db8414a2-e0f0-4b4d-bb8b-24ef80bfac8a\n",
    "Total jobs = 1\n",
    "Launching Job 1 out of 1\n",
    "Number of reduce tasks determined at compile time: 1\n",
    "In order to change the average load for a reducer (in bytes):\n",
    "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
    "In order to limit the maximum number of reducers:\n",
    "  set hive.exec.reducers.max=<number>\n",
    "In order to set a constant number of reducers:\n",
    "  set mapreduce.job.reduces=<number>\n",
    "Starting Job = job_1650084714510_39814, Tracking URL = http://m02.itversity.com:19088/proxy/application_1650084714510_39814/\n",
    "Kill Command = /opt/hadoop/bin/mapred job  -kill job_1650084714510_39814\n",
    "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
    "2022-07-17 13:36:50,949 Stage-1 map = 0%,  reduce = 0%\n",
    "2022-07-17 13:36:55,172 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.33 sec\n",
    "2022-07-17 13:37:00,504 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.81 sec\n",
    "MapReduce Total cumulative CPU time: 3 seconds 810 msec\n",
    "Ended Job = job_1650084714510_39814\n",
    "Stage-4 is selected by condition resolver.\n",
    "Stage-3 is filtered out by condition resolver.\n",
    "Stage-5 is filtered out by condition resolver.\n",
    "Moving data to directory hdfs://m01.itversity.com:9000/user/itv002768/warehouse/tushar_test.db/vectorizedtable/.hive-staging_hive_2022-07-17_13-36-18_717_326784811544799765-1/-ext-10000\n",
    "Loading data to table tushar_test.vectorizedtable\n",
    "MapReduce Jobs Launched:\n",
    "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.81 sec   HDFS Read: 16096 HDFS Write: 567 SUCCESS\n",
    "Total MapReduce CPU Time Spent: 3 seconds 810 msec\n",
    "OK\n",
    "Time taken: 46.676 seconds\n",
    "hive> explain select count(*) from vectorizedtable;\n",
    "OK\n",
    "STAGE DEPENDENCIES:\n",
    "  Stage-0 is a root stage\n",
    "\n",
    "STAGE PLANS:\n",
    "  Stage: Stage-0\n",
    "    Fetch Operator\n",
    "      limit: 1\n",
    "      Processor Tree:\n",
    "        ListSink\n",
    "\n",
    "Time taken: 6.96 seconds, Fetched: 10 row(s)\n",
    "```\n",
    "\n",
    "### Changing the Hive engine\n",
    "* Hive supports three execution Engines\n",
    " - Map Reduce(Default)\n",
    " - tez\n",
    " - Spark\n",
    " \n",
    " ```shell\n",
    "hive> set hive.execution.engine;\n",
    "hive.execution.engine=mr\n",
    "hive> set hive.execution.engine=spark;\n",
    "hive> set hive.execution.engine;\n",
    "hive.execution.engine=spark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c27a7",
   "metadata": {},
   "source": [
    "## Apache Thrift Server\n",
    "\n",
    "It is also known as Hive server.\n",
    "If you want to connect to hive remotely you can connect to hive using various client programs.\n",
    "If we want to connect to hive in a script written in java/python then it can be done with the help of thrift service.\n",
    "\n",
    "HiveServer is a service that allows remote client to submit requests to Hive using any programming language. Thrift interface acts as a bridge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3507e13",
   "metadata": {},
   "source": [
    "## MSCK repair\n",
    "\n",
    "Let's say you've created an external table that is pointed to a dir and someone has added the partitions manually by creating dir e.g `/data/State=NY`.\n",
    "If the data is populated in this table it will go in respective dir as per partition.\n",
    "\n",
    "If we run the command `show partitions table_name`. It won't show the partitions.\n",
    "`msck repair table_name` command will add the corresponding metadata for hive tables so that partition info is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b3b903",
   "metadata": {},
   "source": [
    "## Miscellaneous\n",
    "\n",
    "**No Drop table**</BR> If we enable this feature for a table no one will be able to drop the table.</br>\n",
    "\n",
    "`ALTER TABLE orders enable no_drop`\n",
    "\n",
    "To disable the no_drop use the followg command:\n",
    "\n",
    "`ALTER TABLE orders disable no_drop`\n",
    "\n",
    "If you want to enable it for a partition use the following command:\n",
    "\n",
    "`ALTER TABLE orders partition(department='HR') enable no_drop`\n",
    "\n",
    "**Offline Feature**</BR> If you enable this feature on a table then you won't be able to query that table.\n",
    "\n",
    "`ALTER TABLE orders enable offline` <=> `ALTER TABLE orders disable offline`\n",
    "\n",
    "**Skipping Header**</BR> If we are getting some random data in a file then we don't want to consider initial n rows.\n",
    "\n",
    "```shell\n",
    "hive> create table skip_test(name string, score int)\n",
    "    > row format delimited fields terminated by ','\n",
    "    > lines terminated by '\\n'\n",
    "    > stored as textfile\n",
    "    > tblproperties(\"skip.header.line.count=3\");\n",
    "OK\n",
    "Time taken: 0.814 seconds\n",
    "```\n",
    "**Making Tables Immutable**</BR> This property will allow data load only for the first time. You won't be able to append or update the data. However, you can overwrite the data.\n",
    "\n",
    "`tblproperties(\"immutable\"=\"true\")`\n",
    "\n",
    "**DROP vs TRUNCATE vs PURGE**</BR>\n",
    "* When we `DROP` a managed table both data and metedata will be deleted.\n",
    "* When we `DROP` an External table only metadata is deleted not the data.\n",
    "* When we `TRUNCATE` all the data will be deleted only metadata will be there.\n",
    "* If `PURGE` is set to true and if we delete the data it will be permanently deleted and cannot be recovered. But if `PURGE` is set to false we can recover the data.\n",
    "\n",
    "**Treating Empty strings as NULL**</BR> If a file has no value for a particular field or you can say it is blank So it will be treated as blank. By setting the following property we can give NULL or any other value for blanks.\n",
    "\n",
    "`tblproperties(\"serialization.null.format\"=\"null\")`\n",
    "\n",
    "**setting hivevar**</BR> We can set the value for a hive variable and can use it in a query.\n",
    "```shell\n",
    "hive> describe orders;\n",
    "OK\n",
    "id                      string\n",
    "customer_id             string\n",
    "product_id              string\n",
    "quantity                int\n",
    "amount                  double\n",
    "zipcode                 char(5)\n",
    "state                   char(2)\n",
    "\n",
    "# Partition Information\n",
    "# col_name              data_type               comment\n",
    "state                   char(2)\n",
    "Time taken: 0.181 seconds, Fetched: 11 row(s)\n",
    "hive> select * from orders ;\n",
    "OK\n",
    "o1       c1      p1     NULL    1.11     9011   CA\n",
    "o2       c2      p2     NULL    2.22     9022   CA\n",
    "o3       c3      p3     NULL    3.33     9033   CA\n",
    "o4       c4      p4     NULL    4.44     9044   CA\n",
    "o10      c10     p10    NULL    10.11    9001   CT\n",
    "o20      c20     p20    NULL    20.22    9002   CT\n",
    "o30      c30     p30    NULL    30.33    9003   CT\n",
    "o40      c40     p40    NULL    40.44    9004   CT\n",
    "o100     c100    p10    NULL    10.11    9001   NY\n",
    "o200     c200    p20    NULL    20.22    9002   NY\n",
    "o300     c300    p30    NULL    30.33    9003   NY\n",
    "o400     c400    p40    NULL    40.44    9004   NY\n",
    "Time taken: 6.393 seconds, Fetched: 12 row(s)\n",
    "\n",
    "hive> set hivevar:zip=9011;\n",
    "hive> select * from orders where zipcode=${zip};\n",
    "OK\n",
    "o1       c1      p1     NULL    1.11     9011   CA\n",
    "Time taken: 7.471 seconds, Fetched: 1 row(s)\n",
    "```\n",
    "\n",
    "\n",
    "**Print table headers**\n",
    "```shell\n",
    "hive> set hive.cli.print.header;\n",
    "hive.cli.print.header=false\n",
    "hive> set hive.cli.print.header=true;\n",
    "hive> select * from orders;\n",
    "OK\n",
    "orders.id       orders.customer_id      orders.product_id       orders.quantity orders.amount   orders.zipcode  orders.state\n",
    "o1       c1      p1     NULL    1.11     9011   CA\n",
    "o2       c2      p2     NULL    2.22     9022   CA\n",
    "o3       c3      p3     NULL    3.33     9033   CA\n",
    "o4       c4      p4     NULL    4.44     9044   CA\n",
    "o10      c10     p10    NULL    10.11    9001   CT\n",
    "o20      c20     p20    NULL    20.22    9002   CT\n",
    "o30      c30     p30    NULL    30.33    9003   CT\n",
    "o40      c40     p40    NULL    40.44    9004   CT\n",
    "o100     c100    p10    NULL    10.11    9001   NY\n",
    "o200     c200    p20    NULL    20.22    9002   NY\n",
    "o300     c300    p30    NULL    30.33    9003   NY\n",
    "o400     c400    p40    NULL    40.44    9004   NY\n",
    "Time taken: 9.859 seconds, Fetched: 12 row(s)\n",
    "```\n",
    "\n",
    "**Cartesion Product**</BR>\n",
    "`SELECT * FROM table1, table2`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c9d3d",
   "metadata": {},
   "source": [
    "## Slowly Changing Dimentions\n",
    "A.K.A Change Data Capture.\n",
    "\n",
    "Consider you have a table in mysql with too many columns(Dimention table).This data might not change frequently. Let's say we scooped this data to HDFS in the form of file and we created a HIVE table on top of this data. **What If data in mysql changes(not too frequently)? How to make sure that the updated data is synced with the HIVE?** Here comes the concept of SCD.\n",
    "\n",
    "### Types of SCDs:\n",
    "\n",
    "#### SCD Type 1:\n",
    "We've to sync the hive table to make sure that the data is latest. We don't want to maintain the history of the previous data.\n",
    "* Overwrite the old data with the new data.\n",
    "* Extremly simple and easy to synchronize the reporting system(OLAP) with operational system(OLTP).\n",
    "* You lose history everytime you update.\n",
    "\n",
    "#### SCD Type 2:\n",
    "We've to sync the hive table to make sure that the data is latest. Here, We have to maintain the history.\n",
    "* Add new rows with versions.\n",
    "* Allows you to track hitory.\n",
    "* Dimentions tables may become very large.\n",
    "* Additional reporting views needs to be created.\n",
    "\n",
    "There are three ways in which we can implement SCD type-2:\n",
    "* Versoning - Always the latest or gratest number represents the latest or updated value.\n",
    "* Flagging - A flag column is created(Active/Inactive). Only Active one will be considered as the latest. But we cannot say what was the previous value.\n",
    "* Effective Date Strategy - start_date and end_date columns for the validity. NULL indicates the current. It is most widely used approach.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
