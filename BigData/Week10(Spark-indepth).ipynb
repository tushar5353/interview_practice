{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "768fbb51",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "* [Shared Variables](#Shared-Variables)\n",
    "* [google ad-campaign analysis](#google-ad-campaign-analysis)\n",
    "* [Removing boring words](#Removing-boring-words)\n",
    "* [Spark Accumulator Practical](#Spark-Accumulator-Practical)\n",
    "* [YARN](#YARN(Yet-Another-Resource-Negotiator))\n",
    "* [Spark On YARN Architecture](#Spark-On-YARN-Architecture)\n",
    "* [Find the number of Warinings and errors in a log **practical**](#Find-the-number-of-Warinings-and-errors-in-a-log)\n",
    "* [Narrow and Wide tranformation](#Narrow-and-Wide-tranformation)\n",
    "* [Stages in Spark](#Stages-in-Spark)\n",
    "* [reduceByKey vs reduce](#reduceByKey-vs-reduce)\n",
    "* [groupByKey vs reduceByKey](#groupByKey-vs-reduceByKey)\n",
    "* [Pair RDD](#Pair-RDD)\n",
    "* [What is the difference between repartition and coalesce?](#What-is-the-difference-between-repartition-and-coalesce?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e6f889",
   "metadata": {},
   "source": [
    "### Shared Variables\n",
    "\n",
    "There are two kinds of shared variable:\n",
    "* Broadcast Variable\n",
    "* Accumulator\n",
    "\n",
    "We'll see what are they and how they work with the help of practicals.\n",
    "\n",
    "### google ad-campaign analysis\n",
    "\n",
    "We're interested the search words that is there in \"search tearm\" column and \"total cost\" column to find the total cost spend on that word.\n",
    "\n",
    "```bash\n",
    "[itv002768@g02 ~]$ hadoop fs -mkdir week10_practical_search_words\n",
    "[itv002768@g02 ~]$ hadoop fs -put bigdatacampaigndata-201014-183159.csv  week10_practical_search_words\n",
    "[itv002768@g02 ~]$ hadoop fs -head week10_practical_search_words/bigdatacampaigndata-201014-183159.csv\n",
    "big data contents,Broad match,None,TrendyTech Search India,Broad Match #3,1,1,100%,INR,24.06,24.06,0,0,0%,Search\n",
    "spark training with lab access,Broad match,None,TrendyTech Search India,Broad Match #3,1,2,200%,INR,29.97,59.94,0,0,0%,Search\n",
    "online hadoop training institutes in hyderabad,Broad match,None,TrendyTech Search India,Broad Match #3,1,1,100%,INR,28.45,28.45,0,0,0%,Search\n",
    "coursera data analytics,Broad match,None,TrendyTech Search India,Broad Match #3,1,1,100%,INR,24.64,24.64,0,0,0%,Search\n",
    "ameerpet big data training cost,Broad match,None,TrendyTech Search India,Broad Match #3,2,1,50%,INR,34.86,34.86,0,0,0%,Search\n",
    "good comment on big data trainer,Broad match,None,TrendyTech Search India,Broad Match #3,1,2,200%,INR,30.47,60.94,0,0,0%,Search\n",
    "spark classes,Broad match,None,TrendyTech Search India,Broad Match #3,3,1,33.33%,INR,29.21,29.21,0,0,0%,Search\n",
    "data analytics course near me,Broad match,None,TrendyTech Search India,Broad Match #3,0,1,--,INR,25.42,25.42,0,0,0%,Search\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d312dc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(data,16394.64)\n",
      "(big,12889.278)\n",
      "(in,5774.84)\n",
      "(hadoop,4818.34)\n",
      "(course,4191.5903)\n",
      "(training,4099.3696)\n",
      "(online,3484.4202)\n",
      "(courses,2565.78)\n",
      "(intellipaat,2081.22)\n",
      "(analytics,1458.51)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inputFile = /user/itv002768/week10_practical_search_words/bigdatacampaigndata-201014-183159.csv MapPartitionsRDD[58] at textFile at <console>:34\n",
       "requiredInfo = MapPartitionsRDD[59] at map at <console>:35\n",
       "allWords = MapPartitionsRDD[61] at map at <console>:36\n",
       "finalInfo = MapPartitionsRDD[67] at sortBy at <console>:37\n",
       "finalAnswer = Array((data,16394.64), (big,12889.278), (in,5774.84), (hadoop,4818.34), (course,4191.5903), (training,4099.3696), (online,3484.4202), (courses,2565.78), (intellipaat,2081.22), (analytics,1458.51), (tutorial,1383.3701), (hyderabad,1118.16), (spark,1078.72), (best,1047.7), (banga...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array((data,16394.64), (big,12889.278), (in,5774.84), (hadoop,4818.34), (course,4191.5903), (training,4099.3696), (online,3484.4202), (courses,2565.78), (intellipaat,2081.22), (analytics,1458.51), (tutorial,1383.3701), (hyderabad,1118.16), (spark,1078.72), (best,1047.7), (banga..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputFile = sc.textFile(\"/user/itv002768/week10_practical_search_words/bigdatacampaigndata-201014-183159.csv\")\n",
    "val requiredInfo = inputFile.map(x=> (x.split(\",\")(10).toFloat, x.split(\",\")(0)))\n",
    "val allWords = requiredInfo.flatMapValues(x => x.split(\" \")).map(x => (x._2.toLowerCase(), x._1))\n",
    "val finalInfo = allWords.reduceByKey((x, y) => x + y).sortBy(x => x._2, false)\n",
    "val finalAnswer = finalInfo.collect()\n",
    "finalAnswer.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef559f0",
   "metadata": {},
   "source": [
    "### Removing boring words\n",
    "\n",
    "In above practical we've to remove the boring words having no significance like 'of', 'in' etc.\n",
    "\n",
    "For this we've created a new file and put all the boring words in it.\n",
    "\n",
    "```bash\n",
    "[itv002768@g02 ~]$ head boringwords-201014-183159.txt\n",
    "shouldnt\n",
    "worrying\n",
    "simplify\n",
    "tidy\n",
    "shouldnt\n",
    "yep\n",
    "the\n",
    "lively\n",
    "borrow\n",
    "whichever\n",
    "```\n",
    "\n",
    "For this we'll use **broadcast join** in spark. This is same as that of **map side join** in hive. We can achive this using broadcast variable.\n",
    "\n",
    "In spark, we have data node and worker node(master). From driver we'll broadcast a variable which will be broadcasted on all the nodes. Complete copy will be broadcasted.\n",
    "\n",
    "Now, In our practical we'll broadcast all the boring words as broadcast variable. In this way boring data words will be there on all the machines and campaign data will be distributed across all the machines.\n",
    "\n",
    "Since we do not want duplicates so we'll put all the words in a set.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71b51564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(hadoop,4818.34)\n",
      "(intellipaat,2081.22)\n",
      "(analytics,1458.51)\n",
      "(hyderabad,1118.16)\n",
      "(spark,1078.72)\n",
      "(bangalore,1039.27)\n",
      "(cloudxlab,707.52)\n",
      "(bigdata,694.48)\n",
      "(dataflair,643.9)\n",
      "(chennai,604.04)\n",
      "(edureka,351.44)\n",
      "(iit,308.73)\n",
      "(coursera,293.25)\n",
      "(pune,284.71)\n",
      "(curso,277.53)\n",
      "(cloudera,258.06)\n",
      "(simplilearn,252.45001)\n",
      "(scala,250.73)\n",
      "(ameerpet,184.94)\n",
      "(flair,154.13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nameSet = Broadcast(0)\n",
       "inputFile = /user/itv002768/week10_practical_search_words/bigdatacampaigndata-201014-183159.csv MapPartitionsRDD[1] at textFile at <console>:48\n",
       "requiredInfo = MapPartitionsRDD[2] at map at <console>:49\n",
       "allWords = MapPartitionsRDD[4] at map at <console>:50\n",
       "filteredWords = MapPartitionsRDD[5] at filter at <console>:57\n",
       "finalInfo = MapPartitionsRDD[11] at sortBy at <console>:58\n",
       "finalAnswer = Array((hadoop,4818.34), (intellipaat,2081.22), (...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "loadBoringWords: ()Set[String]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array((hadoop,4818.34), (intellipaat,2081.22), (..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source\n",
    "\n",
    "def loadBoringWords():Set[String] = {\n",
    "    // Read the file and load words in a variable\n",
    "    var boringWords: Set[String] = Set()\n",
    "    val lines = Source.fromFile(\"/home/itv002768/boringwords-201014-183159.txt\").getLines()\n",
    "    for (line <- lines){\n",
    "        boringWords += line \n",
    "    }\n",
    "    boringWords\n",
    "}\n",
    "\n",
    "var nameSet = sc.broadcast(loadBoringWords)\n",
    "val inputFile = sc.textFile(\"/user/itv002768/week10_practical_search_words/bigdatacampaigndata-201014-183159.csv\")\n",
    "val requiredInfo = inputFile.map(x=> (x.split(\",\")(10).toFloat, x.split(\",\")(0)))\n",
    "val allWords = requiredInfo.flatMapValues(x => x.split(\" \")).map(x => (x._2.toLowerCase(), x._1))\n",
    "/*\n",
    " Check if the word is present in the nameSet\n",
    " If it is there then it'll return true else false\n",
    " Since, we've to ignore the words that are there in the nameSet\n",
    " We've to use this function with a negation(!)\n",
    " */\n",
    "val filteredWords = allWords.filter(x => !nameSet.value(x._1))\n",
    "val finalInfo = filteredWords.reduceByKey((x, y) => x + y).sortBy(x => x._2, false)\n",
    "val finalAnswer = finalInfo.collect()\n",
    "finalAnswer.take(20).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df9c4db",
   "metadata": {},
   "source": [
    "### Spark Accumulator Practical\n",
    "There will be a driver and multiple executer. Let's say you've a file of 500MB and based on the default size it is divided in four parts based on the default block size and there are four partitons residing on four machines. You want to find the number of blank lines in a file.\n",
    "\n",
    "In that case you can create an accumulator variable and keep on incrementing this variable whenever you find a blank line. This is very similar to counter in mapReduce. Here executor won't have the copy of this variable it can only update the value. Executor cannot read the value they can only update.\n",
    "\n",
    "In the below practical we've to calculate the number of blank lines in a file.\n",
    "\n",
    "```bash\n",
    "[itv002768@g02 ~]$ vim somefile.txt\n",
    "[itv002768@g02 ~]$ hadoop fs -mkdir accumulator_practical\n",
    "[itv002768@g02 ~]$ hadoop fs -put somefile.txt accumulator_practical\n",
    "[itv002768@g02 ~]$ hadoop fs -cat  accumulator_practical/somefile.txt\n",
    "This is a line.\n",
    "\n",
    "yes it is.\n",
    "\n",
    "this one too.\n",
    "\n",
    "\n",
    "yes.\n",
    "\n",
    "No.\n",
    "\n",
    "why?\n",
    "\n",
    "why not?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07df2011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputFile = /user/itv002768/accumulator_practical/somefile.txt MapPartitionsRDD[7] at textFile at <console>:33\n",
       "myAccumulator = LongAccumulator(id: 677, name: Some(blank lines Accumulator), value: 7)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputFile = sc.textFile(\"/user/itv002768/accumulator_practical/somefile.txt\")\n",
    "// Initaialized a long type accumulator which is named as \"blank lines Accumulator\" and assigned to a variable myAccumulator\n",
    "val myAccumulator = sc.longAccumulator(\"blank lines Accumulator\")\n",
    "inputFile.foreach(x => if (x==\"\") myAccumulator.add(1))\n",
    "// Below line will tell you the number of empty lines\n",
    "myAccumulator.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec0e60",
   "metadata": {},
   "source": [
    "## YARN(Yet Another Resource Negotiator)\n",
    "\n",
    "Before talking about YARN, Let us recap a bit.\n",
    "* Storage Perspective\n",
    " - HDFS\n",
    "   - Name Node(Master) which holds the metadata in the form of tables.\n",
    "   - Data Node(Slave)  which holds the actual data in terms of blocks.\n",
    "* Processing Perspective(MapReduce - Hadoop 1.0) - Job execution was controlled by two processes.\n",
    "  - Master(Job Tracker) - It runs on a master node. Used to do a lot of work\n",
    "    - Scheduling - what algorithm to use and what job to prirotize accordingly.\n",
    "    - Monitoring - Tracking the progress of a job, If a task fails rerun the task, If the task is slow then based on speculation, execution starts on another machine.\n",
    "  - Slave(Task Tracker) - Runs on many slave nodes or data nodes.\n",
    "    - Tracks the tasks on each data node and informs the job tracker about it.\n",
    "\n",
    "If there are so many data nodes then it'll be very difficult manage all the things because Task tracker only has to send the information but all the processing and then decision making will be done by Job tracker.\n",
    "\n",
    "**Drawbackes of MapReduce 1.0**\n",
    "* **Scalability** - It was obeserved, when the cluster size goes beyond 4k data nodes then the Job tracker becomes a bottleneck.\n",
    "* **Resource Utilization** - There used to be a fixed number of map and reduce slots. e.g 150 slots, 100map slots and 50reduce slots If you want to execute a MapReduce jobs which required 150, you cannot do that, you can only run 100 mappers at a time and 50 mappers will run later. Here, 50 slots are unused as there is no reduce jobs initially. In this way cluster utilization is not good.\n",
    "* Only MapReduce jobs are supported.\n",
    "\n",
    "To overcome these Drawbackes **YARN** was introduced.\n",
    "\n",
    "Three Components of YARN:\n",
    "* Resource Manager(Master)\n",
    "  - Monitoring Aspect was taken away from Job Tracker. After doing that they gave it a new name as Resource Manager that is only responsible for scheduling.\n",
    "\n",
    "\n",
    "* Node Manager(Slave)\n",
    "  - This is almost the same as that of Task Tracker.\n",
    "  - Manages the resources of the containers in respective nodes.\n",
    "  \n",
    "* Application master\n",
    "  - When a request comes from client, Resource manager creates a container on one of the Node Managers. Inside this container the resource manager launches an application master. This application master takes care of end-to-end monitoring for this application. In this way resource manager has deligated its work to Application master.\n",
    "  - Application master negotiate the resources from resource manager or you can say request for the resources(in the form of containers) required from resource manager. Resource manager allocates the resources and will send the container_id and hostname(Node Manager) to the application master. Finally Application master uses these containers to run the tasks and also coordinates.\n",
    "  \n",
    "  \n",
    "How the limitations were handled in MapReduce 2.0 with the introduction of YARN\n",
    "* Scheduling is done by resource manager and monitoring is done by Application master. This solves scalability problem.\n",
    "* It is no longer limited to MapReduce can be used for other jobs like spark, giraphtez etc.\n",
    "* There are no fixed amount of Map and Reduce jobs Instead, a concept of comtainers comes in which is very effective in allocating the resources dynamically.\n",
    "\n",
    "**Uberization**  - When the job is very small and Application master thinks that It can do this job in the same set of resources then it won't ask for more resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3507bb0",
   "metadata": {},
   "source": [
    "### Spark On YARN Architecture\n",
    "\n",
    "***How to execute the spark program on spark cluster?***\n",
    "* Interactive Mode - spark-shell/pyspark/notebook\n",
    "* Submitting a job - spark-submit utility\n",
    "\n",
    "***How does spark executes our programs on the cluster?</BR>***\n",
    "Master/Slave Architecture where each application  has a driver which is the master process and a bunch of executors which are the slaves.\n",
    "\n",
    "Driver is responsible for analysing the work and divide the work in many tasks, distributes the tasks, schedule the tasks and finally monitoring.\n",
    "\n",
    "Executor is responsible for executing the code on JVM locally.\n",
    "\n",
    "Two different set of applications or two different spark-submit jobs will have different set of drivers and executors.\n",
    "\n",
    "***Who executes where?</BR>***\n",
    "Executors always resides on the clusters but Driver has the flexibility to launch it on client machine or the cluster machine. Whenever driver runs on client machine it is known as **Client Mode** on the other hand if it runs on cluster the executor it is known as **Cluster Mode**. Spark offers these two deployment modes.\n",
    "\n",
    "Client mode is only preferred for exploratory purpose whereas Cluster mode is preferred for production. In case of client mode if client stops then the driver is also gone.\n",
    "\n",
    "***Who controls the cluster and how spark gets the driver and executor?</BR>***\n",
    "Cluster manager manages the cluster there are many supported cluster managers such as YARN, Kubernates, Mesos, Spark Standalone.\n",
    "\n",
    "***What is a spark session?</BR>***\n",
    "It is a data structure where driver contains all the information including executor location and status. This is the entry point for any spark application\n",
    "\n",
    "**Client Mode**\n",
    "* Spark session is created automatically whenever you open your spark-shell.\n",
    "* As soon as the request is created, Request goes to YARN's resource manager.\n",
    "* Resource manager creates a container on one of the machines and runs an application master there.\n",
    "* This Application master requests for resources in the form of containers.\n",
    "* Application master creates executors inside the containers.\n",
    "* Now executors can contact directly to the driver that is on client machine.\n",
    "\n",
    "**Cluster Mode**\n",
    "* Submits the code using spark-submit utility\n",
    "* Only difference is that the spark driver runs on the application master."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b42b8a",
   "metadata": {},
   "source": [
    "### Find the number of Warinings and errors in a log\n",
    "\n",
    "We're give the following lines from logger, we've to find the number of WARN and ERROR\n",
    "\n",
    "```\n",
    "\"WARN: Tuesday 8 AUG XXXX\"\n",
    "\"ERROR: Tuesday 8 AUG XXXXE\"\n",
    "\"ERROR: Tuesday 8 AUG XXXXF\"\n",
    "\"ERROR: Tuesday 8 AUG XXXXE\"\n",
    "\"ERROR: Tuesday 8 AUG XXXXT\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1ccdf5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "logLines = List(WARN: Tuesday 8 AUG XXXX, ERROR: Tuesday 8 AUG XXXXE, ERROR: Tuesday 8 AUG XXXXF, ERROR: Tuesday 8 AUG XXXXE, ERROR: Tuesday 8 AUG XXXXT)\n",
       "logLines_ = ParallelCollectionRDD[2] at parallelize at <console>:39\n",
       "info = MapPartitionsRDD[3] at map at <console>:40\n",
       "final_ = ShuffledRDD[4] at reduceByKey at <console>:46\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array((ERROR,4), (WARN,1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val logLines = List(\n",
    "\"WARN: Tuesday 8 AUG XXXX\",\n",
    "\"ERROR: Tuesday 8 AUG XXXXE\",\n",
    "\"ERROR: Tuesday 8 AUG XXXXF\",\n",
    "\"ERROR: Tuesday 8 AUG XXXXE\",\n",
    "\"ERROR: Tuesday 8 AUG XXXXT\"\n",
    ")\n",
    "\n",
    "val logLines_ = sc.parallelize(logLines)\n",
    "val info = logLines_.map(\n",
    "x => {\n",
    "    val splitted = x.split(\":\")\n",
    "    (splitted(0),1)\n",
    "  }\n",
    ")\n",
    "val final_ = info.reduceByKey((x,y) => x+y)\n",
    "final_.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a2bca4",
   "metadata": {},
   "source": [
    "### Narrow and Wide tranformation\n",
    "\n",
    "There are two kind for transformations:\n",
    "1. Narrow\n",
    "\n",
    "These are the transformations where shuffling is not involved. It works on the concept of data locality, No movement of data is required.\n",
    "`map`, `flatMap` and `filter` are narrow tranformations. There are many narrow transformations present.\n",
    "\n",
    "2. wide\n",
    "\n",
    "These are the transformations where shuffling is involved for e.g `reduceByKey`. Here data movement is involved so these are expensive operations. We should avoid them as much as possible\n",
    "\n",
    "```\n",
    "500MB file in hdfs\n",
    "\n",
    "val rdd1 = sc.textFile(\"path_to_file\")\n",
    "It will have 4 partitions because of 4 blocks in hdfs.\n",
    "\n",
    "There is a 1:1 mapping between your file block and rdd partitions.\n",
    "\n",
    "rdd1.map(x => x.length) //It will give length of each line so shuffiling is not required because there is no movement of data\n",
    "\n",
    "partitions p1 p2 p3 p4\n",
    "output     o1 o2 o3 o4\n",
    "\n",
    "\n",
    "When we talk about reduceByKey, shuffling is involved\n",
    "\n",
    "  p1         p2        p3         p4\n",
    "(hello, 1)  (Hi, 1)   (now, 1)   (hello, 1)\n",
    "(how, 1)    (yes, 1)  (world, 1) (how, 1)\n",
    "\n",
    "Shuffling is required because grouped data is required\n",
    "```\n",
    "\n",
    "### Stages in Spark\n",
    "Stages are marked by shuffle boundaries. It means whenever we encounter a shuffle a new stage is created. If we'll use n wide transformations then n+1 stages will be created.\n",
    "\n",
    "Output of stage1 is sent to Disk and stage2 reads it back from the disk.\n",
    "\n",
    "In the above practical `Find the number of Warinings and errors in a log` we two stages will be created.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d532ca",
   "metadata": {},
   "source": [
    "### reduceByKey vs reduce\n",
    "\n",
    "`reduceByKey` is a tranformation and `reduce` is an action.\n",
    "```\n",
    "logLines = List(WARN: Tuesday 8 AUG XXXX, ERROR: Tuesday 8 AUG XXXXE, ERROR: Tuesday 8 AUG XXXXF, ERROR: Tuesday 8 AUG XXXXE, ERROR: Tuesday 8 AUG XXXXT)\n",
    "logLines_ = ParallelCollectionRDD[2] at parallelize at <console>:39\n",
    "info = MapPartitionsRDD[3] at map at <console>:40\n",
    "final_ = ShuffledRDD[4] at reduceByKey at <console>:46\n",
    "Array((ERROR,4), (WARN,1))\n",
    "```\n",
    "\n",
    "whenever we call a transformation on a rdd you'll always get a resultant rdd.\n",
    "\n",
    "whenever you call an action on a rdd you'll get a local variable.\n",
    "\n",
    "`reduceByKey` always works on pair rdds, like touple of two elements.\n",
    "\n",
    "`reduce` is an action, you can see the below code.\n",
    "\n",
    "Why developers gave `reduceByKey` is a transformation and `reduce` as an action?</BR>\n",
    "`reduce` gives you a single output that is very small. It won't distribute the data across the cluster.\n",
    "\n",
    "We can still have huge amount of data from `reduceByKey` and we can do further operations on it that's why we want this output as a rdd. We still want to do the things in parallel manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f08eaa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "input = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100)\n",
       "inputRdd = ParallelCollectionRDD[0] at parallelize at <console>:28\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val input = 1 to 100\n",
    "val inputRdd = sc.parallelize(input)\n",
    "inputRdd.reduce((x, y) => x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481df5c2",
   "metadata": {},
   "source": [
    "### groupByKey vs reduceByKey\n",
    "\n",
    "<font color='red'>**IMPORTANT**</font>\n",
    "\n",
    "**Similarities**\n",
    "* Both of them are wide transformations.\n",
    "\n",
    "Let's say you want to find the frequency of each word and your file is there on two nodes.\n",
    "\n",
    "`reduceByKey` before sending it to the reducer, it will first do a local aggregation. So here it is doing more aggregation before shuffling. Shuffling required is less. There are two advantages - **Less Shuffling** and **More parallelism**. This is same as that of combiner acting at the mapper.\n",
    "\n",
    "In `groupByKey` no aggregation will happen locally and more shuffling is required. All the key/value pairs are sent to another machine for shuffling.\n",
    "\n",
    "NOTE: Always prefer reduceByKey and always use groupByKey.\n",
    "\n",
    "Consider you have 1TB data in HDFS and 1000 node cluster.\n",
    "\n",
    "Number of RDD partititions = 1TB/128MB = 8000\n",
    "\n",
    "On each node we might end up getting 8 partitions.\n",
    "\n",
    "example data:\n",
    "```\n",
    "WARN: xyz\n",
    "ERROR: YYY\n",
    "ERROR: ttt\n",
    "INFO: bbb\n",
    ".\n",
    ".\n",
    "INFO: bbb\n",
    "```\n",
    "\n",
    "you've to calculate the frequency of different log levels. If we'll apply `groupByKey`, same log_levels will go on respective machines. If we have only three log levels mentioned above, Max three machines will hold all the data.\n",
    "\n",
    "Before applying `groupByKey` we'd data well distributed in 1000 but after applying it we have data only on three machines.If three machines hold 1TB data in memory then there is a huge possibility of `Out of memory exception`. Even if we don't get Out Of memory exception it is not suggested to use `groupByKey` because parallelism is restricted.\n",
    "\n",
    "```bash\n",
    "[itv002768@g02 ~]$ hadoop fs -mkdir week10_practical_bigLog\n",
    "[itv002768@g02 ~]$ hadoop fs -put bigLog.txt week10_practical_bigLog\n",
    "[itv002768@g02 ~]$ hadoop fs -ls week10_practical_bigLog\n",
    "Found 1 items\n",
    "-rw-r--r--   3 itv002768 supergroup  365001114 2022-08-17 09:51 week10_practical_bigLog/bigLog.txt\n",
    "[itv002768@g02 ~]$ hadoop fs -head week10_practical_bigLog/bigLog.txt\n",
    "ERROR: Thu Jun 04 10:37:51 BST 2015\n",
    "WARN: Sun Nov 06 10:37:51 GMT 2016\n",
    "WARN: Mon Aug 29 10:37:51 BST 2016\n",
    "ERROR: Thu Dec 10 10:37:51 GMT 2015\n",
    "ERROR: Fri Dec 26 10:37:51 GMT 2014\n",
    "ERROR: Thu Feb 02 10:37:51 GMT 2017\n",
    "WARN: Fri Oct 17 10:37:51 BST 2014\n",
    "ERROR: Wed Jul 01 10:37:51 BST 2015\n",
    "WARN: Thu Jul 27 10:37:51 BST 2017\n",
    "WARN: Thu Oct 19 10:37:51 BST 2017\n",
    "WARN: Wed Jul 30 10:37:51 BST 2014\n",
    "ERROR: Fri Jan 12 10:37:51 GMT 2018\n",
    "WARN: Fri May 15 10:37:51 BST 2015\n",
    "ERROR: Tue Jan 16 10:37:51 GMT 2018\n",
    "WARN: Wed Nov 12 10:37:51 GMT 2014\n",
    "ERROR: Fri Jul 25 10:37:51 BST 2014\n",
    "```\n",
    "**Please go through the spark UI for more info for the below example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bce787a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(WARN,4998886)\n",
      "(ERROR,5001114)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "logLines_ = /user/itv002768/week10_practical_bigLog/bigLog.txt MapPartitionsRDD[25] at textFile at <console>:30\n",
       "info = MapPartitionsRDD[26] at map at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[26] at map at <console>:31"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val logLines_ = sc.textFile(\"/user/itv002768/week10_practical_bigLog/bigLog.txt\")\n",
    "val info = logLines_.map(\n",
    "x => {\n",
    "    val splitted = x.split(\":\")\n",
    "    (splitted(0), 1)\n",
    "  }\n",
    ")\n",
    "info.groupByKey.collect().foreach(x => println(x._1, x._2.size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8bd27be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(WARN,4998886)\n",
      "(ERROR,5001114)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "logLines_ = /user/itv002768/week10_practical_bigLog/bigLog.txt MapPartitionsRDD[17] at textFile at <console>:27\n",
       "info = MapPartitionsRDD[18] at map at <console>:28\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[18] at map at <console>:28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val logLines_ = sc.textFile(\"/user/itv002768/week10_practical_bigLog/bigLog.txt\")\n",
    "val info = logLines_.map(\n",
    "x => {\n",
    "    val splitted = x.split(\":\")\n",
    "    (splitted(0), 1)\n",
    "  }\n",
    ")\n",
    "info.reduceByKey(_ + _).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa6762",
   "metadata": {},
   "source": [
    "### Pair RDD\n",
    "Rdd which holds touple of two elements. Transformation like groupByKey, reduceByKey etc. can only work on pair RDDs.\n",
    "\n",
    "*is touple of two elements are same as that of a map?*</BR>\n",
    "No, Because In a map we can only have unique keys. But in case of pair RDDs keys can repeat.\n",
    "\n",
    "This is the same example from week9 where we've to find the top customers\n",
    "\n",
    "```bash\n",
    "[itv002768@g02 ~]$ hadoop fs -head customerorders_practical/customerorders-201008-180523.csv\n",
    "44,8602,37.19\n",
    "35,5368,65.89\n",
    "2,3391,40.64\n",
    "47,6694,14.98\n",
    "29,680,13.08\n",
    "91,8900,24.59\n",
    "70,3959,68.68\n",
    "85,1733,28.53\n",
    "53,9900,83.55\n",
    "```\n",
    "In the below example we are storing the output in a file rather then in a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca3c8cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawCustomersInfo = /user/itv002768/customerorders_practical/customerorders-201008-180523.csv MapPartitionsRDD[21] at textFile at <console>:31\n",
       "splitCust = MapPartitionsRDD[22] at map at <console>:34\n",
       "totalPurchase = MapPartitionsRDD[28] at sortBy at <console>:37\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[28] at sortBy at <console>:37"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawCustomersInfo = sc.textFile(\"/user/itv002768/customerorders_practical/customerorders-201008-180523.csv\")\n",
    "// split on \",\" and take only 1st and third element of an array, and convert third element to float\n",
    "// Put these in a touple\n",
    "val splitCust = rawCustomersInfo.map(x => (x.split(\",\")(0), x.split(\",\")(2).toFloat ) )\n",
    "\n",
    "// Calculate the sum of amount and sort by amount in descending order\n",
    "val totalPurchase = splitCust.reduceByKey((x, y) => x+y).sortBy(x => x._2, false)\n",
    "//val finalInfo = totalPurchase.collect()\n",
    "totalPurchase.saveAsTextFile(\"/user/itv002768/top_customers_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523a8649",
   "metadata": {},
   "source": [
    "```bash\n",
    "[itv002768@g02 ~]$ hadoop fs -ls /user/itv002768/top_customers_output\n",
    "Found 3 items\n",
    "-rw-r--r--   3 itv002768 supergroup          0 2022-08-18 06:17 /user/itv002768/top_customers_output/_SUCCESS\n",
    "-rw-r--r--   3 itv002768 supergroup        704 2022-08-18 06:17 /user/itv002768/top_customers_output/part-00000\n",
    "-rw-r--r--   3 itv002768 supergroup        691 2022-08-18 06:17 /user/itv002768/top_customers_output/part-00001\n",
    "[itv002768@g02 ~]$ hadoop fs -cat /user/itv002768/top_customers_output/*\n",
    "(68,6375.45)\n",
    "(73,6206.199)\n",
    "(39,6193.1104)\n",
    "(54,6065.39)\n",
    "(71,5995.66)\n",
    "(2,5994.591)\n",
    "(97,5977.1895)\n",
    "(46,5963.111)\n",
    "(42,5696.8403)\n",
    "(59,5642.8906)\n",
    "(41,5637.619)\n",
    "(0,5524.9497)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f43b8cb",
   "metadata": {},
   "source": [
    "same example with other new things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26be6e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,10118.861)\n",
      "(42,11393.681)\n",
      "(62,10506.643)\n",
      "(6,10795.759)\n",
      "(46,11926.222)\n",
      "(2,11989.182)\n",
      "(93,10531.5)\n",
      "(28,10001.421)\n",
      "(59,11285.781)\n",
      "(24,10519.84)\n",
      "(39,12386.221)\n",
      "(11,10304.58)\n",
      "(64,10577.38)\n",
      "(8,11034.48)\n",
      "(60,10081.419)\n",
      "(15,10827.0205)\n",
      "(35,10310.84)\n",
      "(97,11954.379)\n",
      "(0,11049.899)\n",
      "(55,10596.18)\n",
      "(40,10372.859)\n",
      "(71,11991.32)\n",
      "(22,10038.898)\n",
      "(26,10500.801)\n",
      "(68,12750.9)\n",
      "(33,10509.318)\n",
      "(17,10065.359)\n",
      "(73,12412.398)\n",
      "(69,10246.02)\n",
      "(41,11275.238)\n",
      "(92,10758.562)\n",
      "(9,10645.299)\n",
      "(34,10661.599)\n",
      "(61,10994.96)\n",
      "(81,10225.42)\n",
      "(25,10115.221)\n",
      "(63,10830.3)\n",
      "(65,10280.699)\n",
      "(29,10065.061)\n",
      "(90,10580.82)\n",
      "(32,10992.101)\n",
      "(85,11006.861)\n",
      "(54,12130.78)\n",
      "(72,10674.879)\n",
      "(52,10490.121)\n",
      "(58,10875.461)\n",
      "(87,10412.799)\n",
      "(70,10736.501)\n",
      "(43,10737.66)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rawCustomersInfo = /user/itv002768/customerorders_practical/customerorders-201008-180523.csv MapPartitionsRDD[41] at textFile at <console>:33\n",
       "splitCust = MapPartitionsRDD[42] at map at <console>:36\n",
       "totalPurchase = ShuffledRDD[43] at reduceByKey at <console>:39\n",
       "finalInfo = MapPartitionsRDD[44] at filter at <console>:40\n",
       "doubledAmount = MapPartitionsRDD[45] at map at <console>:41\n",
       "final_ = Array((19,10118.861), (42,11393.681), (62,10506.643), (6,10795.759), (46,11926.222), (2,11989.182), (93,10531.5), (28,10001.421), (59,11285.781), (24,10519.84), (39,12386.221), (...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array((19,10118.861), (42,11393.681), (62,10506.643), (6,10795.759), (46,11926.222), (2,11989.182), (93,10531.5), (28,10001.421), (59,11285.781), (24,10519.84), (39,12386.221), (..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawCustomersInfo = sc.textFile(\"/user/itv002768/customerorders_practical/customerorders-201008-180523.csv\")\n",
    "// split on \",\" and take only 1st and third element of an array, and convert third element to float\n",
    "// Put these in a touple\n",
    "val splitCust = rawCustomersInfo.map(x => (x.split(\",\")(0), x.split(\",\")(2).toFloat ) )\n",
    "\n",
    "// Calculate the sum of amount and sort by amount in descending order\n",
    "val totalPurchase = splitCust.reduceByKey((x, y) => x+y)\n",
    "val finalInfo = totalPurchase.filter(x => x._2>5000) //customers spending more than 5K\n",
    "val doubledAmount = finalInfo.map(x => (x._1 , x._2*2))\n",
    "val final_ = doubledAmount.collect()\n",
    "for(info <- final_){\n",
    "    println(info)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8837035e",
   "metadata": {},
   "source": [
    "Whenver we call the actions, all the transformations from the very beginning starts executing. What if you call another action like count. Again all the transformations will be executed again from the very beginning.\n",
    "\n",
    "But in case of second action spark can do some optimization and it will only execute the last stage and takes the input from disk for previous stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049e7630",
   "metadata": {},
   "source": [
    "In the below code example how many partitions will be there since we're not loading any file from local filesystem or from HDFS?\n",
    "\n",
    "We can check with a property using `sc.defaultParallelism` and it can give different result for different systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c3b7239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6969d2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logLines = List(WARN: Tuesday 8 AUG XXXX, ERROR: Tuesday 8 AUG XXXXE, ERROR: Tuesday 8 AUG XXXXF, ERROR: Tuesday 8 AUG XXXXE, ERROR: Tuesday 8 AUG XXXXT)\n",
       "logLines_ = ParallelCollectionRDD[49] at parallelize at <console>:38\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val logLines = List(\n",
    "\"WARN: Tuesday 8 AUG XXXX\",\n",
    "\"ERROR: Tuesday 8 AUG XXXXE\",\n",
    "\"ERROR: Tuesday 8 AUG XXXXF\",\n",
    "\"ERROR: Tuesday 8 AUG XXXXE\",\n",
    "\"ERROR: Tuesday 8 AUG XXXXT\"\n",
    ")\n",
    "\n",
    "val logLines_ = sc.parallelize(logLines)\n",
    "// to check the number of  partitions\n",
    "logLines_.getNumPartitions\n",
    "\n",
    "/*\n",
    "val info = logLines_.map(\n",
    "x => {\n",
    "    val splitted = x.split(\":\")\n",
    "    (splitted(0),1)\n",
    "  }\n",
    ")\n",
    "val final_ = info.reduceByKey((x,y) => x+y)\n",
    "final_.collect\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d35130c",
   "metadata": {},
   "source": [
    "We're expecting only one partition because the file is only of few KBs.\n",
    "\n",
    "But we'll end up getting 2 partitions because there is a propery that will allocate minimum partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7393095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultMinPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "346ef432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawCustomersInfo = /user/itv002768/customerorders_practical/customerorders-201008-180523.csv MapPartitionsRDD[51] at textFile at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawCustomersInfo = sc.textFile(\"/user/itv002768/customerorders_practical/customerorders-201008-180523.csv\")\n",
    "rawCustomersInfo.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67065e5b",
   "metadata": {},
   "source": [
    "### What is the difference between repartition and coalesce?\n",
    "<font color='red'>**IMPORTANT**</font>\n",
    "\n",
    "### Repartition\n",
    "Consider you have 500MB file in HDFS and a spark cluster of 20 worker nodes by default your RDD will have 4 partitions, at max 4 machines will be used out of 20 and remaining 16 will remain idle.\n",
    "\n",
    "In this case you can modify the number of partitions using `rdd.repartition(10)`. You can increase or decrease the number of partitions.\n",
    "\n",
    "*When to decrease the partitions?*</BR>\n",
    "Consider you've 1TB file in hdfs then the number of blocks will be 8000. You've 1000 node clusters then each node will hold 8 partitions.</BR>\n",
    "Bunch of transformations - map -> filter -> filter -> reduce\n",
    "\n",
    "when we started, each partition was having 128MB of data but after applying transformations like filter data will start decreasing since most of the data is getting eliminated. Do you still want to maintain a partition having few MBs of data? No, In this case we'll reduce the number of partitions.\n",
    "\n",
    "`Repartition` is a wide transformation because shuffling is involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a2ff94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "rawCustomersInfo = /user/itv002768/customerorders_practical/customerorders-201008-180523.csv MapPartitionsRDD[7] at textFile at <console>:29\n",
       "rawCustomersInfoNewPartition = MapPartitionsRDD[11] at repartition at <console>:30\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawCustomersInfo = sc.textFile(\"/user/itv002768/customerorders_practical/customerorders-201008-180523.csv\")\n",
    "val rawCustomersInfoNewPartition = rawCustomersInfo.repartition(10)\n",
    "rawCustomersInfoNewPartition.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238508e",
   "metadata": {},
   "source": [
    "### Coalesce\n",
    "It can only decrease the number of partitions. In the below example you can see the working of coalesce. It won't give you any error when you try to increase the number of partitions but won't increase.\n",
    "\n",
    "If you want to decrease the partitions you can use any of these `coalesce` or `repartition`. In case you want to increase the partitions you've to use `repartition`.\n",
    "\n",
    "To decrease the number of partitions coalesce is preferred because it will try to minimize the shuffling and give you more performance.\n",
    "\n",
    "Consider you have \n",
    "\n",
    "```\n",
    "node1  - p1 p2 p3 p4\n",
    "node2  - p5 p6 p7 p8\n",
    "node3  - p9 p10 p11 p12\n",
    "node4  - p13 p14 p15 p16\n",
    "```\n",
    "rdd1 has 16 partitions\n",
    "\n",
    "if you'll do rdd1.repartition(8)\n",
    "\n",
    "Repartition has an intention to have final partitions of exectly equal size for this it'll go through complete shuffling\n",
    "\n",
    "if you'll use rdd1.coalesce(8)\n",
    "\n",
    "whenever feasible, It'll try to combine the existing partitions on the same machine to achive the goal. It'll minimize the shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46c389ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawCustomersInfo = /user/itv002768/customerorders_practical/customerorders-201008-180523.csv MapPartitionsRDD[20] at textFile at <console>:31\n",
       "rawCustomersInfoNewPartition = MapPartitionsRDD[24] at repartition at <console>:32\n",
       "decreasePartition = CoalescedRDD[25] at coalesce at <console>:33\n",
       "increasePartition = CoalescedRDD[26] at coalesce at <console>:34\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawCustomersInfo = sc.textFile(\"/user/itv002768/customerorders_practical/customerorders-201008-180523.csv\")\n",
    "val rawCustomersInfoNewPartition = rawCustomersInfo.repartition(10)\n",
    "val decreasePartition = rawCustomersInfoNewPartition.coalesce(3)\n",
    "val increasePartition = decreasePartition.coalesce(4)\n",
    "increasePartition.getNumPartitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b269d24",
   "metadata": {},
   "source": [
    "### Practicals(pyspark)\n",
    "\n",
    "```\n",
    "[itv002768@g02 ~]$ hadoop fs -head /user/itv002768/week10_practical_search_words/bigdatacampaigndata-201014-183159.csv\n",
    "big data contents,Broad match,None,TrendyTech Search India,Broad Match #3,1,1,100%,INR,24.06,24.06,0,0,0%,Search\n",
    "spark training with lab access,Broad match,None,TrendyTech Search India,Broad Match #3,1,2,200%,INR,29.97,59.94,0,0,0%,Search\n",
    "online hadoop training institutes in hyderabad,Broad match,None,TrendyTech Search India,Broad Match #3,1,1,100%,INR,28.45,28.45,0,0,0%,Search\n",
    "coursera data analytics,Broad match,None,TrendyTech Search India,Broad Match #3,1,1,100%,INR,24.64,24.64,0,0,0%,Search\n",
    "ameerpet big data training cost,Broad match,None,TrendyTech Search India,Broad Match #3,2,1,50%,INR,34.86,34.86,0,0,0%,Search\n",
    "good comment on big data trainer,Broad match,None,TrendyTech Search India,Broad Match #3,1,2,200%,INR,30.47,60.94,0,0,0%,Search\n",
    "spark classes,Broad match,None,TrendyTech Search India,Broad Match #3,3,1,33.33%,INR,29.21,29.21,0,0,0%,Search\n",
    "data analytics course near me,Broad match,None,TrendyTech Search India,Broad Match #3,0,1,--,INR,25.42,25.42,0,0,0%,Search\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71e99920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 16394.64),\n",
       " ('big', 12889.279999999999),\n",
       " ('in', 5774.84),\n",
       " ('hadoop', 4818.34),\n",
       " ('course', 4191.59),\n",
       " ('training', 4099.37),\n",
       " ('online', 3484.42),\n",
       " ('courses', 2565.7800000000007),\n",
       " ('intellipaat', 2081.22),\n",
       " ('analytics', 1458.5099999999998),\n",
       " ('tutorial', 1383.37),\n",
       " ('hyderabad', 1118.1600000000003),\n",
       " ('spark', 1078.72),\n",
       " ('best', 1047.7),\n",
       " ('bangalore', 1039.2699999999998),\n",
       " ('and', 985.8),\n",
       " ('certification', 967.44),\n",
       " ('for', 967.05),\n",
       " ('of', 871.4199999999998),\n",
       " ('to', 848.3299999999999)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "val inputFile = sc.textFile(\"/user/itv002768/week10_practical_search_words/bigdatacampaigndata-201014-183159.csv\")\n",
    "val requiredInfo = inputFile.map(x=> (x.split(\",\")(10).toFloat, x.split(\",\")(0)))\n",
    "val allWords = requiredInfo.flatMapValues(x => x.split(\" \")).map(x => (x._2.toLowerCase(), x._1))\n",
    "val finalInfo = allWords.reduceByKey((x, y) => x + y).sortBy(x => x._2, false)\n",
    "val finalAnswer = finalInfo.collect()\n",
    "finalAnswer.take(10).foreach(println)\n",
    "\"\"\"\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "my_conf = SparkConf()\n",
    "my_conf.set(\"spark.app.name\", \"My pysoark Application\")\n",
    "my_conf.set(\"spark.master\", \"local[*]\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=my_conf).getOrCreate()\n",
    "\n",
    "input_file = spark.sparkContext.textFile(\"/user/itv002768/week10_practical_search_words/bigdatacampaigndata-201014-183159.csv\")\n",
    "required_info = input_file.map(lambda x: (float(x.split(\",\")[10]), x.split(\",\")[0]))\n",
    "all_words = required_info.flatMapValues(lambda x: x.split(\" \")).map(lambda x: (x[1].lower(), x[0]))\n",
    "reduced_info = all_words.reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], False)\n",
    "final = reduced_info.collect()\n",
    "final[:20]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2682208c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hadoop', 4818.34),\n",
       " ('intellipaat', 2081.22),\n",
       " ('analytics', 1458.5099999999998),\n",
       " ('hyderabad', 1118.1600000000003),\n",
       " ('spark', 1078.72),\n",
       " ('bangalore', 1039.2699999999998),\n",
       " ('cloudxlab', 707.52),\n",
       " ('bigdata', 694.48),\n",
       " ('dataflair', 643.9000000000001),\n",
       " ('chennai', 604.0400000000001),\n",
       " ('edureka', 351.44),\n",
       " ('iit', 308.73),\n",
       " ('coursera', 293.25),\n",
       " ('pune', 284.71),\n",
       " ('curso', 277.53000000000003),\n",
       " ('cloudera', 258.06),\n",
       " ('simplilearn', 252.45),\n",
       " ('scala', 250.73000000000002),\n",
       " ('ameerpet', 184.94),\n",
       " ('flair', 154.13)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def loadBoringWords():Set[String] = {\n",
    "    // Read the file and load words in a variable\n",
    "    var boringWords: Set[String] = Set()\n",
    "    val lines = Source.fromFile(\"/home/itv002768/boringwords-201014-183159.txt\").getLines()\n",
    "    for (line <- lines){\n",
    "        boringWords += line \n",
    "    }\n",
    "    boringWords\n",
    "}\n",
    "\n",
    "var nameSet = sc.broadcast(loadBoringWords)\n",
    "val inputFile = sc.textFile(\"/user/itv002768/week10_practical_search_words/bigdatacampaigndata-201014-183159.csv\")\n",
    "val requiredInfo = inputFile.map(x=> (x.split(\",\")(10).toFloat, x.split(\",\")(0)))\n",
    "val allWords = requiredInfo.flatMapValues(x => x.split(\" \")).map(x => (x._2.toLowerCase(), x._1))\n",
    "/*\n",
    " Check if the word is present in the nameSet\n",
    " If it is there then it'll return true else false\n",
    " Since, we've to ignore the words that are there in the nameSet\n",
    " We've to use this function with a negation(!)\n",
    " */\n",
    "val filteredWords = allWords.filter(x => !nameSet.value(x._1))\n",
    "val finalInfo = filteredWords.reduceByKey((x, y) => x + y).sortBy(x => x._2, false)\n",
    "val finalAnswer = finalInfo.collect()\n",
    "finalAnswer.take(20).foreach(println)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_boring_words():\n",
    "    boring_words = set((line.strip() for line in \\\n",
    "                        open(\"/home/itv002768/boringwords-201014-183159.txt\")))\n",
    "    return boring_words\n",
    "\n",
    "named_set = spark.sparkContext.broadcast(load_boring_words())\n",
    "input_file = spark.sparkContext.textFile(\"/user/itv002768/week10_practical_search_words/bigdatacampaigndata-201014-183159.csv\")\n",
    "required_info = input_file.map(lambda x: (float(x.split(\",\")[10]), x.split(\",\")[0]))\n",
    "all_words = required_info.flatMapValues(lambda x: x.split(\" \")).map(lambda x: (x[1].lower(), x[0]))\n",
    "filtered_words = all_words.filter(lambda x: x[0] not in named_set.value)\n",
    "reduced_info = filtered_words.reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], False)\n",
    "final = reduced_info.collect()\n",
    "final[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8b8a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('68', 6375.450000000001),\n",
       " ('73', 6206.200000000001),\n",
       " ('39', 6193.110000000001),\n",
       " ('54', 6065.390000000001),\n",
       " ('71', 5995.660000000002),\n",
       " ('2', 5994.59),\n",
       " ('97', 5977.1900000000005),\n",
       " ('46', 5963.109999999999),\n",
       " ('42', 5696.840000000002),\n",
       " ('59', 5642.889999999999),\n",
       " ('41', 5637.620000000001),\n",
       " ('0', 5524.949999999999),\n",
       " ('8', 5517.24),\n",
       " ('85', 5503.43),\n",
       " ('61', 5497.4800000000005),\n",
       " ('32', 5496.05),\n",
       " ('58', 5437.73),\n",
       " ('63', 5415.1500000000015),\n",
       " ('15', 5413.51),\n",
       " ('6', 5397.879999999999),\n",
       " ('92', 5379.279999999999),\n",
       " ('43', 5368.83),\n",
       " ('70', 5368.249999999999),\n",
       " ('72', 5337.439999999999),\n",
       " ('34', 5330.8),\n",
       " ('9', 5322.65),\n",
       " ('55', 5298.089999999999),\n",
       " ('90', 5290.41),\n",
       " ('64', 5288.689999999999),\n",
       " ('93', 5265.75),\n",
       " ('24', 5259.92),\n",
       " ('33', 5254.660000000002),\n",
       " ('62', 5253.3200000000015),\n",
       " ('26', 5250.4),\n",
       " ('52', 5245.0599999999995),\n",
       " ('87', 5206.4),\n",
       " ('40', 5186.429999999999),\n",
       " ('35', 5155.420000000001),\n",
       " ('11', 5152.289999999999),\n",
       " ('65', 5140.35),\n",
       " ('69', 5123.01),\n",
       " ('81', 5112.709999999999),\n",
       " ('19', 5059.43),\n",
       " ('25', 5057.610000000001),\n",
       " ('60', 5040.709999999999),\n",
       " ('17', 5032.68),\n",
       " ('29', 5032.530000000001),\n",
       " ('22', 5019.449999999999),\n",
       " ('28', 5000.709999999999),\n",
       " ('30', 4990.72),\n",
       " ('16', 4979.06),\n",
       " ('51', 4975.220000000001),\n",
       " ('1', 4958.600000000001),\n",
       " ('53', 4945.300000000001),\n",
       " ('18', 4921.269999999999),\n",
       " ('27', 4915.89),\n",
       " ('86', 4908.810000000001),\n",
       " ('76', 4904.210000000001),\n",
       " ('38', 4898.460000000001),\n",
       " ('95', 4876.839999999998),\n",
       " ('89', 4851.48),\n",
       " ('20', 4836.860000000001),\n",
       " ('88', 4830.55),\n",
       " ('10', 4819.7),\n",
       " ('4', 4815.050000000001),\n",
       " ('82', 4812.49),\n",
       " ('31', 4765.049999999999),\n",
       " ('44', 4756.890000000001),\n",
       " ('7', 4755.069999999999),\n",
       " ('37', 4735.200000000001),\n",
       " ('14', 4735.030000000001),\n",
       " ('80', 4727.86),\n",
       " ('21', 4707.41),\n",
       " ('56', 4701.02),\n",
       " ('66', 4681.92),\n",
       " ('12', 4664.589999999999),\n",
       " ('3', 4659.63),\n",
       " ('84', 4652.9400000000005),\n",
       " ('74', 4647.130000000001),\n",
       " ('91', 4642.26),\n",
       " ('83', 4635.8),\n",
       " ('57', 4628.4),\n",
       " ('5', 4561.07),\n",
       " ('78', 4524.51),\n",
       " ('50', 4517.2699999999995),\n",
       " ('67', 4505.79),\n",
       " ('94', 4475.570000000001),\n",
       " ('49', 4394.6),\n",
       " ('48', 4384.33),\n",
       " ('13', 4367.619999999999),\n",
       " ('77', 4327.73),\n",
       " ('47', 4316.299999999998),\n",
       " ('98', 4297.259999999999),\n",
       " ('36', 4278.049999999999),\n",
       " ('75', 4178.5),\n",
       " ('99', 4172.29),\n",
       " ('23', 4042.65),\n",
       " ('96', 3924.2300000000005),\n",
       " ('79', 3790.5699999999997),\n",
       " ('45', 3309.3799999999997)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 39588)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark-2.4.7-bin-hadoop2.7/python/pyspark/accumulators.py\", line 269, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark-2.4.7-bin-hadoop2.7/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark-2.4.7-bin-hadoop2.7/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark-2.4.7-bin-hadoop2.7/python/pyspark/serializers.py\", line 724, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "val rawCustomersInfo = sc.textFile(\"/user/itv002768/customerorders_practical/customerorders-201008-180523.csv\")\n",
    "// split on \",\" and take only 1st and third element of an array, and convert third element to float\n",
    "// Put these in a touple\n",
    "val splitCust = rawCustomersInfo.map(x => (x.split(\",\")(0), x.split(\",\")(2).toFloat ) )\n",
    "\n",
    "// Calculate the sum of amount and sort by amount in descending order\n",
    "val totalPurchase = splitCust.reduceByKey((x, y) => x+y).sortBy(x => x._2, false)\n",
    "//val finalInfo = totalPurchase.collect()\n",
    "totalPurchase.saveAsTextFile(\"/user/itv002768/top_customers_output\")\n",
    "\"\"\"\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "my_conf = SparkConf()\n",
    "my_conf.set(\"spark.app.name\", \"My pysoark Application\")\n",
    "my_conf.set(\"spark.master\", \"local[*]\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=my_conf).getOrCreate()\n",
    "\n",
    "cust_info = spark.sparkContext.textFile(\"/user/itv002768/customerorders_practical/customerorders-201008-180523.csv\")\n",
    "split_cust = cust_info.map(lambda x: (x.split(\",\")[0], float(x.split(\",\")[2])))\n",
    "total_purchase = split_cust.reduceByKey(lambda x, y: x+y).sortBy(lambda x: x[1], False)\n",
    "total_purchase.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c41cf418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 'dddeeefff'), ('a', 'aaabbbccc')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = spark.sparkContext.parallelize([(\"a\", \"aaa bbb ccc\"), (\"b\", \"ddd eee fff\")])\n",
    "y = x.flatMapValues(lambda x: x.split(\" \"))\n",
    "y.reduceByKey(lambda x, y: x+y).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
