{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d696d8e4",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "* [Categories of Optimizations](#Categories-of-Optimizations)\n",
    "  * [Cluster Level Optimizations](#Cluster-Level-Optimizations)\n",
    "  * [Balanced Approach](#Balanced-Approach)\n",
    "  * [Practical](#Practical)\n",
    "* [Salting](#Salting)\n",
    "* [Static Resource Allocation](#Static-Resource-Allocation)\n",
    "* [Memory in Apache Spark](#Memory-in-Apache-Spark)\n",
    "* [Cache and Persist](#Cache-and-Persist)\n",
    "* [Serializer](#Serializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5422c0b4",
   "metadata": {},
   "source": [
    "### Categories of Optimizations\n",
    "\n",
    "There are basically two main areas we should focus on when we talk about Spark Performance Tuning:\n",
    "\n",
    "1. **Cluster configuration level** - This is basically thinking on resource perspective like CPU/Memory etc.\n",
    "\n",
    "2. **Application Code level** - How we write the code. partitioning, bucketing, cache & persist, avoiding shuffling, join optimizations, using optimized file formats, using reduceByKey instead of groupByKey etc.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f157a",
   "metadata": {},
   "source": [
    "### Cluster Level Optimizations\n",
    "\n",
    "When we talk about the resources we mainly focus on Memory(RAM) and CPU cores(Compute) and when we talk about optimization then we've to make sure that job should get the right amount of resources.\n",
    "\n",
    "* Let's say we've 10 node cluster or 10 worker nodes.\n",
    "* Each node has 16 CPU cores.\n",
    "* Each node has 64GB RAM.\n",
    "\n",
    "Executor(It is like a container of resources or JVM, It means it contains both RAM and CPU. It is upon use that how big should be an executor).\n",
    "\n",
    "*Can a single node can have more than one executor?*</BR>\n",
    "Yes, a single node can have more than one execotors or multiple containers.\n",
    "\n",
    "*How many executors can we created in a node?*</BR>\n",
    "16 Cores, 64GB RAM - 1 Core will be alloted to background processing and 1GB RAM to OS</BR>\n",
    "\n",
    "There are two strategies:\n",
    "\n",
    "1. **Thin Executor** - We'll try to create more executors with each executor holding minimum possible resources. As per above configuration we can create 15 executors with each executor holding 1 core and 63/13 GB RAM.\n",
    "\n",
    "Drawbacks:\n",
    "* Here, multithreading is not possible because we've only one core in each container.\n",
    "* A lot of copies of a broadcast variable is required since each executors should recieve a copy of the B variable.\n",
    "\n",
    "2. **Fat Executor** - Intentions is to give maximium resources to each container/executor. As per above configuration we can allocate 15Cores and 63GB RAM to one container/executor.\n",
    "\n",
    "Drawbacks:\n",
    "* It has observed, If an executor holds more than 5 CPU cores than the HDFS throughput suffers.\n",
    "* If executor holds huge amount of memory then garbage collection will take a lot of time. Garbage collection is to remove the unused objects from the memory if the memory is huge it takes time to remove the objects from the memory.\n",
    "\n",
    "Instead of going with Thin and Fat approaches we should go with the balanced way of allocating resources to the executors to avoid the above mentioned bottlenecks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f29fe5",
   "metadata": {},
   "source": [
    "### Balanced Approach\n",
    "10 Node Cluster</BR>\n",
    "16 Cores, 64GB RAM - 1 Core will be alloted to background processing and 1GB RAM to OS</BR>\n",
    "\n",
    "-> We want multithreading, that can be achieved with more than one CPU cores.</BR>\n",
    "-> We do not want our HDFS throughput to suffer, and this happend when we use more than 5 cores.</BR>\n",
    "-> That means 5 CPU cores is the right choice for the number of CPU cores.</BR>\n",
    "\n",
    "* Each executor will contain 5 CPU cores and 21GB RAM.\n",
    "* Out of this 21GB RAM some of it will go as part of overhead(off heap memory). RAW memory that is not used by JVM. That will be MAX(384MB, 7% of executor's memory) ~ 1.5GB.\n",
    "* So each executor will have 5CPU cores and (21-1.5)19.5GB of memory.\n",
    "* Number of executors = 10 * 3 = 30\n",
    "* Out of these 30 executors one will be give for YARN(Application Manager).\n",
    "\n",
    "**Note:** If we store something on RAW memory/off-heap Memory then we can save some time on garbage collection since that won't be required there as it is not a part of executor/JVM. But we've to do our own memory management.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a891b995",
   "metadata": {},
   "source": [
    "### Practical\n",
    "\n",
    "Increase the file size after copying\n",
    "\n",
    "```\n",
    "\n",
    "[itv002768@g02 ~]$ cat bigLogNew.txt bigLogNew.txt bigLogNew.txt >> bigLogLatest.txt\n",
    "[itv002768@g02 ~]$ cat bigLogNew.txt bigLogNew.txt bigLogNew.txt >> bigLogLatest.txt\n",
    "\n",
    "[itv002768@g02 ~]$ ll -h bigLogLatest.txt\n",
    "-rw-r--r-- 1 itv002768 students 8.2G Sep 13 09:48 bigLogLatest.txt\n",
    "\n",
    "[itv002768@g02 ~]$ hadoop fs -put bigLogLatest.txt /user/itv002768/week13_practicals\n",
    "[itv002768@g02 ~]$ hadoop fs -head /user/itv002768/week13_practicals/bigLogLatest.txt\n",
    "ERROR: Thu Jun 04 10:37:51 BST 2015\n",
    "WARN: Sun Nov 06 10:37:51 GMT 2016\n",
    "WARN: Mon Aug 29 10:37:51 BST 2016\n",
    "ERROR: Thu Dec 10 10:37:51 GMT 2015\n",
    "ERROR: Fri Dec 26 10:37:51 GMT 2014\n",
    "ERROR: Thu Feb 02 10:37:51 GMT 2017\n",
    "WARN: Fri Oct 17 10:37:51 BST 2014\n",
    "ERROR: Wed Jul 01 10:37:51 BST 2015\n",
    "WARN: Thu Jul 27 10:37:51 BST 2017\n",
    "WARN: Thu Oct 19 10:37:51 BST 2017\n",
    "WARN: Wed Jul 30 10:37:51 BST 2014\n",
    "ERROR: Fri Jan 12 10:37:51 GMT 2018\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c94d276a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some(http://g02.itversity.com:4040)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@2818e77\n",
       "spark = org.apache.spark.sql.SparkSession@6dd9169d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@6dd9169d"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.functions.column\n",
    "import org.apache.spark.sql.functions.expr\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application\")\n",
    "sparkConfig.set(\"spark.master\", \"yarn\")\n",
    "\n",
    "\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "print(spark.sparkContext.uiWebUrl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32dae127",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.props.update(\"spark.ui.proxyBase\", \"\") // to fix the broken UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6b47e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e151dbb2",
   "metadata": {},
   "source": [
    "If you'll look at the UI you'll see that one executor is added because we are operating on local mode(`local[2]`) that's why executor and driver are same. One container is allocated which is on local machine. It will work on the edge node that is local.\n",
    "\n",
    "For development purpose this is fine but for prod this is not preferred.\n",
    "\n",
    "Now we'll start it with `master=YARN` mode. After this we can see in UI that one driver and two executors are added.\n",
    "\n",
    "*Why two executors are added?*</BR>\n",
    "There are two ways to allocate the resources:\n",
    "\n",
    "* Allocating Manually\n",
    "\n",
    "* Allocating Dynamically - In the cluster that we've in itversity dynamic alloation is `true`. Whenever we go with this, There are some properties that are configured initial executors(2), max.executors(10) and min.executors(0)..\n",
    "\n",
    "**initially 2 executors will be allocated and if the job is big it can go max to 10 and when the job finishes or nothing is required it will come down to 0. It is based on idle time to release the executor that is again a property(executorIdleTime)**\n",
    "\n",
    "You can check for the default configuration of the executor in Ambari.\n",
    "\n",
    "* **Storage Memory** - whenever we allocate memory to the container 300MB goes for some overhead. In our case 1024-300 = 724MB. Again it will be divided into two parts. **Storage & execution Memory**(60% of 724MB) and **Additional Buffer Memory**(40% of 724MB) for other purposes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4ee764e",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": "Job aborted due to stage failure: Task 6 in stage 1.0 failed 4 times, most recent failure: Lost task 6.3 in stage 1.0 (TID 137, w01.itversity.com, executor 16): ExecutorLostFailure (executor 16 exited caused by one of the running tasks) Reason: Container from a bad node: container_1658918988971_19299_01_000017 on host: w01.itversity.com. Exit status: 143. Diagnostics: [2022-09-13 11:59:07.054]Container killed on request. Exit code is 143\n[2022-09-13 11:59:07.054]Container exited with a non-zero exit code 143. \n[2022-09-13 11:59:07.054]Killed by external signal\n.\nDriver stacktrace:",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 1.0 failed 4 times, most recent failure: Lost task 6.3 in stage 1.0 (TID 137, w01.itversity.com, executor 16): ExecutorLostFailure (executor 16 exited caused by one of the running tasks) Reason: Container from a bad node: container_1658918988971_19299_01_000017 on host: w01.itversity.com. Exit status: 143. Diagnostics: [2022-09-13 11:59:07.054]Container killed on request. Exit code is 143",
      "[2022-09-13 11:59:07.054]Container exited with a non-zero exit code 143.",
      "[2022-09-13 11:59:07.054]Killed by external signal",
      ".",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)",
      "  at scala.Option.foreach(Option.scala:257)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)",
      "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)",
      "  at org.apache.spark.rdd.RDD.collect(RDD.scala:989)",
      "  ... 52 elided"
     ]
    }
   ],
   "source": [
    "\n",
    "val inputRdd = spark.sparkContext.textFile(\"/user/itv002768/week13_practicals/bigLogLatest.txt\")\n",
    "val splitRdd = inputRdd.map(x => (x.split(\":\")(0), x.split(\":\")(0)))\n",
    "val rdd3 = splitRdd.groupByKey\n",
    "val rdd4 = rdd3.map(x=>(x._1, x._2.size))\n",
    "rdd4.collect()\n",
    "\n",
    "/* \n",
    "file size - 8.2GB\n",
    "total partitions = (8.2*1024)/128 = 66\n",
    "Since there are two stages so 66*2 = 132 Tasks\n",
    "\n",
    "Stage-1\n",
    "10 executors have to execute 66 tasks and Some of the executors might have to execute multiple tasks\n",
    "If in executor there is one CPU core then only one task can be performed at a time - Our case. At max 10 tasks can run in parallel.\n",
    "Parallelism  - num_executors * num_cpu_cores\n",
    "\n",
    "Stage 0 had 66 tasks and each of the partition was having equal amount of data\n",
    "\n",
    "stage 1 - After we used groupByKey, shuffling happened and we had only two keys ERROR and WARN and only two partitions were full.\n",
    "        Let's assume If I'll divide the data in two parts then one of the partitions definately will have data > 4.25GB. Only two partitions will\n",
    "        hold the data out of 66 partitions created in stage 1. Entire data will be executed in two tasks and remaining 64tasks won't do anything.\n",
    "        Even executor cannot handle this much of data. Executor can handle 400MB and we want this executor to handle 4.25GB this will give Out Of Memory\n",
    "        Error.\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26251ef",
   "metadata": {},
   "source": [
    "### Salting\n",
    "In the above case where we've uneven data we can append some number let's say (1 to 10) after every key then data can be distributed evenly.\n",
    "e.g WARN1, WARN2 ..... WARN10 ERROR1, ERROR2 ...... ERROR10\n",
    "\n",
    "20 Distinct keys and It can use 20 different partitions.\n",
    "\n",
    "### Static Resource Allocation\n",
    "\n",
    "To disable the dynamic allocation:\n",
    "\n",
    "`spark2-shell --conf spark.dynamicAllocation.enabled=false --master yarn --num-executors 20 --executor-cores 2 --executor-memory 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f6a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.functions.column\n",
    "import org.apache.spark.sql.functions.expr\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application\")\n",
    "sparkConfig.set(\"spark.master\", \"yarn\")\n",
    "sparkConfig.set(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "sparkConfig.set(\"spark.executor.memory\", \"3500m\")\n",
    "sparkConfig.set(\"spark.executor.cores\", \"2\")\n",
    "sparkConfig.set(\"spark.executor.instances\", \"20\")\n",
    "\n",
    "\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "print(spark.sparkContext.uiWebUrl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26f4577f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@10d25f40"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf42e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd2ff62f",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": "Job 0 cancelled ",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job 0 cancelled",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1860)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)",
      "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)",
      "  at org.apache.spark.rdd.RDD.collect(RDD.scala:989)",
      "  ... 52 elided"
     ]
    }
   ],
   "source": [
    "val inputRdd = spark.sparkContext.textFile(\"/user/itv002768/week13_practicals/bigLogLatest.txt\")\n",
    "val splitRdd = inputRdd.map(x => (x.split(\":\")(0), x.split(\":\")(0)))\n",
    "val rdd3 = splitRdd.groupByKey\n",
    "val rdd4 = rdd3.map(x=>(x._1, x._2.size))\n",
    "rdd4.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f6ed7",
   "metadata": {},
   "source": [
    "* If you've a long running job then it's better to go with `Dynamic Resource Allocation`.\n",
    "\n",
    "For the above practical we need container with more memory that is not possible So in this case we'll use **Salting**.\n",
    "\n",
    "We'll generate a random number in range(1 to 40) for both the WARN and ERROR keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96eaa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some(http://g02.itversity.com:4046)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@2764efff\n",
       "spark = org.apache.spark.sql.SparkSession@6732a97f\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@6732a97f"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.functions.column\n",
    "import org.apache.spark.sql.functions.expr\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application\")\n",
    "sparkConfig.set(\"spark.master\", \"yarn\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "print(spark.sparkContext.uiWebUrl)\n",
    "sys.props.update(\"spark.ui.proxyBase\", \"\") // to fix the broken UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e8fbdf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "random = scala.util.Random@28ed973c\n",
       "start = 1\n",
       "end = 60\n",
       "inputRdd = /user/itv002768/week13_practicals/bigLogLatest.txt MapPartitionsRDD[1] at textFile at <console>:40\n",
       "rdd1 = MapPartitionsRDD[2] at map at <console>:42\n",
       "rdd2 = ShuffledRDD[3] at groupByKey at <console>:47\n",
       "rdd3 = MapPartitionsRDD[4] at map at <console>:49\n",
       "rdd4 = MapPartitionsRDD[5] at map at <console>:51\n",
       "rdd5 = ShuffledRDD[6] at reduceByKey at <console>:59\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array((WARN,119973264), (ERROR,120026736))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val random = new scala.util.Random\n",
    "val start = 1\n",
    "val end = 60\n",
    "\n",
    "val inputRdd = spark.sparkContext.textFile(\"/user/itv002768/week13_practicals/bigLogLatest.txt\")\n",
    "\n",
    "val rdd1 = inputRdd.map(x => {\n",
    "    val num = start + random.nextInt((end - start) + 1)\n",
    "    (x.split(\":\")(0) + num, x.split(\":\")(1))\n",
    "})\n",
    "\n",
    "val rdd2 = rdd1.groupByKey\n",
    "\n",
    "val rdd3 = rdd2.map(x=>(x._1, x._2.size))\n",
    "\n",
    "val rdd4 = rdd3.map(x => {\n",
    "    if(x._1.substring(0, 4)==\"WARN\"){\n",
    "        (\"WARN\", x._2)\n",
    "    }else{\n",
    "        (\"ERROR\", x._2)\n",
    "    }\n",
    "})\n",
    "\n",
    "val rdd5 = rdd4.reduceByKey(_ + _)\n",
    "\n",
    "rdd5.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9db4a1",
   "metadata": {},
   "source": [
    "### Memory in Apache Spark\n",
    "\n",
    "**Spark Optimization - session 10**\n",
    "\n",
    "Memory use in spark falls under two broad categories:\n",
    "\n",
    "* Execution Memory</BR>\n",
    "Memory required for computations like join, shuffles, sorts, aggregations etc.\n",
    "\n",
    "\n",
    "* Storage Memory</BR>\n",
    "Memory used for caching.\n",
    "\n",
    "In spark, execution and storage memory share a common region.</BR>\n",
    "\n",
    "**Benefit of  Common region**</BR>\n",
    "When there is no execution is happening then storage can accuire all the available memory or vice versa.\n",
    "\n",
    "Common Region - 2GB</BR>\n",
    "* If necessary, Execution may evict storeage. It means execution has the presendency. But this eviction can only happen unit total storage memory falls under a certain threshold. If some executions/computions are coming it cannot accuire all the 2GB.\n",
    "\n",
    "* Storage cannot evict Execution. Execution can't stop because storage is required.\n",
    "\n",
    "Advantage\n",
    "\n",
    "This Design ensure several desirable properties:\n",
    "* Applications which do not use caching can use entire space for execution.\n",
    "* Applications which do not use caching right now but want to use at later point of time can reserver minumum storage space. This makes sure that the data blockes are immune from getting evicted.\n",
    "\n",
    "* Let's say we're requesting a container with 4GB so it will give 4GB + max(384MB, 10% of heap) to a container. 4GB will be Heap(JVM) and other will be Off Heap(reserved for VM overheads)\n",
    "* 300MB is again reserved for running the executors.\n",
    "* We are left with 3.7GB.\n",
    "* Out of this 3.7GB, 60% will go to unified(storage + execution memory) ~ 2.3GB\n",
    "* Remaining 40% of 3.7GB(~1.4GB) will go to user memory - that holds user data structures, spark related metadata and sefeguarding Out of memory Errors.\n",
    "\n",
    "* Out of 2.3GB(storage + execution memory), 50% is the threshold for storage. It means we can cache data upto 1.15GB without worrying about eviction by executions/computations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c287f1e",
   "metadata": {},
   "source": [
    "### Cache and Persist\n",
    "\n",
    "**Spark Optimization - session 10**</BR>\n",
    "Please go through the Spark UI after running the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eea4bd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some(http://g02.itversity.com:4040)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkConfig = org.apache.spark.SparkConf@15822350\n",
       "spark = org.apache.spark.sql.SparkSession@14c5aff7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@14c5aff7"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.functions.column\n",
    "import org.apache.spark.sql.functions.expr\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "sparkConfig.set(\"spark.app.name\", \"My Application\")\n",
    "sparkConfig.set(\"spark.master\", \"yarn\")\n",
    "\n",
    "val spark = SparkSession.builder().config(sparkConfig).getOrCreate()\n",
    "\n",
    "print(spark.sparkContext.uiWebUrl)\n",
    "sys.props.update(\"spark.ui.proxyBase\", \"\") // to fix the broken UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9911aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "random = scala.util.Random@13a5434c\n",
       "start = 1\n",
       "end = 60\n",
       "inputRdd = /user/itv002768/week13_practicals/bigLogLatest.txt MapPartitionsRDD[1] at textFile at <console>:40\n",
       "rdd1 = MapPartitionsRDD[2] at map at <console>:42\n",
       "rdd2 = ShuffledRDD[3] at groupByKey at <console>:47\n",
       "rdd3 = MapPartitionsRDD[4] at map at <console>:52\n",
       "rdd4 = MapPartitionsRDD[5] at map at <console>:56\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array((WARN,1994728), (WARN,2000208), (WARN,1998905), (WARN,2002428), (WARN,2004661), (WARN,1990811), (WARN,1986703), (WARN,2002536), (WARN,1989488), (WARN,20..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val random = new scala.util.Random\n",
    "val start = 1\n",
    "val end = 60\n",
    "\n",
    "val inputRdd = spark.sparkContext.textFile(\"/user/itv002768/week13_practicals/bigLogLatest.txt\")\n",
    "\n",
    "val rdd1 = inputRdd.map(x => {\n",
    "    val num = start + random.nextInt((end - start) + 1)\n",
    "    (x.split(\":\")(0) + num, x.split(\":\")(1))\n",
    "})\n",
    "\n",
    "val rdd2 = rdd1.groupByKey\n",
    "\n",
    "/*\n",
    "If we'll cache the following as it is doing a lot of things \n",
    "*/\n",
    "val rdd3 = rdd2.map(x=>(x._1, x._2.size))\n",
    "\n",
    "rdd3.cache\n",
    "\n",
    "val rdd4 = rdd3.map(x => {\n",
    "    if(x._1.substring(0, 4)==\"WARN\"){\n",
    "        (\"WARN\", x._2)\n",
    "    }else{\n",
    "        (\"ERROR\", x._2)\n",
    "    }\n",
    "})\n",
    "\n",
    "//val rdd5 = rdd4.reduceByKey(_ + _)\n",
    "\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36e5936b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((WARN,1994728), (WARN,2000208), (WARN,1998905), (WARN,2002428), (WARN,2004661), (WARN,1990811), (WARN,1986703), (WARN,2002536), (WARN,1989488), (WARN,2006403), (WARN,1999421), (WARN,1993077), (WARN,2004461), (WARN,2009241), (WARN,2006287), (WARN,1999164), (WARN,2027641), (WARN,2000844), (WARN,1990925), (WARN,1995284), (ERROR,1989047), (WARN,1997947), (ERROR,2010830), (WARN,1985123), (WARN,2002821), (ERROR,1999714), (WARN,1994828), (ERROR,2008639), (ERROR,1995269), (ERROR,1996067), (ERROR,1997713), (ERROR,1998259), (ERROR,2003380), (ERROR,2003405), (ERROR,1993746), (ERROR,2001933), (ERROR,1986944), (ERROR,2004486), (ERROR,1992079), (ERROR,2008356), (ERROR,2003895), (ERROR,1994750), (ERROR,2002428), (ERROR,2008659), (ERROR,2006985), (ERROR,2001868), (ER..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b471bb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[6] at map at <console>:43"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "To uncache use unpersist\n",
    "\n",
    "After that you can check in storage Tab\n",
    "*/\n",
    "rdd3.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f59aacac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[6] at map at <console>:43"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42a549e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((WARN,1994728), (WARN,2000208), (WARN,1998905), (WARN,2002428), (WARN,2004661), (WARN,1990811), (WARN,1986703), (WARN,2002536), (WARN,1989488), (WARN,2006403), (WARN,1999421), (WARN,1993077), (WARN,2004461), (WARN,2009241), (WARN,2006287), (WARN,1999164), (WARN,2027641), (WARN,2000844), (WARN,1990925), (WARN,1995284), (ERROR,1989047), (WARN,1997947), (ERROR,2010830), (WARN,1985123), (WARN,2002821), (ERROR,1999714), (WARN,1994828), (ERROR,2008639), (ERROR,1995269), (ERROR,1996067), (ERROR,1997713), (ERROR,1998259), (ERROR,2003380), (ERROR,2003405), (ERROR,1993746), (ERROR,2001933), (ERROR,1986944), (ERROR,2004486), (ERROR,1992079), (ERROR,2008356), (ERROR,2003895), (ERROR,1994750), (ERROR,2002428), (ERROR,2008659), (ERROR,2006985), (ERROR,2001868), (ER..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab234538",
   "metadata": {},
   "source": [
    "### Serializer\n",
    "\n",
    "We should prefer kryo serializer insted of java serializer.\n",
    "\n",
    "Whenever the data is stored on disc and has to be transfer over the disc it has to be in serialized form.\n",
    "\n",
    "Whenever we use java based serializer size is still more than kryo seraializer. It means by using kryo searializer, size will be much seraializer. kryo searializer is also much more faster than java serializer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2 - Scala",
   "language": "scala",
   "name": "spark_2_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
